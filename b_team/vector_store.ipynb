{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31baff17",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# !pip install langchain-text-splitters qdrant-client langchain-qdrant sentence-transformers torch torchvision torchaudio\n",
    "# !pip install ragas rapidfuzz ipywidgets langchain-huggingface accelerate\n",
    "\n",
    "# !pip install ragas datasets pandas openai\n",
    "# !pip install langchain langchain-openai langchain-community typing-extention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd2a6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "import torch,uuid\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain_huggingface import HuggingFaceEmbeddings ,HuggingFacePipeline\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough,RunnableLambda\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from typing import Any, List, Optional\n",
    "\n",
    "## ragas\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics.collections import Faithfulness, AnswerRelevancy\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52746bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 파일 data load\n",
    "\n",
    "# 파일 경로 지정\n",
    "file_path = '사회복지_법령_전체.txt'\n",
    "\n",
    "# 파일 내용이 담긴 변수\n",
    "law_data=''\n",
    "\n",
    "\n",
    "# 파일 내용 load\n",
    "try:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        law_data = f.read()\n",
    "    print(f\"전체 글자 수: {len(law_data):,}자\")\n",
    "except FileNotFoundError:\n",
    "    print(\"파일을 찾을 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a36d5ad",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### 문서 분할\n",
    "\n",
    "# 필요없는 부분 법령명으로 replace\n",
    "law_data = law_data.replace(\"\"\"판례\n",
    "연혁\n",
    "위임행정규칙\n",
    "규제\n",
    "생활법령\n",
    "한눈보기\"\"\",\"\")\n",
    "\n",
    "# 분할할 방식 설정\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\\n\",\n",
    "    chunk_size=1,           # 구분자 기준으로 바로 쪼개지도록 최소값 설정\n",
    "    chunk_overlap=0,        # 중복 없음\n",
    "    is_separator_regex=False # 일반 문자열로 취급\n",
    ")\n",
    "\n",
    "# 분할\n",
    "chunks = text_splitter.split_text(law_data)\n",
    "\n",
    "# vectorDB에 넣을 문서 리스트\n",
    "documents=[]\n",
    "\n",
    "# vectorDB에 넣을 형식으로 변환\n",
    "for chunk in chunks:\n",
    "    #문서의 법령을 제목으로 사용하기 위한 개행으로 split\n",
    "    law_name = chunk.splitlines()\n",
    "    \n",
    "    # vectorDB에 넣을 형식으로 변환\n",
    "    doc = Document(\n",
    "        page_content=chunk,\n",
    "        metadata={\n",
    "            \"law_name\": law_name[0], \n",
    "            \"length\": len(chunk)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # vectorDB에 넣을 list에 변환한 문서 append\n",
    "    documents.append(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cee044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# huggingface login\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "\n",
    "load_dotenv(\"env\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6439eec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### VectorDB에 저장\n",
    "\n",
    "# 모델에 따라 달라질 코드(임베딩)\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"woong0322/ko-legal-sbert-finetuned\",\n",
    "    model_kwargs={'device': device},\n",
    "    encode_kwargs={'normalize_embeddings': True} # 의미 기반 검색 최적화\n",
    ")\n",
    "\n",
    "# qdrant 연결\n",
    "# url = \"http://localhost:6333\"\n",
    "client = QdrantClient(\":memory:\")\n",
    "\n",
    "# 각 법령의 구분 키값\n",
    "ids = [\n",
    "    str(uuid.uuid5(uuid.NAMESPACE_DNS, f\"{doc.metadata['law_name']}_{i}\")) \n",
    "    for i, doc in enumerate(documents)\n",
    "]\n",
    "\n",
    "# DB명\n",
    "collection_name = \"B-TEAM\"\n",
    "\n",
    "# vectorDB에 저장\n",
    "vector_store = QdrantVectorStore.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings,\n",
    "    ids = ids,\n",
    "    location=\":memory:\",\n",
    "    client=client,\n",
    "    collection_name=collection_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a54a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### RAG\n",
    "\n",
    "# 임베딩 모델 설정\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"woong0322/ko-legal-sbert-finetuned\",\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "# vectorDB연결\n",
    "url = \"http://localhost:6333\"\n",
    "collection_name = \"B-TEAM\"\n",
    "\n",
    "vector_store = QdrantVectorStore.from_existing_collection(\n",
    "    embedding=embeddings,\n",
    "    collection_name=collection_name,\n",
    "    url=url,\n",
    ")\n",
    "\n",
    "# 질문에 대한 답은 가장 유사한 것 하나\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "#llm 모델 설정\n",
    "model_id = \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "torch_dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    device_map=\"auto\",               \n",
    "    torch_dtype=torch_dtype,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model_engine = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.1,\n",
    "    do_sample=True,\n",
    "    return_full_text=False,\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=model_engine)\n",
    "\n",
    "# 프롬프트 구성\n",
    "qa_system_prompt = \"\"\"너는 대한민국 사회복지사를 지원하는 법령 검색 전문 AI 도우미이다.\n",
    "\n",
    "**기본 규칙**\n",
    "답변은 항상 한국어로 한다.\n",
    "당신은 전문가 도우미로 제공된 [법령 정보] 데이터에 기반한 정보를 제공한다.\n",
    "질문이 불확실한 경우 한 번 더 질문하여 질문을 구체화 한다.\n",
    "정보가 부족하면 \"정보를 찾을 수 없다\"고 답한다.\n",
    "데이터에 없는 내용은 추측해 답하지 않는다.\n",
    "\n",
    "역할:\n",
    "- 사회복지 관련 법령(예: 사회복지사업법, 노인복지법, 아동복지법, 장애인복지법 등)을 근거 문서에 기반하여 정확하게 안내한다.\n",
    "- 사용자의 질문에 대해 관련 법 조항을 우선적으로 제시하고, 사회복지 실무 관점에서 이해하기 쉽게 설명한다.\n",
    "\n",
    "원칙:\n",
    "- 제공된 문서[법령 정보] 안의 정보만을 근거로 답변한다.\n",
    "- 문서에 없는 내용은 추측하지 말고 \"관련 근거를 찾을 수 없다\"고 답변한다.\n",
    "- 법률적 최종 판단이나 자문은 하지 않는다.\n",
    "- 항상 조항 번호와 법령명을 명시한다.\n",
    "\n",
    "\n",
    "**질문 처리 절차 **\n",
    "1. 질문에서 \"핵심단어\"를 인식한다.\n",
    "-\"핵심단어\"란 질문자가 알고 싶어하는 정보를 찾기 위한 keyword 다.\n",
    "-예시: \"사회복지법인 설립 조건을 알고 싶어\", \"사회복지법인을 만들려면 어떻게 해야하지?\" 등의 질문의 \"핵심단어\"는 \"사회복지법인\", \"설립 조건\", \"만들다\"이다.\n",
    "\n",
    "2. \"핵심단어\"를 기준으로 조회한 법령들 중에서 질문과 가장 유사한 법령을 찾는다.\n",
    "-필요한 경우 질문과 연관된 추가 조항도 검토하여 답변의 완성도를 높인다.\n",
    "-예시: \"사회복지법인 설립 조건을 알고 싶어\" -> 제16조(법인의 설립허가) 항목 이외에 17조(정관)등에 대한 내용까지 요약 정리.\n",
    "\n",
    "\n",
    "답변 형식:\n",
    "1. 물어본 질문에 대해 간결하게 답변할 것.\n",
    "2. 근거가 된 [법령정보]에 대해 3가지 이하로 첨부할 것. 최소한으로 덧붙인다.\n",
    "3. 주의사항 또는 한계 안내할 것.\n",
    "\n",
    "사용자 : (사용자의 질문 내용)\n",
    "모델 : (질문에 대해 대화하듯이 친절하게 설명)\n",
    "\n",
    "관련 조항\n",
    "1. 제O조(명칭) : 조항의 핵심 내용 \n",
    "2. 제O조(명칭) : 조항의 핵심 내용 \n",
    "3. 제O조(명칭) : 조항의 핵심 내용 \n",
    "\n",
    "주의사항 또는 한계\n",
    "이 답변은 법률 자문이 아니며, 구체적인 행정 해석이나 적용 여부는 관할 행정기관 또는 법률 전문가에게 확인해야 한다.\n",
    "\n",
    "[법령 정보]:\n",
    "{context}\"\"\"\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", qa_system_prompt),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "# LCEL 체인\n",
    "def extract_content(docs):\n",
    "    return docs[0].page_content if docs else \"관련 법령 없음\"\n",
    "\n",
    "# LCEL 체인: 구조 변경 없이 그대로 사용\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": (lambda x: x[\"input\"]) | retriever | extract_content,\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"chat_history\": lambda x: x[\"chat_history\"]\n",
    "    }\n",
    "    | qa_prompt \n",
    "    | llm  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d98f541-c7f5-4c27-8ea1-afd37b5e36f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.embeddings import embedding_factory\n",
    "from ragas.llms import llm_factory\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a337ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 성능 평가 모델\n",
    "evaluator_llm = llm_factory('gpt-4o-mini',client=OpenAI(api_key=os.getenv('OPENAI_API_KEY'))) # 판사 LLM\n",
    "evaluator_embeddings =embedding_factory(provider=\"openai\",model=\"text-embedding-3-small\",client=OpenAI(api_key=os.getenv('OPENAI_API_KEY')))\n",
    "\n",
    "#실제 대화\n",
    "chat_history = []\n",
    "print(\"법률 상담을 시작합니다. (종료: exit)\")\n",
    "metrics = [\n",
    "    Faithfulness(llm=evaluator_llm),\n",
    "    AnswerRelevancy(llm=evaluator_llm, embeddings=evaluator_embeddings)\n",
    "]\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"\\n나: \")\n",
    "    if user_input.lower() in [\"exit\", \"종료\"]: break\n",
    "\n",
    "    #성능 평가 텍스트\n",
    "    retrieved_docs = retriever.invoke(f\"query: {user_input}\")\n",
    "    contexts = [doc.page_content for doc in retrieved_docs]\n",
    "\n",
    "    # 체인 호출\n",
    "\n",
    "    response = rag_chain.invoke({\"input\": user_input, \"chat_history\": chat_history})\n",
    "    print(f\"{response}\")\n",
    "\n",
    "    #성능 평가\n",
    "    current_eval_data = {\n",
    "        \"question\": [user_input],        # user_input 대신 question\n",
    "        \"answer\": [str(response)],       # response 대신 answer\n",
    "        \"contexts\": [contexts]           # retrieved_contexts 대신 contexts\n",
    "    }\n",
    "    eval_dataset = Dataset.from_dict(current_eval_data)\n",
    "\n",
    "    score = evaluate(\n",
    "            dataset=eval_dataset,\n",
    "            metrics =metrics,\n",
    "            llm=evaluator_llm,\n",
    "            embeddings=evaluator_embeddings\n",
    "        )\n",
    "        \n",
    "        # 점수 출력\n",
    "    f_score = score[\"faithfulness\"]\n",
    "    ar_score = score[\"answer_relevancy\"]\n",
    "    print(f\"   [ 성능 점수] 충실도(Faithfulness): {f_score:.2f} | 관련성(Relevancy): {ar_score:.2f}\")\n",
    "\n",
    "    # 대화 기록 업데이트 (최근 3턴만 유지하여 CPU 부담 감소)\n",
    "    chat_history.extend([HumanMessage(content=user_input), AIMessage(content=response)])\n",
    "    chat_history = chat_history[-6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0376fd-19be-4181-984d-aa3bb73430ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
