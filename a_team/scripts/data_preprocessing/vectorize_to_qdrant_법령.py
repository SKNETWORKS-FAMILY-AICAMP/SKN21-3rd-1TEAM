"""
ë²•ë ¹ ë°ì´í„° Qdrant ë²¡í„° DB ì—…ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸
- processed/law_chunks.json ë¡œë“œ
- Qdrant ì»¬ë ‰ì…˜(A-TEAM)ì— ì—…ì„œíŠ¸
"""

from sentence_transformers import SentenceTransformer
from qdrant_client.models import VectorParams, Distance, PointStruct
from qdrant_client import QdrantClient
from dotenv import load_dotenv
import json
import os
import sys
from typing import List, Dict, Any
from pathlib import Path

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ============================================================
# ì„¤ì •
# ============================================================
SCRIPT_DIR = Path(__file__).parent
DATA_DIR = SCRIPT_DIR / '..' / 'data'
PROCESSED_FILE = DATA_DIR / 'processed' / 'fd_ë²•ë ¹_chunked.json'

EMBEDDING_MODEL = "Qwen/Qwen3-Embedding-0.6B"
EMBEDDING_DIM = 1024
COLLECTION_NAME = os.getenv("QDRANT_COLLECTION_NAME", "A-TEAM")


# ============================================================
# Qdrant í´ë¼ì´ì–¸íŠ¸ í´ë˜ìŠ¤
# ============================================================
class LegalVectorDB:
    def __init__(self, url: str = None, api_key: str = None, host: str = None, port: int = 6333):
        """Qdrant í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        if url and api_key:
            print(f"ğŸŒ Qdrant í´ë¼ìš°ë“œ ì—°ê²°: {url[:30]}...")
            self.client = QdrantClient(url=url, api_key=api_key, timeout=60)
        elif host:
            print(f"ğŸ  Qdrant ì„œë²„ ì—°ê²°: {host}:{port}")
            self.client = QdrantClient(host=host, port=port, timeout=60)
        else:
            print("âš ï¸ ì—°ê²° ì •ë³´ ì—†ìŒ, ë©”ëª¨ë¦¬ ëª¨ë“œ")
            self.client = QdrantClient(":memory:")

        print(f"ğŸ§  ì„ë² ë”© ëª¨ë¸ ë¡œë”©: {EMBEDDING_MODEL}")
        self.model = SentenceTransformer(EMBEDDING_MODEL)

    def create_collection(self, name: str, recreate: bool = False):
        """ì»¬ë ‰ì…˜ ìƒì„±"""
        collections = [
            c.name for c in self.client.get_collections().collections]
        if name in collections:
            if recreate:
                print(f"â™»ï¸  ì»¬ë ‰ì…˜ '{name}' ì¬ìƒì„± (ì‚­ì œ í›„ ìƒì„±)")
                self.client.delete_collection(name)
            else:
                print(f"âœ… ì»¬ë ‰ì…˜ '{name}' ì´ë¯¸ ì¡´ì¬")
                return

        self.client.create_collection(
            collection_name=name,
            vectors_config=VectorParams(
                size=EMBEDDING_DIM, distance=Distance.COSINE)
        )
        print(f"âœ¨ ì»¬ë ‰ì…˜ '{name}' ìƒì„± ì™„ë£Œ")

    def upsert_chunks(self, collection_name: str, chunks: List[Dict[str, Any]], batch_size: int = 16, start_id: int = 0):
        """ì²­í¬ ì—…ì„œíŠ¸ (ë°°ì¹˜ ì²˜ë¦¬ + ì¬ì‹œë„ ë¡œì§)"""
        import time
        from qdrant_client.http.exceptions import UnexpectedResponse

        if not chunks:
            print("âŒ ì—…ë¡œë“œí•  ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        total = len(chunks)
        print(
            f"ğŸš€ ì´ {total}ê°œ ì²­í¬ ì—…ë¡œë“œ ì‹œì‘ (Batch Size: {batch_size}, Start ID: {start_id})...")

        for i in range(0, total, batch_size):
            batch = chunks[i: i + batch_size]
            texts = [c['text'] for c in batch]

            # ì„ë² ë”©
            embeddings = self.model.encode(
                texts, show_progress_bar=False, convert_to_numpy=True)

            points = []
            for idx, (chunk, vector) in enumerate(zip(batch, embeddings)):
                payload = chunk['metadata'].copy()
                payload['text'] = chunk['text']

                points.append(PointStruct(
                    id=start_id + i + idx,
                    vector=vector.tolist(),
                    payload=payload
                ))

            # ì¬ì‹œë„ ë¡œì§ (ìµœëŒ€ 3íšŒ)
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    self.client.upsert(
                        collection_name=collection_name, points=points)
                    break
                except Exception as e:
                    if attempt < max_retries - 1:
                        wait = (attempt + 1) * 2
                        print(
                            f"\nâš ï¸  ì—…ë¡œë“œ ì‹¤íŒ¨ (ì‹œë„ {attempt+1}/{max_retries}): {e}")
                        print(f"â³ {wait}ì´ˆ í›„ ì¬ì‹œë„...")
                        time.sleep(wait)
                    else:
                        print(f"\nâŒ ìµœì¢… ì‹¤íŒ¨: {e}")
                        raise e

            print(
                f"\rğŸ“¥ ì €ì¥ ì¤‘: {i + len(batch)}/{total} ({(i + len(batch))/total*100:.1f}%)", end='', flush=True)

        print(f"\nâœ… '{collection_name}' ì—…ë¡œë“œ ì™„ë£Œ!")

    def search(self, img_query: str, top_k: int = 3):
        """í…ŒìŠ¤íŠ¸ ê²€ìƒ‰"""
        vec = self.model.encode(img_query).tolist()
        hits = self.client.query_points(
            collection_name=COLLECTION_NAME, query=vec, limit=top_k).points
        return hits


# ============================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================
def main():
    print("=" * 60)
    print("âš–ï¸  ë²•ë ¹ ë°ì´í„° Qdrant ì—…ë¡œë“œ")
    print("=" * 60)

    # 1. ì „ì²˜ë¦¬ëœ íŒŒì¼ ë¡œë“œ
    if not PROCESSED_FILE.exists():
        print(f"âŒ ì „ì²˜ë¦¬ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {PROCESSED_FILE}")
        print("ğŸ’¡ ë¨¼ì € 'uv run a_team/scripts/preprocesser_ë²•ë ¹.py'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    print(f"ğŸ“‚ íŒŒì¼ ë¡œë“œ ì¤‘: {PROCESSED_FILE}")
    with open(PROCESSED_FILE, 'r', encoding='utf-8') as f:
        chunks = json.load(f)
    print(f"ğŸ“Š ë¡œë“œëœ ì²­í¬: {len(chunks)}ê°œ")

    # 2. Qdrant ì—°ê²°
    url = os.getenv("QDRANT_URL")
    key = os.getenv("QDRANT_API_KEY")

    if not url:
        # ë¡œì»¬ í´ë°±
        db = LegalVectorDB(host='localhost', port=6333)
    else:
        db = LegalVectorDB(url=url, api_key=key)

    # 3. ì—…ë¡œë“œ
    # ê¸°ì¡´ ë°ì´í„° í™•ì¸
    try:
        current_count = db.client.count(collection_name=COLLECTION_NAME).count
        print(f"ğŸ“Š í˜„ì¬ ì»¬ë ‰ì…˜ ë°ì´í„° ìˆ˜: {current_count}ê°œ")
    except:
        current_count = 0
        print("âš ï¸ ì»¬ë ‰ì…˜ì´ ì—†ê±°ë‚˜ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")

    if current_count > 0:
        print(f"ğŸ”„ ì´ì–´í•˜ê¸° ëª¨ë“œ: {current_count}ë²ˆ ì¸ë±ìŠ¤ë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.")
        # recreate=Falseë¡œ ì„¤ì •í•˜ì—¬ ê¸°ì¡´ ë°ì´í„° ìœ ì§€
        db.create_collection(COLLECTION_NAME, recreate=False)

        # ì´ë¯¸ ì—…ë¡œë“œëœ ë¶„ëŸ‰ë§Œí¼ ê±´ë„ˆë›°ê³  ë‚˜ë¨¸ì§€ ì—…ë¡œë“œ
        if current_count < len(chunks):
            remaining_chunks = chunks[current_count:]
            # start_idë¥¼ current_countë¡œ ì„¤ì •í•˜ì—¬ ID ì¶©ëŒ ë°©ì§€
            db.upsert_chunks(COLLECTION_NAME, remaining_chunks,
                             start_id=current_count)
        else:
            print("âœ… ì´ë¯¸ ëª¨ë“  ë°ì´í„°ê°€ ì—…ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
    else:
        # ì²˜ìŒë¶€í„° ì‹œì‘
        print("ğŸš€ ìƒˆë¡œìš´ ì—…ë¡œë“œ ì‹œì‘")
        db.create_collection(COLLECTION_NAME, recreate=True)
        db.upsert_chunks(COLLECTION_NAME, chunks, start_id=0)

    # 4. ê²€ì¦
    print("\nğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸: 'í‡´ì§ê¸ˆ ì¤‘ê°„ì •ì‚°'")
    hits = db.search("í‡´ì§ê¸ˆ ì¤‘ê°„ì •ì‚°")
    for i, h in enumerate(hits, 1):
        meta = h.payload
        print(
            f"\n[{i}] {meta.get('law_name')} {meta.get('article_title')} (Score: {h.score:.3f})")
        print(f"    {meta.get('text')[:100]}...")


if __name__ == '__main__':
    main()
