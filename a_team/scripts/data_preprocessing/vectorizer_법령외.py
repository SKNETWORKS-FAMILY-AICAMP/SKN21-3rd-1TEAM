"""
ì£¼ìš”íŒì •ì‚¬ë¡€, í–‰ì •í•´ì„ ë°ì´í„° Qdrant ì—…ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ (Unified / BGE-M3)
"""
from a_team.scripts.common.vector_db import LegalVectorDB
import os
import sys
import json
from pathlib import Path
from dotenv import load_dotenv

# Common Module Import (Fix: 3 levels up)
sys.path.append(os.path.abspath(os.path.join(
    os.path.dirname(__file__), '..', '..', '..')))

# ============================================================
# ê²½ë¡œ ì„¤ì •
# ============================================================
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(SCRIPT_DIR, '..', '..', 'data')
PROCESSED_DIR = os.path.join(DATA_DIR, 'processed')

EMBEDDING_MODEL = "Qwen/Qwen3-Embedding-0.6B"
SPARSE_MODEL = "BAAI/bge-m3"

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (Project Root .env)
PROJECT_ROOT = Path(os.path.abspath(__file__)).parent.parent.parent.parent
ENV_PATH = PROJECT_ROOT / '.env'
if not ENV_PATH.exists():
    ENV_PATH = Path(os.getcwd()) / '.env'
print(f"ğŸŒ Loading .env from: {ENV_PATH}")
load_dotenv(ENV_PATH)

QDRANT_URL = os.getenv("QDRANT_URL")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")
COLLECTION_NAME = os.getenv("QDRANT_COLLECTION_NAME", "A-TEAM")


def load_json(filepath):
    with open(filepath, 'r', encoding='utf-8') as f:
        return json.load(f)


def chunk_text(text: str, chunk_size: int = 800, overlap: int = 100) -> list:
    """í…ìŠ¤íŠ¸ ì²­í‚¹"""
    if not text:
        return []

    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end]
        chunks.append(chunk)

        start = end - overlap
        if start >= len(text) - overlap:
            break

    return chunks


def main():
    print("=" * 60)
    print("âš–ï¸  ë²•ë ¹ì™¸ ë°ì´í„°(íŒë¡€/í•´ì„) Qdrant ì—…ë¡œë“œ (Hybrid)")
    print("=" * 60)

    # íƒ€ê²Ÿ íŒŒì¼ë“¤
    targets = [
        "fd_ë²•ë ¹ì™¸_ì£¼ìš”íŒì •ì‚¬ë¡€.json",
        "fd_ë²•ë ¹ì™¸_í–‰ì •í•´ì„.json",
        "fd_ë²•ë ¹ì™¸_ê³ ìš©ë…¸ë™ë¶€QA.json"
    ]

    # DB ì´ˆê¸°í™”
    db = LegalVectorDB(
        url=QDRANT_URL,
        api_key=QDRANT_API_KEY,
        dense_model_name=EMBEDDING_MODEL,
        sparse_model_name=SPARSE_MODEL
    )

    # ì»¬ë ‰ì…˜ í™•ì¸ (ì¬ìƒì„± X)
    db.create_collection(COLLECTION_NAME, recreate=False)

    # í˜„ì¬ ID ì¡°íšŒ (ì´ì–´ì“°ê¸°)
    info = db.get_collection_info(COLLECTION_NAME)
    start_id = info['points_count']
    print(f"Current Points: {start_id}")

    for filename in targets:
        filepath = os.path.join(PROCESSED_DIR, filename)
        if not os.path.exists(filepath):
            print(f"âš ï¸ íŒŒì¼ ì—†ìŒ: {filename} (Skip)")
            continue

        print(f"\nProcessing {filename}...")
        documents = load_json(filepath)

        all_chunks = []
        for doc in documents:
            text = doc.get('text', '')
            metadata = doc.get('metadata', {})

            # ì²­í‚¹
            text_chunks = chunk_text(text)

            for i, chunk_text_str in enumerate(text_chunks):
                chunk_meta = metadata.copy()
                chunk_meta['chunk_index'] = i
                all_chunks.append({
                    'text': chunk_text_str,
                    'metadata': chunk_meta
                })

        print(f"Uploading {len(all_chunks)} chunks for {filename}...")
        db.upsert_chunks(COLLECTION_NAME, all_chunks,
                         batch_size=12, start_id=start_id)

        # ID ì—…ë°ì´íŠ¸
        start_id += len(all_chunks)


if __name__ == "__main__":
    main()
