"""
ë²•ë ¹ ë°ì´í„° ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸
- Raw JSON ë°ì´í„° ë¡œë“œ (ë…¸ë™ë²•/ë¯¼ì‚¬ë²•/í˜•ì‚¬ë²•)
- í…ìŠ¤íŠ¸ ì •ì œ ë° ì²­í‚¹
- ë¶€ì¹™ ì²˜ë¦¬ (ìµœì‹  ë¶€ì¹™ë§Œ ìœ ì§€)
- ë³„í‘œ ì²˜ë¦¬ (ê´€ë ¨ ì¡°ë¬¸ ë³‘í•© ë˜ëŠ” ë…ë¦½ ì²­í¬)
- ê²°ê³¼ ì €ì¥: processed/law_chunks.json
"""

import json
import os
import re
from typing import List, Dict, Any
from pathlib import Path

# ============================================================
# ì„¤ì •
# ============================================================
SCRIPT_DIR = Path(__file__).parent
DATA_DIR = SCRIPT_DIR / '..' / '..' / 'data'
RAW_DIR = DATA_DIR / 'raw'
PROCESSED_DIR = DATA_DIR / 'processed'

# ì²­í‚¹ ì„¤ì •
CHUNK_SIZE = 1000
CHUNK_OVERLAP = 200
TABLE_MERGE_THRESHOLD = 300  # ë³„í‘œ ë³‘í•© ê¸°ì¤€ (ê¸€ì ìˆ˜)


# ============================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================================
def load_json(filepath: Path) -> Any:
    """JSON íŒŒì¼ ë¡œë“œ"""
    with open(filepath, 'r', encoding='utf-8') as f:
        return json.load(f)


def save_json(data: Any, filepath: Path):
    """JSON íŒŒì¼ ì €ì¥"""
    filepath.parent.mkdir(parents=True, exist_ok=True)
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"âœ… ì €ì¥ ì™„ë£Œ: {filepath}")


def clean_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ ì •ì œ"""
    if not text:
        return ""

    # HTML íƒœê·¸ ì œê±° (ê°„ë‹¨í•œ ì •ê·œì‹, í•„ìš”ì‹œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©)
    text = re.sub(r'<[^>]+>', '', text)

    # ê°œì • ì´ë ¥ íƒœê·¸ ê°„ì†Œí™”: <ê°œì • 2021. 1. 5.> -> [ê°œì • 2021.1.5]
    text = re.sub(r'<ê°œì •\s*([^>]+)>', r'[ê°œì • \1]', text)

    # ì—°ì† ê³µë°±/ì¤„ë°”ê¿ˆ ì •ê·œí™”
    text = re.sub(r'\n{3,}', '\n\n', text)
    text = re.sub(r' {2,}', ' ', text)

    # í—¤ë” ì •ê·œí™” (ì˜ˆ: ë¶€    ì¹™ -> ë¶€ì¹™, ë³„    í‘œ -> ë³„í‘œ)
    text = re.sub(r'ë¶€\s+ì¹™', 'ë¶€ì¹™', text)
    text = re.sub(r'ë³„\s+í‘œ', 'ë³„í‘œ', text)

    return text.strip()


def split_with_overlap(text: str, chunk_size: int, overlap: int) -> List[str]:
    """í…ìŠ¤íŠ¸ ì˜¤ë²„ë© ë¶„í• """
    if len(text) <= chunk_size:
        return [text]

    chunks = []
    start = 0

    while start < len(text):
        end = start + chunk_size

        # ë¬¸ì¥/ë‹¨ì–´ ê²½ê³„ ì²˜ë¦¬
        if end < len(text):
            for sep in ['. ', ', ', ' ', '\n']:
                last_sep = text.rfind(sep, start, end)
                if last_sep > start + chunk_size // 2:
                    end = last_sep + len(sep)
                    break

        chunk = text[start:end].strip()
        if chunk:
            chunks.append(chunk)

        start = end - overlap
        if start >= len(text) - overlap:
            break

    return chunks


# ============================================================
# ì²­í‚¹ ë¡œì§
# ============================================================
def process_law_data(law: Dict[str, Any]) -> List[Dict[str, Any]]:
    """ë‹¨ì¼ ë²•ë ¹ ë°ì´í„° ì²˜ë¦¬ (ì¡°ë¬¸, ë¶€ì¹™, ë³„í‘œ)"""
    processed_chunks = []
    law_meta = law.get('meta_info', {})

    # 1. ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
    base_meta = {
        'source': 'law',
        'law_name': law_meta.get('law_name', ''),
        'law_id': law_meta.get('law_id', ''),
        'category': law_meta.get('category', ''),
        'enforce_date': law_meta.get('enforce_date', ''),
        'revision_type': law_meta.get('revision_type', ''),
        'url': law_meta.get('url', '')
    }

    law_name = base_meta['law_name']

    # ------------------------------------------------------------
    # 2. ë³„í‘œ (Table) ì „ì²˜ë¦¬: ì¡°ë¬¸ ë³‘í•©ìš© ë§¤í•‘ ìƒì„±
    # ------------------------------------------------------------
    table_map = {}  # { article_no: [table_content, ...] }
    independent_tables = []

    for table in law.get('tables', []):
        raw_html = table.get('content_html', '')
        # HTMLì—ì„œ í…ìŠ¤íŠ¸ë§Œ ëŒ€ëµ ì¶”ì¶œ (ë‹¤ìš´ë¡œë“œ ë§í¬ ë“± ì œì™¸)
        # ì‹¤ì œë¡œëŠ” ì œëª©ì´ ê°€ì¥ ì¤‘ìš”í•¨: [ë³„í‘œ 1] ...
        # ê°„ë‹¨í•˜ê²Œ íƒœê·¸ ì œê±° í›„ ì •ì œ
        table_text = clean_text(raw_html)

        # ì œëª© ì¶”ì¶œ ì‹œë„ (raw ë°ì´í„° êµ¬ì¡°ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ, ì—¬ê¸°ì„  HTML title ì†ì„±ì´ë‚˜ í…ìŠ¤íŠ¸ ì•ë¶€ë¶„ í™œìš©)
        # ë³„í‘œ í…ìŠ¤íŠ¸ê°€ [ë³„í‘œ 1] ... ì œ50ì¡° ê´€ë ¨ ... í˜•ì‹ì„ ë¤ë‹¤ê³  ê°€ì •

        # ê´€ë ¨ ì¡°ë¬¸ ì°¾ê¸° (ì˜ˆ: "ì œ50ì¡°", "ì œ50ì¡°ì˜2")
        # ì •ê·œì‹: ì œ([0-9]+(ì˜[0-9]+)?)ì¡°
        match = re.search(r'ì œ(\d+(?:ì˜\d+)?)ì¡°', table_text)
        related_article_no = match.group(1) if match else None

        # ì¡°ê±´: ê´€ë ¨ ì¡°ë¬¸ì´ ìˆê³  + ê¸¸ì´ê°€ ì§§ìœ¼ë©´ => ë³‘í•© ëŒ€ìƒ
        if related_article_no and len(table_text) < TABLE_MERGE_THRESHOLD:
            if related_article_no not in table_map:
                table_map[related_article_no] = []
            table_map[related_article_no].append(table_text)
        else:
            # ë…ë¦½ ì²­í¬ë¡œ ì²˜ë¦¬
            independent_tables.append(table_text)

    # ------------------------------------------------------------
    # 3. ì¡°ë¬¸ (Body) ì²˜ë¦¬ - ê³„ì¸µì  ì²­í‚¹
    # ------------------------------------------------------------
    for article in law.get('body', []):
        article_no = article.get('article_no', '')
        article_title = article.get('article_title', '')
        paragraphs = article.get('paragraphs', [])

        if not paragraphs:
            continue

        # ë³„í‘œ ë‚´ìš© ë³‘í•©
        table_text = ""
        if article_no in table_map:
            merged_tables = "\n\n".join(table_map[article_no])
            table_text = f"\n\n[ê´€ë ¨ ë³„í‘œ]\n{merged_tables}"

        # í—¤ë” ìƒì„±
        context_header = f"[{law_name}] {article_title}\n\n"

        # ë©”íƒ€ë°ì´í„° êµ¬ì„±
        chunk_meta = base_meta.copy()
        chunk_meta.update({
            'article_no': article_no,
            'article_title': article_title,
            'type': 'article'
        })

        # ========== Level 1: ì¡°ë¬¸ ì „ì²´ë¥¼ 1ê°œ ì²­í¬ë¡œ ì‹œë„ ==========
        full_article_text = "\n\n".join(
            [p.get('content', '') for p in paragraphs]) + table_text
        full_text = context_header + clean_text(full_article_text)

        if len(full_text) <= CHUNK_SIZE:
            # Level 1 ì„±ê³µ: ì¡°ë¬¸ ì „ì²´ê°€ 1ê°œ ì²­í¬ì— ë“¤ì–´ê°
            processed_chunks.append({
                'text': full_text,
                'metadata': {
                    **chunk_meta,
                    'chunk_index': 0,
                    'total_chunks': 1,
                    'is_continuation': False,
                    'chunking_level': 'article'  # ì „ì²´ ì¡°ë¬¸
                }
            })
        else:
            # ========== Level 2: í•­(paragraph) ë‹¨ìœ„ë¡œ ë¶„í•  ì‹œë„ ==========
            paragraph_chunks = []
            current_chunk = ""
            current_paragraphs = []

            for para in paragraphs:
                para_content = clean_text(para.get('content', ''))
                if not para_content:
                    continue

                # í˜„ì¬ ì²­í¬ì— ì´ í•­ì„ ì¶”ê°€í–ˆì„ ë•Œì˜ ê¸¸ì´ ê³„ì‚°
                test_chunk = current_chunk + "\n\n" + \
                    para_content if current_chunk else para_content
                test_full = context_header + test_chunk + table_text

                if len(test_full) <= CHUNK_SIZE:
                    # ì¶”ê°€ ê°€ëŠ¥
                    current_chunk = test_chunk
                    current_paragraphs.append(para)
                else:
                    # í˜„ì¬ ì²­í¬ë¥¼ ì €ì¥í•˜ê³  ìƒˆë¡œìš´ ì²­í¬ ì‹œì‘
                    if current_chunk:
                        paragraph_chunks.append({
                            'text': current_chunk,
                            'paragraphs': current_paragraphs.copy()
                        })

                    # ì´ í•­ìœ¼ë¡œ ìƒˆë¡œìš´ ì²­í¬ ì‹œì‘
                    current_chunk = para_content
                    current_paragraphs = [para]

                    # ë§Œì•½ ë‹¨ì¼ í•­ë„ ë„ˆë¬´ ê¸¸ë©´ Level 3ìœ¼ë¡œ ì´ë™
                    single_para_full = context_header + para_content + table_text
                    if len(single_para_full) > CHUNK_SIZE:
                        # ========== Level 3: í˜¸(subparagraph) ë‹¨ìœ„ ë¶„í•  í•„ìš” ==========
                        # ì¼ë‹¨ í•­ ìì²´ë¥¼ ì¶”ê°€í•˜ê³ , ë‚˜ì¤‘ì— ì²˜ë¦¬
                        pass

            # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
            if current_chunk:
                paragraph_chunks.append({
                    'text': current_chunk,
                    'paragraphs': current_paragraphs.copy()
                })

            # Level 2 ì²­í¬ë“¤ì„ processed_chunksì— ì¶”ê°€
            total = len(paragraph_chunks)
            for idx, pchunk in enumerate(paragraph_chunks):
                is_cont = (idx > 0)

                # ë³„í‘œëŠ” ë§ˆì§€ë§‰ ì²­í¬ì—ë§Œ ì¶”ê°€
                chunk_table = table_text if idx == total - 1 else ""
                text_content = context_header + pchunk['text'] + chunk_table

                if is_cont:
                    text_content = f"{context_header}[ì´ì–´ì§ {idx+1}/{total}]\n{pchunk['text']}{chunk_table}"

                processed_chunks.append({
                    'text': text_content,
                    'metadata': {
                        **chunk_meta,
                        'chunk_index': idx,
                        'total_chunks': total,
                        'is_continuation': is_cont,
                        'chunking_level': 'paragraph'  # í•­ ë‹¨ìœ„
                    }
                })

    # ------------------------------------------------------------
    # 4. ë…ë¦½ ë³„í‘œ (Independent Tables) ì²˜ë¦¬
    # ------------------------------------------------------------
    for i, table_text in enumerate(independent_tables):
        # ê¸´ ë³„í‘œëŠ” ìì²´ì ìœ¼ë¡œ ì²­í‚¹
        header = f"[{law_name}] ë³„í‘œ/ì„œì‹ {i+1}\n\n"
        full_text = header + table_text

        chunk_meta = base_meta.copy()
        chunk_meta.update({
            'article_no': f"ë³„í‘œ{i+1}",
            'article_title': "ë³„í‘œ/ì„œì‹",
            'type': 'table'
        })

        if len(full_text) <= CHUNK_SIZE:
            processed_chunks.append({
                'text': full_text,
                'metadata': {**chunk_meta, 'chunk_index': 0, 'total_chunks': 1}
            })
        else:
            splits = split_with_overlap(
                table_text, CHUNK_SIZE - len(header) - 30, CHUNK_OVERLAP)
            for j, split in enumerate(splits):
                text_content = f"{header}[ì´ì–´ì§ {j+1}/{len(splits)}]\n{split}"
                processed_chunks.append({
                    'text': text_content,
                    'metadata': {
                        **chunk_meta,
                        'chunk_index': j,
                        'total_chunks': len(splits),
                        'is_continuation': j > 0
                    }
                })

    # ------------------------------------------------------------
    # 5. ë¶€ì¹™ (Addenda) ì²˜ë¦¬ - ìµœì‹  1ê°œë§Œ
    # ------------------------------------------------------------
    addenda = law.get('addenda', [])
    if addenda:
        last_addendum = addenda[-1]
        t = clean_text(last_addendum.get('content', ''))
        title = clean_text(last_addendum.get('article_title', 'ë¶€ì¹™'))

        # í—¤ë”
        header = f"[{law_name}] {title}\n\n"
        full_text = header + t

        chunk_meta = base_meta.copy()
        chunk_meta.update({
            'article_no': 'ë¶€ì¹™',
            'article_title': title,
            'type': 'addendum'
        })

        if len(full_text) <= CHUNK_SIZE:
            processed_chunks.append({
                'text': full_text,
                'metadata': {**chunk_meta, 'chunk_index': 0, 'total_chunks': 1}
            })
        else:
            splits = split_with_overlap(
                t, CHUNK_SIZE - len(header) - 30, CHUNK_OVERLAP)
            for j, split in enumerate(splits):
                text_content = f"{header}[ì´ì–´ì§ {j+1}/{len(splits)}]\n{split}"
                processed_chunks.append({
                    'text': text_content,
                    'metadata': {
                        **chunk_meta,
                        'chunk_index': j,
                        'total_chunks': len(splits),
                        'is_continuation': j > 0
                    }
                })

    return processed_chunks


# ============================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================
def main():
    print("ğŸš€ ë²•ë ¹ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...")

    all_chunks = []

    # Raw ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  law JSON íŒŒì¼ ì°¾ê¸°
    law_files = list(RAW_DIR.glob('rd_ë²•ë ¹_*.json')) + \
        list(RAW_DIR.glob('rd_*.json'))
    # ì¤‘ë³µ ì œê±° ë° ë²•ë ¹ì™¸ íŒŒì¼ ì œì™¸
    law_files = [f for f in set(law_files) if 'ë²•ë ¹ì™¸' not in f.name]

    if not law_files:
        print("âš ï¸ Raw ë””ë ‰í† ë¦¬ì— ë²•ë ¹ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    for filepath in sorted(law_files):
        print(f"ğŸ“‚ ì²˜ë¦¬ ì¤‘: {filepath.name}")
        data = load_json(filepath)

        count = 0
        for law in data:
            chunks = process_law_data(law)
            all_chunks.extend(chunks)
            count += 1

        print(f"   - ë²•ë ¹ ìˆ˜: {count}, ìƒì„±ëœ ì²­í¬: {len(chunks)} (ë§ˆì§€ë§‰ ë²•ë ¹ ê¸°ì¤€)")

    print(f"\nğŸ“Š ì´ ì²­í¬ ìˆ˜: {len(all_chunks)}")

    # ê²°ê³¼ ì €ì¥
    out_path = PROCESSED_DIR / 'fd_ë²•ë ¹_chunked.json'
    save_json(all_chunks, out_path)


if __name__ == '__main__':
    main()
