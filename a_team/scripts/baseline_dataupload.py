import json
from pathlib import Path
from dotenv import load_dotenv
from tqdm import tqdm

from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams
from langchain_qdrant import QdrantVectorStore
from langchain_huggingface import HuggingFaceEmbeddings

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
_DOTENV_PATH = Path(__file__).with_name(".env")
load_dotenv(dotenv_path=_DOTENV_PATH)


def load_labor_law_data(file_path):
    """ë…¸ë™ë²• ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError) as e:
        print(f"  âŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ({file_path.name}): {e}")
        return [], []
    
    documents = []
    metadatas = []
    
    for law in tqdm(data, desc=f"ì²˜ë¦¬ ì¤‘: {file_path.name}"):
        title = law.get('title', '')
        category = law.get('category', 'ë…¸ë™ë²•')
        url = law.get('url', '')
        
        for article in law.get('articles', []):
            article_num = article.get('article_num', '')
            content = article.get('content', '').strip()
            
            if content:
                # í…ìŠ¤íŠ¸ êµ¬ì„±: [ë²•ë ¹ëª…] ì¡°í•­ ë²ˆí˜¸\në³¸ë¬¸
                text = f"[{title}] {article_num}\n{content}"
                documents.append(text)
                metadatas.append({
                    'source': 'labor_law',
                    'title': title,
                    'article_num': article_num,
                    'category': category,
                    'url': url
                })
    
    return documents, metadatas


def load_case_law_data(file_path):
    """íŒë¡€ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError) as e:
        print(f"  âŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ({file_path.name}): {e}")
        return [], []
    
    documents = []
    metadatas = []
    
    for case in tqdm(data, desc=f"ì²˜ë¦¬ ì¤‘: {file_path.name}"):
        ì œëª© = case.get('ì œëª©', '')
        ìë£Œêµ¬ë¶„ = case.get('ìë£Œêµ¬ë¶„', '')
        íŒì •ì‚¬í•­ = case.get('íŒì •ì‚¬í•­', '').strip()
        íŒì •ìš”ì§€ = case.get('íŒì •ìš”ì§€', '').strip()
        
        if íŒì •ì‚¬í•­ or íŒì •ìš”ì§€:
            # í…ìŠ¤íŠ¸ êµ¬ì„±: [íŒë¡€] ì œëª©\níŒì •ì‚¬í•­\níŒì •ìš”ì§€
            text_parts = [f"[íŒë¡€: {ì œëª©}]"]
            if íŒì •ì‚¬í•­:
                text_parts.append(f"íŒì •ì‚¬í•­: {íŒì •ì‚¬í•­}")
            if íŒì •ìš”ì§€:
                text_parts.append(f"íŒì •ìš”ì§€: {íŒì •ìš”ì§€}")
            
            text = "\n".join(text_parts)
            documents.append(text)
            metadatas.append({
                'source': 'case_law',
                'title': ì œëª©,
                'category': ìë£Œêµ¬ë¶„,
                'number': case.get('ë²ˆí˜¸', ''),
                'reg_date': case.get('ë“±ë¡ì¼', '')
            })
    
    return documents, metadatas


def load_interpretation_data(file_path):
    """í–‰ì •í•´ì„ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError) as e:
        print(f"  âŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ({file_path.name}): {e}")
        return [], []
    
    documents = []
    metadatas = []
    
    for item in tqdm(data, desc=f"ì²˜ë¦¬ ì¤‘: {file_path.name}"):
        title = item.get('title', '').strip()
        url = item.get('url', '')
        department = item.get('department', '')
        
        if title:
            # í…ìŠ¤íŠ¸ êµ¬ì„±: [í–‰ì •í•´ì„] ì œëª©
            text = f"[í–‰ì •í•´ì„] {title}"
            documents.append(text)
            metadatas.append({
                'source': 'interpretation',
                'title': title,
                'url': url,
                'department': department,
                'number': item.get('number', ''),
                'reg_date': item.get('reg_date', '')
            })
    
    return documents, metadatas


def main():
    """ë°ì´í„° ì—…ë¡œë“œ ë©”ì¸ í•¨ìˆ˜"""
    
    print("\n" + "="*60)
    print("ğŸš€ ë²•ë¥  ë°ì´í„° Qdrant ì—…ë¡œë“œ ì‹œì‘")
    print("="*60 + "\n")
    
    # 1. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
    print("ğŸ“¥ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘ (Qwen/Qwen3-Embedding-0.6B)...")
    embeddings = HuggingFaceEmbeddings(
        model_name="Qwen/Qwen3-Embedding-0.6B",
        model_kwargs={'trust_remote_code': True},
        encode_kwargs={'normalize_embeddings': True}
    )
    print("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\n")
    
    # 2. ë¡œì»¬ Qdrant í´ë¼ì´ì–¸íŠ¸ ì—°ê²°
    print("ğŸ“¡ ë¡œì»¬ Qdrant ì—°ê²° ì¤‘...")
    try:
        client = QdrantClient(host="localhost", port=6333)
        # ì—°ê²° í…ŒìŠ¤íŠ¸
        client.get_collections()
        print("âœ… Qdrant ì—°ê²° ì™„ë£Œ\n")
    except Exception as e:
        print(f"âŒ Qdrant ì—°ê²° ì‹¤íŒ¨: {e}")
        print("ğŸ’¡ ë¡œì»¬ Qdrantê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”: docker run -p 6333:6333 qdrant/qdrant")
        return
    
    # 3. ì»¬ë ‰ì…˜ í™•ì¸ ë° ìƒì„±
    collection_name = "A-TEAM-local"
    embedding_dim = 1024  # Qwen3-Embedding-0.6B ì‹¤ì œ ì°¨ì›
    print(f"ğŸ” ì»¬ë ‰ì…˜ í™•ì¸ ì¤‘ ({collection_name})...")
    
    collections = [c.name for c in client.get_collections().collections]
    if collection_name in collections:
        # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì •ë³´ í™•ì¸
        collection_info = client.get_collection(collection_name)
        existing_dim = collection_info.config.params.vectors.size
        
        if existing_dim != embedding_dim:
            print(f"âš ï¸  ì°¨ì› ë¶ˆì¼ì¹˜ ê°ì§€ (ê¸°ì¡´: {existing_dim}, í•„ìš”: {embedding_dim})")
            print(f"ğŸ—‘ï¸  ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ ì¤‘...")
            client.delete_collection(collection_name)
            print(f"âœ… ì»¬ë ‰ì…˜ ì‚­ì œ ì™„ë£Œ")
            collections.remove(collection_name)
        else:
            print(f"âœ… ì»¬ë ‰ì…˜ '{collection_name}' ì¡´ì¬ í™•ì¸ (ì°¨ì›: {existing_dim})")
    
    if collection_name not in collections:
        print(f"ğŸ†• ì»¬ë ‰ì…˜ ìƒì„± ì¤‘ (ì°¨ì›: {embedding_dim})...")
        client.create_collection(
            collection_name=collection_name,
            vectors_config=VectorParams(size=embedding_dim, distance=Distance.COSINE)
        )
        print(f"âœ… ì»¬ë ‰ì…˜ '{collection_name}' ìƒì„± ì™„ë£Œ")
    print()
    
    # 4. ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
    print(f"ğŸ—‚ï¸  ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” ì¤‘...")
    vectorstore = QdrantVectorStore(
        client=client,
        collection_name=collection_name,
        embedding=embeddings,
        content_payload_key="text"
    )
    print("âœ… ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” ì™„ë£Œ\n")
    
    # 5. ë°ì´í„° íŒŒì¼ ê²½ë¡œ ì„¤ì •
    data_dir = Path(__file__).parent / "data" / "raw"
    files = {
        "labor_law": data_dir / "rd_ë…¸ë™ë²•.json",
        "case_law": data_dir / "rd_ì£¼ìš”íŒë¡€.json",
        "interpretation": data_dir / "rd_í–‰ì •í•´ì„.json"
    }
    
    # 6. ë°ì´í„° ë¡œë“œ
    all_documents = []
    all_metadatas = []
    
    print("ğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...\n")
    
    # ë…¸ë™ë²• ë°ì´í„°
    if files["labor_law"].exists():
        docs, metas = load_labor_law_data(files["labor_law"])
        all_documents.extend(docs)
        all_metadatas.extend(metas)
        print(f"  âœ… ë…¸ë™ë²•: {len(docs)}ê°œ ë¬¸ì„œ ë¡œë“œ")
    else:
        print(f"  âš ï¸  ë…¸ë™ë²• íŒŒì¼ ì—†ìŒ: {files['labor_law']}")
    
    # íŒë¡€ ë°ì´í„°
    if files["case_law"].exists():
        docs, metas = load_case_law_data(files["case_law"])
        all_documents.extend(docs)
        all_metadatas.extend(metas)
        print(f"  âœ… ì£¼ìš”íŒë¡€: {len(docs)}ê°œ ë¬¸ì„œ ë¡œë“œ")
    else:
        print(f"  âš ï¸  íŒë¡€ íŒŒì¼ ì—†ìŒ: {files['case_law']}")
    
    # í–‰ì •í•´ì„ ë°ì´í„°
    if files["interpretation"].exists():
        docs, metas = load_interpretation_data(files["interpretation"])
        all_documents.extend(docs)
        all_metadatas.extend(metas)
        print(f"  âœ… í–‰ì •í•´ì„: {len(docs)}ê°œ ë¬¸ì„œ ë¡œë“œ")
    else:
        print(f"  âš ï¸  í–‰ì •í•´ì„ íŒŒì¼ ì—†ìŒ: {files['interpretation']}")
    
    print(f"\nğŸ“Š ì´ {len(all_documents)}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")
    
    # 7. Qdrantì— ì—…ë¡œë“œ
    if all_documents:
        print("\nâ¬†ï¸  Qdrantì— ì—…ë¡œë“œ ì¤‘ (ì„ë² ë”© ìƒì„± ë° ì €ì¥)...")
        print("   â³ ëŒ€ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ì¤‘... ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n")
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì—…ë¡œë“œ (ë©”ëª¨ë¦¬ íš¨ìœ¨) - ì‘ì€ ë°°ì¹˜ë¡œ GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ë°©ì§€
        batch_size = 5
        total_batches = (len(all_documents) + batch_size - 1) // batch_size
        
        try:
            for i in range(0, len(all_documents), batch_size):
                batch_docs = all_documents[i:i+batch_size]
                batch_metas = all_metadatas[i:i+batch_size]
                
                vectorstore.add_texts(
                    texts=batch_docs,
                    metadatas=batch_metas
                )
                
                current_batch = i // batch_size + 1
                print(f"   âœ… ë°°ì¹˜ {current_batch}/{total_batches} ì—…ë¡œë“œ ì™„ë£Œ ({len(batch_docs)}ê°œ ë¬¸ì„œ)")
            
            print(f"\nâœ… ë°ì´í„° ì—…ë¡œë“œ ì™„ë£Œ!")
            print(f"   - ì´ {len(all_documents)}ê°œ ë¬¸ì„œ")
            print(f"   - ì»¬ë ‰ì…˜: {collection_name}")
            print(f"   - Qdrant: localhost:6333")
            print("\n" + "="*60)
            print("ğŸ‰ ì—…ë¡œë“œ ì„±ê³µ! ì´ì œ baseline_local.pyë¡œ ì±—ë´‡ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
            print("="*60 + "\n")
            
        except Exception as e:
            print(f"\nâŒ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print("ğŸ’¡ ì¼ë¶€ ë°ì´í„°ëŠ” ì—…ë¡œë“œë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    else:
        print("\nâŒ ì—…ë¡œë“œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()