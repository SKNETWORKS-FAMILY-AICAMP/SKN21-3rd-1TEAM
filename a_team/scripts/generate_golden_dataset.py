"""
ë…¸ë™ë²• RAG ì±—ë´‡ í‰ê°€ìš© Golden Set ìë™ ìƒì„± ìŠ¤í¬ë¦½íŠ¸ (Ragas 0.4.x)

Ragas TestsetGeneratorë¥¼ ì‚¬ìš©í•˜ì—¬ PDF/í…ìŠ¤íŠ¸ ë¬¸ì„œë¡œë¶€í„°
ë‹¤ì–‘í•œ ìœ í˜•ì˜ í‰ê°€ìš© ì§ˆë¬¸-ë‹µë³€ ìŒì„ ìë™ ìƒì„±í•©ë‹ˆë‹¤.

Tech Stack:
    - Python 3.10+
    - ragas 0.4.x
    - langchain / langchain-openai / langchain-community
    - pandas

Usage:
    # ê¸°ë³¸ ì‹¤í–‰ (OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ í•„ìš”)
    python generate_golden_dataset.py

    # ìƒì„±í•  í…ŒìŠ¤íŠ¸ì…‹ í¬ê¸° ì§€ì •
    python generate_golden_dataset.py --test-size 50

    # ë°ì´í„° í´ë” ê²½ë¡œ ì§€ì •
    python generate_golden_dataset.py --data-dir ./custom_data
"""

from ragas.testset import TestsetGenerator
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader, TextLoader
import os
import warnings
from pathlib import Path

import pandas as pd
from dotenv import load_dotenv

# .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ê²½ê³  ë©”ì‹œì§€ í•„í„°ë§ (ì„ íƒì‚¬í•­)
warnings.filterwarnings("ignore", category=DeprecationWarning)


def load_documents(data_dir: str) -> list:
    """
    ì§€ì •ëœ í´ë”ì—ì„œ JSON íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.

    ì‚¬ìš©í•˜ëŠ” íŒŒì¼:
    - raw/rd_ë…¸ë™ë²•.json, rd_ë¯¼ì‚¬ë²•.json, rd_í˜•ì‚¬ë²•.json (ë²•ë ¹)
    - processed/fd_ë²•ë ¹ì™¸_ê³ ìš©ë…¸ë™ë¶€QA.json (ê³ ìš©ë…¸ë™ë¶€ Q&A)
    - processed/fd_ë²•ë ¹ì™¸_íŒë¡€.json (íŒë¡€)
    - processed/fd_ë²•ë ¹ì™¸_í–‰ì •í•´ì„.json (í–‰ì •í•´ì„)

    Returns:
        List of LangChain Document objects
    """
    from langchain_core.documents import Document
    import json

    documents = []
    data_path = Path(data_dir)

    if not data_path.exists():
        raise FileNotFoundError(f"ë°ì´í„° í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_dir}")

    print(f"ğŸ“‚ ë¬¸ì„œ ë¡œë“œ ì¤‘: {data_dir}")

    # ==========================================================
    # 1. processed í´ë”: {text, metadata} í˜•ì‹
    # ==========================================================
    processed_files = [
        data_path / "processed" / "fd_ë²•ë ¹ì™¸_ê³ ìš©ë…¸ë™ë¶€QA.json",
        data_path / "processed" / "fd_ë²•ë ¹ì™¸_íŒë¡€.json",
        data_path / "processed" / "fd_ë²•ë ¹ì™¸_í–‰ì •í•´ì„.json",
    ]

    for filepath in processed_files:
        if not filepath.exists():
            print(f"  âš ï¸ íŒŒì¼ ì—†ìŒ: {filepath.name}")
            continue
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)

            count = 0
            for item in data:
                text = item.get('text', '')
                if text and len(text) > 30:
                    metadata = item.get('metadata', {})
                    doc = Document(
                        page_content=text,
                        metadata={
                            'source': metadata.get('source', filepath.stem),
                            'title': metadata.get('title', ''),
                            'category': metadata.get('category', ''),
                            'url': metadata.get('url', '')
                        }
                    )
                    documents.append(doc)
                    count += 1
            print(f"  âœ… {filepath.name} â†’ {count}ê°œ ë¬¸ì„œ")
        except Exception as e:
            print(f"  âš ï¸ ë¡œë“œ ì˜¤ë¥˜ ({filepath.name}): {e}")

    # ==========================================================
    # 2. raw í´ë”: ë²•ë ¹ ë°ì´í„° (ìƒˆ êµ¬ì¡°)
    #    {meta_info, body: [{article_text_full, ...}], addenda, tables}
    # ==========================================================
    law_files = [
        data_path / "raw" / "rd_ë…¸ë™ë²•.json",
        data_path / "raw" / "rd_ë¯¼ì‚¬ë²•.json",
        data_path / "raw" / "rd_í˜•ì‚¬ë²•.json",
    ]

    for filepath in law_files:
        if not filepath.exists():
            print(f"  âš ï¸ íŒŒì¼ ì—†ìŒ: {filepath.name}")
            continue
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)

            count = 0
            for law in data:
                # meta_infoì—ì„œ ë²•ë ¹ ì •ë³´ ì¶”ì¶œ
                meta_info = law.get('meta_info', {})
                law_name = meta_info.get('law_name', '')
                category = meta_info.get('category', '')

                # bodyì—ì„œ ì¡°ë¬¸ ì¶”ì¶œ
                body = law.get('body', [])
                for article in body:
                    article_text = article.get('article_text_full', '')
                    article_title = article.get('article_title', '')

                    if article_text and len(article_text) > 30:
                        text = f"[{law_name}] {article_title}\n\n{article_text}"
                        doc = Document(
                            page_content=text,
                            metadata={
                                'source': 'law',
                                'law_name': law_name,
                                'law_id': meta_info.get('law_id', ''),
                                'article_no': article.get('article_no', ''),
                                'article_title': article_title,
                                'category': category,
                                'enforce_date': meta_info.get('enforce_date', ''),
                                'revision_type': meta_info.get('revision_type', '')
                            }
                        )
                        documents.append(doc)
                        count += 1
            print(f"  âœ… {filepath.name} â†’ {count}ê°œ ì¡°ë¬¸")
        except Exception as e:
            print(f"  âš ï¸ ë²•ë ¹ ë¡œë“œ ì˜¤ë¥˜ ({filepath.name}): {e}")

    # ==========================================================
    # 3. PDF íŒŒì¼
    # ==========================================================
    try:
        pdf_loader = DirectoryLoader(
            path=str(data_path),
            glob="**/*.pdf",
            loader_cls=PyPDFLoader,
            show_progress=False,
            use_multithreading=True,
            silent_errors=True
        )
        pdf_docs = pdf_loader.load()
        if pdf_docs:
            print(f"  âœ… PDF: {len(pdf_docs)}ê°œ í˜ì´ì§€")
        documents.extend(pdf_docs)
    except Exception as e:
        pass

    print(f"\nğŸ“„ ì´ {len(documents)}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ\n")
    return documents

# ---------------------------------------------------------
# [ìˆ˜ì •] ì˜¨ë„ë¥¼ ê°•ì œë¡œ 1ë¡œ ê³ ì •í•˜ëŠ” ì»¤ìŠ¤í…€ LLM í´ë˜ìŠ¤
# (ì¼ë¶€ ëª¨ë¸ì´ temperature!=1ì„ ì§€ì›í•˜ì§€ ì•Šì„ ë•Œ ì‚¬ìš©)
# ---------------------------------------------------------


class ForceTemperature1ChatOpenAI(ChatOpenAI):
    def _generate(self, messages, stop=None, run_manager=None, **kwargs):
        if 'temperature' in kwargs:
            kwargs['temperature'] = 1
        return super()._generate(messages, stop=stop, run_manager=run_manager, **kwargs)

    async def _agenerate(self, messages, stop=None, run_manager=None, **kwargs):
        if 'temperature' in kwargs:
            kwargs['temperature'] = 1
        return await super()._agenerate(messages, stop=stop, run_manager=run_manager, **kwargs)


def setup_generator(model_name: str = "gpt-5.2") -> TestsetGenerator:
    """
    Ragas 0.4.x TestsetGeneratorë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.

    Args:
        model_name: ì‚¬ìš©í•  OpenAI ëª¨ë¸ëª… (gpt-4o, gpt-4-turbo ë“±)

    Returns:
        ì„¤ì •ëœ TestsetGenerator ì¸ìŠ¤í„´ìŠ¤
    """
    print(f"ğŸ¤– LLM ì„¤ì • ì¤‘: {model_name}")

    # ---------------------------------------------------------
    # Generator LLM ì„¤ì • (ì»¤ìŠ¤í…€ ë˜í¼ ì‚¬ìš©)
    # ---------------------------------------------------------
    # Ragasê°€ ë‚´ë¶€ì ìœ¼ë¡œ temperatureë¥¼ 0.01 ë“±ìœ¼ë¡œ ë‚®ì¶”ë ¤ í•´ë„
    # ì´ í´ë˜ìŠ¤ê°€ ê°€ë¡œì±„ì„œ 1ë¡œ ê°•ì œ ê³ ì •í•©ë‹ˆë‹¤.
    generator_llm = ForceTemperature1ChatOpenAI(
        model=model_name,
        temperature=1,
    )

    # ---------------------------------------------------------
    # Embeddings ì„¤ì •
    # ---------------------------------------------------------
    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-large"
    )

    # ---------------------------------------------------------
    # Ragas 0.4.x: TestsetGenerator.from_langchain() ì‚¬ìš©
    # ---------------------------------------------------------
    generator = TestsetGenerator.from_langchain(
        llm=generator_llm,
        embedding_model=embeddings
    )

    print("âœ… TestsetGenerator ì„¤ì • ì™„ë£Œ\n")
    return generator


def generate_testset(
    generator: TestsetGenerator,
    documents: list,
    test_size: int = 30
) -> pd.DataFrame:
    """
    ë¬¸ì„œë¡œë¶€í„° í…ŒìŠ¤íŠ¸ì…‹ì„ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        generator: ì„¤ì •ëœ TestsetGenerator
        documents: ë¡œë“œëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        test_size: ìƒì„±í•  ì§ˆë¬¸ ê°œìˆ˜

    Returns:
        ìƒì„±ëœ í…ŒìŠ¤íŠ¸ì…‹ DataFrame
    """
    print(f"ğŸ“ í…ŒìŠ¤íŠ¸ì…‹ ìƒì„± ì¤‘ (ëª©í‘œ: {test_size}ê°œ)")
    print("   ë…¸ë™ë²• íŠ¹ì„±ìƒ ì¡°ê±´ë¶€/ì¶”ë¡  ì§ˆë¬¸ì´ ìë™ìœ¼ë¡œ ë§ì´ ìƒì„±ë©ë‹ˆë‹¤.")
    print()

    # ---------------------------------------------------------
    # Ragas 0.4.x: generate_with_langchain_docs ë©”ì„œë“œ ì‚¬ìš©
    # ---------------------------------------------------------
    testset = generator.generate_with_langchain_docs(
        documents=documents,
        testset_size=test_size,
        raise_exceptions=False,
    )

    # DataFrameìœ¼ë¡œ ë³€í™˜
    df = testset.to_pandas()

    print(f"\nâœ… {len(df)}ê°œ ì§ˆë¬¸-ë‹µë³€ ìŒ ìƒì„± ì™„ë£Œ")
    return df


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse

    # ---------------------------------------------------------
    # CLI ì¸ì íŒŒì‹±
    # ---------------------------------------------------------
    parser = argparse.ArgumentParser(
        description='ë…¸ë™ë²• RAG í‰ê°€ìš© Golden Set ìƒì„± (Ragas 0.4.x TestsetGenerator)'
    )
    parser.add_argument(
        '--data-dir',
        type=str,
        default='../data',
        help='PDF/TXT ë¬¸ì„œê°€ ìˆëŠ” í´ë” ê²½ë¡œ (ê¸°ë³¸ê°’: ../data)'
    )
    parser.add_argument(
        '--test-size',
        type=int,
        default=30,
        help='ìƒì„±í•  í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ê°œìˆ˜ (ê¸°ë³¸ê°’: 30)'
    )
    parser.add_argument(
        '--sample-size',
        type=int,
        default=200,
        help='ì‚¬ìš©í•  ë¬¸ì„œ ìƒ˜í”Œë§ ê°œìˆ˜ (0ì´ë©´ ì „ì²´ ì‚¬ìš©, ê¸°ë³¸ê°’: 200)'
    )
    parser.add_argument(
        '--model',
        type=str,
        default='gpt-5.2',
        help='ì‚¬ìš©í•  LLM ëª¨ë¸ (ê¸°ë³¸ê°’: gpt-5.2)'
    )
    parser.add_argument(
        '--output',
        type=str,
        default='labor_law_golden_set.csv',
        help='ì¶œë ¥ íŒŒì¼ëª… (ê¸°ë³¸ê°’: labor_law_golden_set.csv)'
    )
    args = parser.parse_args()

    # ---------------------------------------------------------
    # API í‚¤ í™•ì¸
    # ---------------------------------------------------------
    if not os.environ.get("OPENAI_API_KEY"):
        print("âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        print("   export OPENAI_API_KEY='your-api-key'")
        return

    print("=" * 60)
    print("ğŸ›ï¸  ë…¸ë™ë²• RAG í‰ê°€ìš© Golden Set ìƒì„±ê¸° (Ragas 0.4.x)")
    print("=" * 60)
    print()

    # ---------------------------------------------------------
    # Step 1: ë¬¸ì„œ ë¡œë“œ
    # ---------------------------------------------------------
    documents = load_documents(args.data_dir)

    if not documents:
        print("âŒ ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„° í´ë”ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return

    # ---------------------------------------------------------
    # [ì¶”ê°€] ê³¼ë„í•œ ë¹„ìš©/ì‹œê°„ ë°©ì§€ë¥¼ ìœ„í•œ ëœë¤ ìƒ˜í”Œë§
    # ---------------------------------------------------------
    if args.sample_size > 0 and len(documents) > args.sample_size:
        import random
        print(
            f"âœ‚ï¸  ë¬¸ì„œê°€ ë„ˆë¬´ ë§ì•„ {args.sample_size}ê°œë¡œ ìƒ˜í”Œë§í•©ë‹ˆë‹¤. (ì „ì²´: {len(documents)}ê°œ)")
        documents = random.sample(documents, args.sample_size)

    # ---------------------------------------------------------
    # Step 2: Generator ì„¤ì •
    # ---------------------------------------------------------
    generator = setup_generator(args.model)

    # ---------------------------------------------------------
    # Step 3: í…ŒìŠ¤íŠ¸ì…‹ ìƒì„±
    # ---------------------------------------------------------
    df = generate_testset(
        generator=generator,
        documents=documents,
        test_size=args.test_size
    )

    # ---------------------------------------------------------
    # Step 4: ê²°ê³¼ ì €ì¥
    # ---------------------------------------------------------
    script_dir = Path(__file__).parent
    output_dir = script_dir.parent / "data" / "evaluation"
    output_dir.mkdir(parents=True, exist_ok=True)

    output_path = output_dir / args.output
    df.to_csv(output_path, index=False, encoding='utf-8-sig')

    print(f"\nğŸ’¾ ì €ì¥ ì™„ë£Œ: {output_path}")

    # ---------------------------------------------------------
    # Step 5: ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
    # ---------------------------------------------------------
    print("\n" + "=" * 60)
    print("ğŸ“Š ìƒì„±ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
    print("=" * 60)
    print(f"\nì»¬ëŸ¼: {list(df.columns)}")

    print("\nìƒ˜í”Œ ì§ˆë¬¸ 3ê°œ:")
    for i, row in df.head(3).iterrows():
        q = row.get('user_input', row.get('question', 'N/A'))
        a = row.get('reference', row.get('ground_truth', 'N/A'))
        print(f"\n[{i+1}] Q: {str(q)[:80]}...")
        print(f"    A: {str(a)[:80]}...")


if __name__ == '__main__':
    main()
