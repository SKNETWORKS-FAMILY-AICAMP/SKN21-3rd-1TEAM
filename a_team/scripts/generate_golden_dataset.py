"""
ë…¸ë™ë²• RAG ì±—ë´‡ í‰ê°€ìš© Golden Set ìë™ ìƒì„± ìŠ¤í¬ë¦½íŠ¸ (Ragas 0.4.x)

Ragas TestsetGeneratorë¥¼ ì‚¬ìš©í•˜ì—¬ PDF/í…ìŠ¤íŠ¸ ë¬¸ì„œë¡œë¶€í„°
ë‹¤ì–‘í•œ ìœ í˜•ì˜ í‰ê°€ìš© ì§ˆë¬¸-ë‹µë³€ ìŒì„ ìë™ ìƒì„±í•©ë‹ˆë‹¤.

Tech Stack:
    - Python 3.10+
    - ragas 0.4.x
    - langchain / langchain-openai / langchain-community
    - pandas

Usage:
    # ê¸°ë³¸ ì‹¤í–‰ (OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ í•„ìš”)
    python generate_golden_dataset.py

    # ìƒì„±í•  í…ŒìŠ¤íŠ¸ì…‹ í¬ê¸° ì§€ì •
    python generate_golden_dataset.py --test-size 50

    # ë°ì´í„° í´ë” ê²½ë¡œ ì§€ì •
    python generate_golden_dataset.py --data-dir ./custom_data
"""

from ragas.testset import TestsetGenerator
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader, TextLoader
import os
import warnings
from pathlib import Path

import pandas as pd
from dotenv import load_dotenv

# .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ê²½ê³  ë©”ì‹œì§€ í•„í„°ë§ (ì„ íƒì‚¬í•­)
warnings.filterwarnings("ignore", category=DeprecationWarning)


def load_documents_from_qdrant(
    collection_name: str = None,
    limit: int = 0
) -> list:
    """
    Qdrant DBì—ì„œ ì²­í‚¹ëœ ë¬¸ì„œë¥¼ ì§ì ‘ ê°€ì ¸ì˜µë‹ˆë‹¤.

    ì‹¤ì œ ë²¡í„° DBì— ì €ì¥ëœ ì²­í¬ë¥¼ ì‚¬ìš©í•˜ì—¬ Golden Setì„ ë§Œë“¤ë©´
    Context Precision/Recall í‰ê°€ê°€ ì •í™•í•´ì§‘ë‹ˆë‹¤.

    Args:
        collection_name: Qdrant ì»¬ë ‰ì…˜ ì´ë¦„ (ê¸°ë³¸ê°’: í™˜ê²½ë³€ìˆ˜ QDRANT_COLLECTION_NAME)
        limit: ê°€ì ¸ì˜¬ ìµœëŒ€ ë¬¸ì„œ ìˆ˜ (0ì´ë©´ ì „ì²´)

    Returns:
        List of LangChain Document objects
    """
    from langchain_core.documents import Document
    from qdrant_client import QdrantClient

    # ìŠ¤í¬ë¦½íŠ¸ ë””ë ‰í† ë¦¬ì˜ .env íŒŒì¼ ë¡œë“œ (ê°•ì œ)
    script_dir = Path(__file__).parent
    load_dotenv(script_dir / ".env", override=True)

    # í™˜ê²½ë³€ìˆ˜ì—ì„œ Qdrant ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    qdrant_url = os.getenv("QDRANT_URL")
    qdrant_api_key = os.getenv("QDRANT_API_KEY")
    collection = collection_name or os.getenv(
        "QDRANT_COLLECTION_NAME", "A-TEAM")

    print(f"ğŸ“‚ Qdrant DBì—ì„œ ë¬¸ì„œ ë¡œë“œ ì¤‘...")
    print(f"   Collection: {collection}")

    # Qdrant í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    if qdrant_url and qdrant_api_key:
        print(f"   URL: {qdrant_url[:30]}...")
        client = QdrantClient(
            url=qdrant_url,
            api_key=qdrant_api_key,
            timeout=60
        )
    else:
        # ë¡œì»¬ Docker Qdrant
        print("   Local Docker: localhost:6333")
        client = QdrantClient(host="localhost", port=6333)

    # ì»¬ë ‰ì…˜ ì •ë³´ í™•ì¸
    try:
        collection_info = client.get_collection(collection_name=collection)
        total_points = collection_info.points_count
        print(f"   ì´ í¬ì¸íŠ¸ ìˆ˜: {total_points}")
    except Exception as e:
        raise ConnectionError(f"Qdrant ì»¬ë ‰ì…˜ '{collection}'ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")

    # ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸° (ìŠ¤í¬ë¡¤ API ì‚¬ìš©)
    # ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸° (ìŠ¤í¬ë¡¤ API ì‚¬ìš©)
    # ë‹¤ì–‘ì„±ì„ ìœ„í•´ ì—¬ëŸ¬ êµ¬ê°„ì—ì„œ ì¡°ê¸ˆì”© ê°€ì ¸ì˜¤ëŠ” ì „ëµ ì‚¬ìš©
    documents = []

    if limit > 0 and total_points > limit:
        # ë¶„í•  ê°€ì ¸ì˜¤ê¸° ì„¤ì •
        num_partitions = 10  # 10êµ°ë°ì—ì„œ ë‚˜ëˆ ì„œ ê°€ì ¸ì˜´
        limit_per_partition = max(1, limit // num_partitions)

        print(
            f"\n   ğŸ² ë‹¤ì–‘ì„± í™•ë³´ë¥¼ ìœ„í•´ {num_partitions}ê°œ êµ¬ê°„ì—ì„œ ê° {limit_per_partition}ê°œì”© ëœë¤ ìƒ˜í”Œë§í•©ë‹ˆë‹¤.")

        # ëœë¤ ì‹œì‘ ìœ„ì¹˜ë“¤ ìƒì„± (ê²¹ì¹˜ì§€ ì•Šê²Œ ì •ë ¬)
        import random
        max_start = max(0, total_points - limit_per_partition - 1)
        start_offsets = sorted([random.randint(0, max_start)
                               for _ in range(num_partitions)])

        for i, start_offset in enumerate(start_offsets):
            # Qdrantì—ì„œ í•´ë‹¹ ìœ„ì¹˜ì˜ í¬ì¸íŠ¸ ê°€ì ¸ì˜¤ê¸°
            results = client.scroll(
                collection_name=collection,
                limit=limit_per_partition,
                offset=start_offset,
                with_payload=True,
                with_vectors=False
            )

            points, _ = results

            # ë¬¸ì„œ ë³€í™˜ ë° ì¶”ê°€
            chunk_docs = []
            for point in points:
                payload = point.payload or {}
                text = payload.get("text", "")

                if text and len(text) > 30:
                    doc = Document(
                        page_content=text,
                        metadata={
                            "id": str(point.id),
                            "source": payload.get("source", ""),
                            "law_name": payload.get("law_name", ""),
                            "law_id": payload.get("law_id", ""),
                            "article_no": payload.get("article_no", ""),
                            "article_title": payload.get("article_title", ""),
                            "paragraph_no": payload.get("paragraph_no", ""),
                            "chunk_type": payload.get("chunk_type", ""),
                            "category": payload.get("category", ""),
                            "chunk_index": payload.get("chunk_index", 0),
                        }
                    )
                    chunk_docs.append(doc)

            print(
                f"      [{i+1}/{num_partitions}] Offset {start_offset} ~ : {len(chunk_docs)}ê°œ ë¡œë“œ")
            documents.extend(chunk_docs)

            # ëª©í‘œ ìˆ˜ëŸ‰ì´ ì±„ì›Œì§€ë©´ ì¤‘ë‹¨ (í˜¹ì‹œ ëª¨ë¥¼ ì˜¤ë²„í—¤ë“œ ë°©ì§€)
            if len(documents) >= limit:
                break

        # ë¦¬ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ì–´ì§€ë©´ ìë¥´ê¸°
        if len(documents) > limit:
            documents = documents[:limit]

    else:
        # ì „ì²´ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ë°ì´í„°ê°€ ì ì„ ë•Œ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        offset = None
        batch_size = 100

        while True:
            # Qdrantì—ì„œ ë°°ì¹˜ë¡œ í¬ì¸íŠ¸ ê°€ì ¸ì˜¤ê¸°
            results = client.scroll(
                collection_name=collection,
                limit=batch_size,
                offset=offset,
                with_payload=True,
                with_vectors=False  # ë²¡í„°ëŠ” í•„ìš” ì—†ìŒ
            )

            points, next_offset = results

            if not points:
                break

            for point in points:
                payload = point.payload or {}
                text = payload.get("text", "")

                if text and len(text) > 30:
                    doc = Document(
                        page_content=text,
                        metadata={
                            "id": str(point.id),
                            "source": payload.get("source", ""),
                            "law_name": payload.get("law_name", ""),
                            "law_id": payload.get("law_id", ""),
                            "article_no": payload.get("article_no", ""),
                            "article_title": payload.get("article_title", ""),
                            "paragraph_no": payload.get("paragraph_no", ""),
                            "chunk_type": payload.get("chunk_type", ""),
                            "category": payload.get("category", ""),
                            "chunk_index": payload.get("chunk_index", 0),
                        }
                    )
                    documents.append(doc)

            # ë‹¤ìŒ ë°°ì¹˜
            offset = next_offset

            # limitì´ ì§€ì •ë˜ì–´ ìˆê³  ë„ë‹¬í–ˆìœ¼ë©´ ì¤‘ë‹¨
            if limit > 0 and len(documents) >= limit:
                documents = documents[:limit]
                break

            if next_offset is None:
                break

    # ì†ŒìŠ¤ë³„ í†µê³„ ì¶œë ¥
    source_counts = {}
    for doc in documents:
        src = doc.metadata.get("source", "unknown")
        source_counts[src] = source_counts.get(src, 0) + 1

    print(f"\n   ğŸ“Š ì†ŒìŠ¤ë³„ ë¬¸ì„œ ìˆ˜:")
    for src, count in sorted(source_counts.items()):
        print(f"      â€¢ {src}: {count}ê°œ")

    print(f"\nğŸ“„ ì´ {len(documents)}ê°œ ì²­í‚¹ëœ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ\n")
    return documents

# ---------------------------------------------------------
# [ìˆ˜ì •] ì˜¨ë„ë¥¼ ê°•ì œë¡œ 1ë¡œ ê³ ì •í•˜ëŠ” ì»¤ìŠ¤í…€ LLM í´ë˜ìŠ¤
# (ì¼ë¶€ ëª¨ë¸ì´ temperature!=1ì„ ì§€ì›í•˜ì§€ ì•Šì„ ë•Œ ì‚¬ìš©)
# ---------------------------------------------------------


# ---------------------------------------------------------
# [ìˆ˜ì •] ì˜¨ë„ë¥¼ 1ë¡œ ê³ ì •í•˜ê³ , í•œêµ­ì–´ ì¶œë ¥ì„ ê°•ì œí•˜ëŠ” ì»¤ìŠ¤í…€ LLM
# ---------------------------------------------------------
class KoreanForceChatOpenAI(ChatOpenAI):
    def _generate(self, messages, stop=None, run_manager=None, **kwargs):
        from langchain_core.messages import HumanMessage

        # í•œêµ­ì–´ ê°•ì œ ì§€ì¹¨ ì¶”ê°€ (ë§ˆì§€ë§‰ì— ì¶”ê°€í•˜ì—¬ ê°€ì¥ ë†’ì€ ìš°ì„ ìˆœìœ„ ë¶€ì—¬)
        korean_instruction = "IMPORTANT: You must generate ALL outputs (Questions, Answers, Reasoning, Scenarios) in Korean (í•œêµ­ì–´). Do not use English."
        messages.append(HumanMessage(content=korean_instruction))

        if 'temperature' in kwargs:
            kwargs['temperature'] = 1

        return super()._generate(messages, stop=stop, run_manager=run_manager, **kwargs)

    async def _agenerate(self, messages, stop=None, run_manager=None, **kwargs):
        from langchain_core.messages import HumanMessage

        korean_instruction = "IMPORTANT: You must generate ALL outputs (Questions, Answers, Reasoning, Scenarios) in Korean (í•œêµ­ì–´). Do not use English."
        messages.append(HumanMessage(content=korean_instruction))

        if 'temperature' in kwargs:
            kwargs['temperature'] = 1

        return await super()._agenerate(messages, stop=stop, run_manager=run_manager, **kwargs)


def setup_generator(model_name: str = "gpt-5.2") -> TestsetGenerator:
    """
    Ragas 0.4.x TestsetGeneratorë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.

    Args:
        model_name: ì‚¬ìš©í•  OpenAI ëª¨ë¸ëª… (gpt-4o, gpt-4-turbo ë“±)

    Returns:
        ì„¤ì •ëœ TestsetGenerator ì¸ìŠ¤í„´ìŠ¤
    """
    print(f"ğŸ¤– LLM ì„¤ì • ì¤‘: {model_name} (í•œêµ­ì–´ ê°•ì œ ì ìš©)")

    # ---------------------------------------------------------
    # Generator LLM ì„¤ì • (ì»¤ìŠ¤í…€ ë˜í¼ ì‚¬ìš©)
    # ì…ë ¥ ë¬¸ì„œê°€ í•œêµ­ì–´ì´ë¯€ë¡œ ì¶œë ¥ë„ í•œêµ­ì–´ë¡œ ìƒì„±ë¨
    # ---------------------------------------------------------
    generator_llm = KoreanForceChatOpenAI(
        model=model_name,
        temperature=1,
    )

    # ---------------------------------------------------------
    # Embeddings ì„¤ì •
    # ---------------------------------------------------------
    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-large"
    )

    # ---------------------------------------------------------
    # Ragas 0.4.x: TestsetGenerator.from_langchain() ì‚¬ìš©
    # [ìˆ˜ì •] NERExtractor ì—ëŸ¬ íšŒí”¼ë¥¼ ìœ„í•´ transformsë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì§€ì •
    # ---------------------------------------------------------
    from ragas.testset.transforms import KeyphraseExtractor, SummaryExtractor

    # ì‚¬ìš©í•  Transform ì •ì˜ (NER ì œì™¸)
    # NERExtractorê°€ Pydantic output parser ì—ëŸ¬ë¥¼ ìœ ë°œí•˜ë¯€ë¡œ ì œì™¸í•¨
    transforms = [
        KeyphraseExtractor(llm=generator_llm),
        SummaryExtractor(llm=generator_llm),
    ]

    generator = TestsetGenerator.from_langchain(
        llm=generator_llm,
        embedding_model=embeddings
    )

    # [ì¤‘ìš”] Default transformsë¥¼ ì»¤ìŠ¤í…€ transformsë¡œ êµì²´
    generator.knowledge_graph.transforms = transforms

    print("âœ… TestsetGenerator ì„¤ì • ì™„ë£Œ\n")
    return generator


def generate_testset(
    generator: TestsetGenerator,
    documents: list,
    test_size: int = 30
) -> pd.DataFrame:
    """
    ë¬¸ì„œë¡œë¶€í„° í…ŒìŠ¤íŠ¸ì…‹ì„ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        generator: ì„¤ì •ëœ TestsetGenerator
        documents: ë¡œë“œëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        test_size: ìƒì„±í•  ì§ˆë¬¸ ê°œìˆ˜

    Returns:
        ìƒì„±ëœ í…ŒìŠ¤íŠ¸ì…‹ DataFrame
    """
    from ragas.run_config import RunConfig

    print(f"ğŸ“ í…ŒìŠ¤íŠ¸ì…‹ ìƒì„± ì¤‘ (ëª©í‘œ: {test_size}ê°œ)")
    print("   ë…¸ë™ë²• íŠ¹ì„±ìƒ ì¡°ê±´ë¶€/ì¶”ë¡  ì§ˆë¬¸ì´ ìë™ìœ¼ë¡œ ë§ì´ ìƒì„±ë©ë‹ˆë‹¤.")
    print()

    # ---------------------------------------------------------
    # RunConfig: ì—ëŸ¬ ì‹œ ì¬ì‹œë„ ë° ì˜ˆì™¸ ë¬´ì‹œ ì„¤ì •
    # ---------------------------------------------------------
    run_config = RunConfig(
        max_retries=3,           # ì‹¤íŒ¨ ì‹œ ìµœëŒ€ 3íšŒ ì¬ì‹œë„
        max_wait=60,             # ì¬ì‹œë„ ê°„ ìµœëŒ€ ëŒ€ê¸° ì‹œê°„
        max_workers=4,           # ë™ì‹œ ì²˜ë¦¬ ìˆ˜ ì œí•œ
        timeout=120,             # ê°œë³„ ì‘ì—… íƒ€ì„ì•„ì›ƒ
        exception_types=(Exception,),  # ëª¨ë“  ì˜ˆì™¸ ì¬ì‹œë„
    )

    # ---------------------------------------------------------
    # Ragas 0.4.x: generate_with_langchain_docs ë©”ì„œë“œ ì‚¬ìš©
    # [ìˆ˜ì •] distributions ëª…ì‹œ
    # ---------------------------------------------------------
    from ragas.testset.evolutions import simple, reasoning, multi_context
    dist = {
        simple: 0.5,
        reasoning: 0.3,
        multi_context: 0.2
    }

    try:
        testset = generator.generate_with_langchain_docs(
            documents=documents,
            testset_size=test_size,
            distributions=dist,
            raise_exceptions=False,
            run_config=run_config,
        )
    except Exception as e:
        print(f"\nâš ï¸ í…ŒìŠ¤íŠ¸ì…‹ ìƒì„± ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        print("   ì¼ë¶€ ë¬¸ì„œì—ì„œ íŒŒì‹± ì‹¤íŒ¨. ìƒ˜í”Œ ì‚¬ì´ì¦ˆë¥¼ ì¤„ì´ê±°ë‚˜ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
        raise

    # DataFrameìœ¼ë¡œ ë³€í™˜
    df = testset.to_pandas()

    print(f"\nâœ… {len(df)}ê°œ ì§ˆë¬¸-ë‹µë³€ ìŒ ìƒì„± ì™„ë£Œ")
    return df


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse

    # ---------------------------------------------------------
    # CLI ì¸ì íŒŒì‹±
    # ---------------------------------------------------------
    parser = argparse.ArgumentParser(
        description='ë…¸ë™ë²• RAG í‰ê°€ìš© Golden Set ìƒì„± (Ragas 0.4.x TestsetGenerator)'
    )
    parser.add_argument(
        '--collection',
        type=str,
        default=None,
        help='Qdrant ì»¬ë ‰ì…˜ ì´ë¦„ (ê¸°ë³¸ê°’: í™˜ê²½ë³€ìˆ˜ QDRANT_COLLECTION_NAME)'
    )
    parser.add_argument(
        '--test-size',
        type=int,
        default=30,
        help='ìƒì„±í•  í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ê°œìˆ˜ (ê¸°ë³¸ê°’: 30)'
    )
    parser.add_argument(
        '--sample-size',
        type=int,
        default=200,
        help='ì‚¬ìš©í•  ë¬¸ì„œ ìƒ˜í”Œë§ ê°œìˆ˜ (0ì´ë©´ ì „ì²´ ì‚¬ìš©, ê¸°ë³¸ê°’: 200)'
    )
    parser.add_argument(
        '--model',
        type=str,
        default='gpt-4o-mini',
        help='ì‚¬ìš©í•  LLM ëª¨ë¸ (ê¸°ë³¸ê°’: gpt-4o-mini)'
    )
    parser.add_argument(
        '--output',
        type=str,
        default='labor_law_golden_set.json',
        help='ì¶œë ¥ íŒŒì¼ëª… (ê¸°ë³¸ê°’: labor_law_golden_set.json)'
    )
    args = parser.parse_args()

    # ---------------------------------------------------------
    # API í‚¤ í™•ì¸
    # ---------------------------------------------------------
    if not os.environ.get("OPENAI_API_KEY"):
        print("âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        print("   export OPENAI_API_KEY='your-api-key'")
        return

    print("=" * 60)
    print("ğŸ›ï¸  ë…¸ë™ë²• RAG í‰ê°€ìš© Golden Set ìƒì„±ê¸° (Ragas 0.4.x)")
    print("=" * 60)
    print()

    # ---------------------------------------------------------
    # Step 1: ë¬¸ì„œ ë¡œë“œ
    # ---------------------------------------------------------
    documents = load_documents_from_qdrant(
        collection_name=args.collection,
        limit=args.sample_size
    )

    if not documents:
        print("âŒ ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. Qdrant ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return

    # ---------------------------------------------------------
    # Step 2: Generator ì„¤ì •
    # ---------------------------------------------------------
    generator = setup_generator(args.model)

    # ---------------------------------------------------------
    # Step 3: í…ŒìŠ¤íŠ¸ì…‹ ìƒì„±
    # ---------------------------------------------------------
    df = generate_testset(
        generator=generator,
        documents=documents,
        test_size=args.test_size
    )

    # ---------------------------------------------------------
    # Step 4: ê²°ê³¼ ì €ì¥
    # ---------------------------------------------------------
    script_dir = Path(__file__).parent
    output_dir = script_dir.parent / "data" / "evaluation"
    output_dir.mkdir(parents=True, exist_ok=True)

    output_path = output_dir / args.output

    # ê¸°ì¡´ íŒŒì¼ì´ ìˆìœ¼ë©´ ë¡œë“œí•´ì„œ ë³‘í•©
    if output_path.exists():
        try:
            existing_df = pd.read_json(output_path)
            print(f"\nğŸ“‚ ê¸°ì¡´ ë°ì´í„°ì…‹ ë¡œë“œ: {len(existing_df)}ê°œ ìƒ˜í”Œ")

            # ì»¬ëŸ¼ ë§¤í•‘ í™•ì¸ (ê¸°ì¡´ ë°ì´í„°ì™€ ìƒˆ ë°ì´í„° ì»¬ëŸ¼ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
            # Ragas ë²„ì „ì— ë”°ë¼ ì»¬ëŸ¼ëª…ì´ ì¡°ê¸ˆì”© ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìœ ì—°í•˜ê²Œ ëŒ€ì²˜
            combined_df = pd.concat([existing_df, df], ignore_index=True)
            print(f"â• ìƒˆ ë°ì´í„° {len(df)}ê°œ ì¶”ê°€ -> ì´ {len(combined_df)}ê°œ")
            df = combined_df
        except Exception as e:
            print(f"âš ï¸ ê¸°ì¡´ íŒŒì¼ ë³‘í•© ì‹¤íŒ¨ (ë®ì–´ì“°ê¸° ì§„í–‰): {e}")

    df.to_json(output_path, orient='records', force_ascii=False, indent=2)

    print(f"\nğŸ’¾ ì €ì¥ ì™„ë£Œ: {output_path}")

    # ---------------------------------------------------------
    # Step 5: ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
    # ---------------------------------------------------------
    print("\n" + "=" * 60)
    print("ğŸ“Š ìƒì„±ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
    print("=" * 60)
    print(f"\nì»¬ëŸ¼: {list(df.columns)}")

    print("\nìƒ˜í”Œ ì§ˆë¬¸ 3ê°œ:")
    for i, row in df.head(3).iterrows():
        q = row.get('user_input', row.get('question', 'N/A'))
        a = row.get('reference', row.get('ground_truth', 'N/A'))
        print(f"\n[{i+1}] Q: {str(q)[:80]}...")
        print(f"    A: {str(a)[:80]}...")


if __name__ == '__main__':
    main()
