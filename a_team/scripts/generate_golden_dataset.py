"""
ë…¸ë™ë²• RAG ì±—ë´‡ í‰ê°€ìš© Golden Set ìë™ ìƒì„± ìŠ¤í¬ë¦½íŠ¸ (Ragas 0.4.x)

Ragas TestsetGeneratorë¥¼ ì‚¬ìš©í•˜ì—¬ PDF/í…ìŠ¤íŠ¸ ë¬¸ì„œë¡œë¶€í„°
ë‹¤ì–‘í•œ ìœ í˜•ì˜ í‰ê°€ìš© ì§ˆë¬¸-ë‹µë³€ ìŒì„ ìë™ ìƒì„±í•©ë‹ˆë‹¤.

Tech Stack:
    - Python 3.10+
    - ragas 0.4.x
    - langchain / langchain-openai / langchain-community
    - pandas

Usage:
    # ê¸°ë³¸ ì‹¤í–‰ (OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ í•„ìš”)
    python generate_golden_dataset.py

    # ìƒì„±í•  í…ŒìŠ¤íŠ¸ì…‹ í¬ê¸° ì§€ì •
    python generate_golden_dataset.py --test-size 50

    # ë°ì´í„° í´ë” ê²½ë¡œ ì§€ì •
    python generate_golden_dataset.py --data-dir ./custom_data
"""

from ragas.testset import TestsetGenerator
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader, TextLoader
import os
import warnings
from pathlib import Path

import pandas as pd
from dotenv import load_dotenv

# .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ê²½ê³  ë©”ì‹œì§€ í•„í„°ë§ (ì„ íƒì‚¬í•­)
warnings.filterwarnings("ignore", category=DeprecationWarning)


def load_documents(data_dir: str, sources: list = None) -> list:
    """
    ì§€ì •ëœ í´ë”ì—ì„œ JSON, PDF, TXT íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.

    Args:
        data_dir: ë¬¸ì„œê°€ ì €ì¥ëœ í´ë” ê²½ë¡œ
        sources: ë¡œë“œí•  ì†ŒìŠ¤ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ['qa', 'case', 'law'])
                 Noneì´ë©´ ëª¨ë“  ì†ŒìŠ¤ ë¡œë“œ

    Returns:
        List of LangChain Document objects
    """
    from langchain_core.documents import Document
    import json

    documents = []
    data_path = Path(data_dir)

    if not data_path.exists():
        raise FileNotFoundError(f"ë°ì´í„° í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_dir}")

    print(f"ğŸ“‚ ë¬¸ì„œ ë¡œë“œ ì¤‘: {data_dir}")

    # ê¸°ë³¸ ì†ŒìŠ¤: ëª¨ë“  íƒ€ì… ë¡œë“œ
    if sources is None:
        sources = ['qa', 'case', 'law', 'interpretation']

    # ---------------------------------------------------------
    # 1. ê³ ìš©ë…¸ë™ë¶€ FAQ (Q&A í˜•ì‹)
    # ---------------------------------------------------------
    if 'qa' in sources:
        qa_files = list(data_path.rglob('*ê³ ìš©ë…¸ë™ë¶€*QA*.json')) + \
            list(data_path.rglob('*FAQ*.json'))
        for filepath in qa_files:
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                for item in data:
                    question = item.get('question', '')
                    answer = item.get('answer', '')
                    if question and answer:
                        text = f"[ì§ˆì˜]\n{question}\n\n[íšŒì‹ ]\n{answer}"
                        doc = Document(
                            page_content=text,
                            metadata={
                                'source': 'qa',
                                'title': item.get('title', ''),
                                'category': item.get('category', ''),
                                'url': item.get('url', '')
                            }
                        )
                        documents.append(doc)
                print(f"  âœ… Q&A: {filepath.name} â†’ {len(data)}ê°œ")
            except Exception as e:
                print(f"  âš ï¸ Q&A ë¡œë“œ ì˜¤ë¥˜ ({filepath.name}): {e}")

    # ---------------------------------------------------------
    # 2. ì£¼ìš”íŒë¡€ (íŒì •ì‚¬í•­/íŒì •ìš”ì§€)
    # ---------------------------------------------------------
    if 'case' in sources:
        case_files = list(data_path.rglob('*ì£¼ìš”íŒë¡€*.json')) + \
            list(data_path.rglob('*íŒë¡€*.json'))
        for filepath in case_files:
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                count = 0
                for item in data:
                    íŒì •ì‚¬í•­ = item.get('íŒì •ì‚¬í•­', '')
                    íŒì •ìš”ì§€ = item.get('íŒì •ìš”ì§€', '')
                    if íŒì •ì‚¬í•­ and íŒì •ìš”ì§€:
                        text = f"[íŒì •ì‚¬í•­]\n{íŒì •ì‚¬í•­}\n\n[íŒì •ìš”ì§€]\n{íŒì •ìš”ì§€}"
                        doc = Document(
                            page_content=text,
                            metadata={
                                'source': 'case',
                                'title': item.get('ì œëª©', ''),
                                'category': item.get('ìë£Œêµ¬ë¶„', ''),
                                'department': item.get('ë‹´ë‹¹ë¶€ì„œ', ''),
                                'reg_date': item.get('ë“±ë¡ì¼', '')
                            }
                        )
                        documents.append(doc)
                        count += 1
                print(f"  âœ… íŒë¡€: {filepath.name} â†’ {count}ê°œ")
            except Exception as e:
                print(f"  âš ï¸ íŒë¡€ ë¡œë“œ ì˜¤ë¥˜ ({filepath.name}): {e}")

    # ---------------------------------------------------------
    # 3. ë²•ë ¹ ë°ì´í„° (ì¡°ë¬¸ ë‹¨ìœ„)
    # ---------------------------------------------------------
    if 'law' in sources:
        law_files = list(data_path.rglob('rd_ë…¸ë™ë²•.json')) + \
            list(data_path.rglob('rd_ë¯¼ì‚¬ë²•.json')) + \
            list(data_path.rglob('rd_í˜•ì‚¬ë²•.json'))
        for filepath in law_files:
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                count = 0
                for law in data:
                    law_title = law.get('title', '')
                    articles = law.get('articles', [])
                    for article in articles:
                        content = article.get('content', '')
                        if content and len(content) > 20:
                            text = f"[{law_title}]\n{content}"
                            doc = Document(
                                page_content=text,
                                metadata={
                                    'source': 'law',
                                    'law_title': law_title,
                                    'article_num': article.get('article_num', ''),
                                    'category': law.get('category', '')
                                }
                            )
                            documents.append(doc)
                            count += 1
                print(f"  âœ… ë²•ë ¹: {filepath.name} â†’ {count}ê°œ ì¡°ë¬¸")
            except Exception as e:
                print(f"  âš ï¸ ë²•ë ¹ ë¡œë“œ ì˜¤ë¥˜ ({filepath.name}): {e}")

    # ---------------------------------------------------------
    # 4. í–‰ì •í•´ì„ (íŒŒì‹±ëœ Q&A)
    # ---------------------------------------------------------
    if 'interpretation' in sources:
        interp_files = list(data_path.rglob('data_í–‰ì •í•´ì„*.json')) + \
            list(data_path.rglob('*í–‰ì •í•´ì„*.json'))
        for filepath in interp_files:
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                count = 0
                for item in data:
                    parsed = item.get('parsed', {})
                    if parsed.get('parse_success'):
                        questions = parsed.get('questions', [])
                        answers = parsed.get('answers', [])
                        for q, a in zip(questions, answers):
                            if q and a:
                                text = f"[ì§ˆì˜]\n{q}\n\n[íšŒì‹ ]\n{a}"
                                doc = Document(
                                    page_content=text,
                                    metadata={
                                        'source': 'interpretation',
                                        'title': item.get('title', ''),
                                        'department': item.get('department', ''),
                                        'reg_date': item.get('reg_date', '')
                                    }
                                )
                                documents.append(doc)
                                count += 1
                print(f"  âœ… í–‰ì •í•´ì„: {filepath.name} â†’ {count}ê°œ Q&A")
            except Exception as e:
                print(f"  âš ï¸ í–‰ì •í•´ì„ ë¡œë“œ ì˜¤ë¥˜ ({filepath.name}): {e}")

    # ---------------------------------------------------------
    # 5. PDF/TXT íŒŒì¼ (ê¸°ì¡´ ë¡œì§)
    # ---------------------------------------------------------
    try:
        pdf_loader = DirectoryLoader(
            path=str(data_path),
            glob="**/*.pdf",
            loader_cls=PyPDFLoader,
            show_progress=False,
            use_multithreading=True,
            silent_errors=True
        )
        pdf_docs = pdf_loader.load()
        if pdf_docs:
            print(f"  âœ… PDF: {len(pdf_docs)}ê°œ í˜ì´ì§€")
        documents.extend(pdf_docs)
    except Exception as e:
        pass  # PDF ì—†ìœ¼ë©´ ì¡°ìš©íˆ ë„˜ì–´ê°

    try:
        txt_loader = DirectoryLoader(
            path=str(data_path),
            glob="**/*.txt",
            loader_cls=TextLoader,
            loader_kwargs={"encoding": "utf-8"},
            show_progress=False,
            silent_errors=True
        )
        txt_docs = txt_loader.load()
        if txt_docs:
            print(f"  âœ… TXT: {len(txt_docs)}ê°œ ë¬¸ì„œ")
        documents.extend(txt_docs)
    except Exception as e:
        pass  # TXT ì—†ìœ¼ë©´ ì¡°ìš©íˆ ë„˜ì–´ê°

    print(f"\nğŸ“„ ì´ {len(documents)}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ\n")

# ---------------------------------------------------------
# [ìˆ˜ì •] ì˜¨ë„ë¥¼ ê°•ì œë¡œ 1ë¡œ ê³ ì •í•˜ëŠ” ì»¤ìŠ¤í…€ LLM í´ë˜ìŠ¤
# (ì¼ë¶€ ëª¨ë¸ì´ temperature!=1ì„ ì§€ì›í•˜ì§€ ì•Šì„ ë•Œ ì‚¬ìš©)
# ---------------------------------------------------------


class ForceTemperature1ChatOpenAI(ChatOpenAI):
    def _generate(self, messages, stop=None, run_manager=None, **kwargs):
        if 'temperature' in kwargs:
            kwargs['temperature'] = 1
        return super()._generate(messages, stop=stop, run_manager=run_manager, **kwargs)

    async def _agenerate(self, messages, stop=None, run_manager=None, **kwargs):
        if 'temperature' in kwargs:
            kwargs['temperature'] = 1
        return await super()._agenerate(messages, stop=stop, run_manager=run_manager, **kwargs)


def setup_generator(model_name: str = "gpt-5.2") -> TestsetGenerator:
    """
    Ragas 0.4.x TestsetGeneratorë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.

    Args:
        model_name: ì‚¬ìš©í•  OpenAI ëª¨ë¸ëª… (gpt-4o, gpt-4-turbo ë“±)

    Returns:
        ì„¤ì •ëœ TestsetGenerator ì¸ìŠ¤í„´ìŠ¤
    """
    print(f"ğŸ¤– LLM ì„¤ì • ì¤‘: {model_name}")

    # ---------------------------------------------------------
    # Generator LLM ì„¤ì • (ì»¤ìŠ¤í…€ ë˜í¼ ì‚¬ìš©)
    # ---------------------------------------------------------
    # Ragasê°€ ë‚´ë¶€ì ìœ¼ë¡œ temperatureë¥¼ 0.01 ë“±ìœ¼ë¡œ ë‚®ì¶”ë ¤ í•´ë„
    # ì´ í´ë˜ìŠ¤ê°€ ê°€ë¡œì±„ì„œ 1ë¡œ ê°•ì œ ê³ ì •í•©ë‹ˆë‹¤.
    generator_llm = ForceTemperature1ChatOpenAI(
        model=model_name,
        temperature=1,
    )

    # ---------------------------------------------------------
    # Embeddings ì„¤ì •
    # ---------------------------------------------------------
    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-large"
    )

    # ---------------------------------------------------------
    # Ragas 0.4.x: TestsetGenerator.from_langchain() ì‚¬ìš©
    # ---------------------------------------------------------
    generator = TestsetGenerator.from_langchain(
        llm=generator_llm,
        embedding_model=embeddings
    )

    print("âœ… TestsetGenerator ì„¤ì • ì™„ë£Œ\n")
    return generator


def generate_testset(
    generator: TestsetGenerator,
    documents: list,
    test_size: int = 30
) -> pd.DataFrame:
    """
    ë¬¸ì„œë¡œë¶€í„° í…ŒìŠ¤íŠ¸ì…‹ì„ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        generator: ì„¤ì •ëœ TestsetGenerator
        documents: ë¡œë“œëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        test_size: ìƒì„±í•  ì§ˆë¬¸ ê°œìˆ˜

    Returns:
        ìƒì„±ëœ í…ŒìŠ¤íŠ¸ì…‹ DataFrame
    """
    print(f"ğŸ“ í…ŒìŠ¤íŠ¸ì…‹ ìƒì„± ì¤‘ (ëª©í‘œ: {test_size}ê°œ)")
    print("   ë…¸ë™ë²• íŠ¹ì„±ìƒ ì¡°ê±´ë¶€/ì¶”ë¡  ì§ˆë¬¸ì´ ìë™ìœ¼ë¡œ ë§ì´ ìƒì„±ë©ë‹ˆë‹¤.")
    print()

    # ---------------------------------------------------------
    # Ragas 0.4.x: generate_with_langchain_docs ë©”ì„œë“œ ì‚¬ìš©
    # ---------------------------------------------------------
    testset = generator.generate_with_langchain_docs(
        documents=documents,
        testset_size=test_size,
    )

    # DataFrameìœ¼ë¡œ ë³€í™˜
    df = testset.to_pandas()

    print(f"\nâœ… {len(df)}ê°œ ì§ˆë¬¸-ë‹µë³€ ìŒ ìƒì„± ì™„ë£Œ")
    return df


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse

    # ---------------------------------------------------------
    # CLI ì¸ì íŒŒì‹±
    # ---------------------------------------------------------
    parser = argparse.ArgumentParser(
        description='ë…¸ë™ë²• RAG í‰ê°€ìš© Golden Set ìƒì„± (Ragas 0.4.x TestsetGenerator)'
    )
    parser.add_argument(
        '--data-dir',
        type=str,
        default='./data',
        help='PDF/TXT ë¬¸ì„œê°€ ìˆëŠ” í´ë” ê²½ë¡œ (ê¸°ë³¸ê°’: ./data)'
    )
    parser.add_argument(
        '--test-size',
        type=int,
        default=30,
        help='ìƒì„±í•  í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ê°œìˆ˜ (ê¸°ë³¸ê°’: 30)'
    )
    parser.add_argument(
        '--sample-size',
        type=int,
        default=200,
        help='ì‚¬ìš©í•  ë¬¸ì„œ ìƒ˜í”Œë§ ê°œìˆ˜ (0ì´ë©´ ì „ì²´ ì‚¬ìš©, ê¸°ë³¸ê°’: 200)'
    )
    parser.add_argument(
        '--model',
        type=str,
        default='gpt-5-mini',
        help='ì‚¬ìš©í•  LLM ëª¨ë¸ (ê¸°ë³¸ê°’: gpt-5.2)'
    )
    parser.add_argument(
        '--output',
        type=str,
        default='labor_law_golden_set.csv',
        help='ì¶œë ¥ íŒŒì¼ëª… (ê¸°ë³¸ê°’: labor_law_golden_set.csv)'
    )
    args = parser.parse_args()

    # ---------------------------------------------------------
    # API í‚¤ í™•ì¸
    # ---------------------------------------------------------
    if not os.environ.get("OPENAI_API_KEY"):
        print("âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        print("   export OPENAI_API_KEY='your-api-key'")
        return

    print("=" * 60)
    print("ğŸ›ï¸  ë…¸ë™ë²• RAG í‰ê°€ìš© Golden Set ìƒì„±ê¸° (Ragas 0.4.x)")
    print("=" * 60)
    print()

    # ---------------------------------------------------------
    # Step 1: ë¬¸ì„œ ë¡œë“œ
    # ---------------------------------------------------------
    documents = load_documents(args.data_dir)

    if not documents:
        print("âŒ ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„° í´ë”ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return

    # ---------------------------------------------------------
    # [ì¶”ê°€] ê³¼ë„í•œ ë¹„ìš©/ì‹œê°„ ë°©ì§€ë¥¼ ìœ„í•œ ëœë¤ ìƒ˜í”Œë§
    # ---------------------------------------------------------
    if args.sample_size > 0 and len(documents) > args.sample_size:
        import random
        print(
            f"âœ‚ï¸  ë¬¸ì„œê°€ ë„ˆë¬´ ë§ì•„ {args.sample_size}ê°œë¡œ ìƒ˜í”Œë§í•©ë‹ˆë‹¤. (ì „ì²´: {len(documents)}ê°œ)")
        documents = random.sample(documents, args.sample_size)

    # ---------------------------------------------------------
    # Step 2: Generator ì„¤ì •
    # ---------------------------------------------------------
    generator = setup_generator(args.model)

    # ---------------------------------------------------------
    # Step 3: í…ŒìŠ¤íŠ¸ì…‹ ìƒì„±
    # ---------------------------------------------------------
    df = generate_testset(
        generator=generator,
        documents=documents,
        test_size=args.test_size
    )

    # ---------------------------------------------------------
    # Step 4: ê²°ê³¼ ì €ì¥
    # ---------------------------------------------------------
    script_dir = Path(__file__).parent
    output_dir = script_dir.parent / "data" / "evaluation"
    output_dir.mkdir(parents=True, exist_ok=True)

    output_path = output_dir / args.output
    df.to_csv(output_path, index=False, encoding='utf-8-sig')

    print(f"\nğŸ’¾ ì €ì¥ ì™„ë£Œ: {output_path}")

    # ---------------------------------------------------------
    # Step 5: ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
    # ---------------------------------------------------------
    print("\n" + "=" * 60)
    print("ğŸ“Š ìƒì„±ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
    print("=" * 60)
    print(f"\nì»¬ëŸ¼: {list(df.columns)}")

    print("\nìƒ˜í”Œ ì§ˆë¬¸ 3ê°œ:")
    for i, row in df.head(3).iterrows():
        q = row.get('user_input', row.get('question', 'N/A'))
        a = row.get('reference', row.get('ground_truth', 'N/A'))
        print(f"\n[{i+1}] Q: {str(q)[:80]}...")
        print(f"    A: {str(a)[:80]}...")


if __name__ == '__main__':
    main()
