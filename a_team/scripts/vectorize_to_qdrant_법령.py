"""
ë²•ë ¹ ë°ì´í„° Qdrant ë²¡í„° DB ì—…ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸
- processed/law_chunks.json ë¡œë“œ
- Qdrant ì»¬ë ‰ì…˜(A-TEAM)ì— ì—…ì„œíŠ¸
"""

from sentence_transformers import SentenceTransformer
from qdrant_client.models import VectorParams, Distance, PointStruct
from qdrant_client import QdrantClient
from dotenv import load_dotenv
import json
import os
import sys
from typing import List, Dict, Any
from pathlib import Path

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ============================================================
# ì„¤ì •
# ============================================================
SCRIPT_DIR = Path(__file__).parent
DATA_DIR = SCRIPT_DIR / '..' / 'data'
PROCESSED_FILE = DATA_DIR / 'processed' / 'law_chunks.json'

EMBEDDING_MODEL = "Qwen/Qwen3-Embedding-0.6B"
EMBEDDING_DIM = 1024
COLLECTION_NAME = os.getenv("QDRANT_COLLECTION_NAME", "A-TEAM")


# ============================================================
# Qdrant í´ë¼ì´ì–¸íŠ¸ í´ë˜ìŠ¤
# ============================================================
class LegalVectorDB:
    def __init__(self, url: str = None, api_key: str = None, host: str = None, port: int = 6333):
        """Qdrant í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        if url and api_key:
            print(f"ğŸŒ Qdrant í´ë¼ìš°ë“œ ì—°ê²°: {url[:30]}...")
            self.client = QdrantClient(url=url, api_key=api_key, timeout=60)
        elif host:
            print(f"ğŸ  Qdrant ì„œë²„ ì—°ê²°: {host}:{port}")
            self.client = QdrantClient(host=host, port=port, timeout=60)
        else:
            print("âš ï¸ ì—°ê²° ì •ë³´ ì—†ìŒ, ë©”ëª¨ë¦¬ ëª¨ë“œ")
            self.client = QdrantClient(":memory:")

        print(f"ğŸ§  ì„ë² ë”© ëª¨ë¸ ë¡œë”©: {EMBEDDING_MODEL}")
        self.model = SentenceTransformer(EMBEDDING_MODEL)

    def create_collection(self, name: str, recreate: bool = False):
        """ì»¬ë ‰ì…˜ ìƒì„±"""
        collections = [
            c.name for c in self.client.get_collections().collections]
        if name in collections:
            if recreate:
                print(f"â™»ï¸  ì»¬ë ‰ì…˜ '{name}' ì¬ìƒì„± (ì‚­ì œ í›„ ìƒì„±)")
                self.client.delete_collection(name)
            else:
                print(f"âœ… ì»¬ë ‰ì…˜ '{name}' ì´ë¯¸ ì¡´ì¬")
                return

        self.client.create_collection(
            collection_name=name,
            vectors_config=VectorParams(
                size=EMBEDDING_DIM, distance=Distance.COSINE)
        )
        print(f"âœ¨ ì»¬ë ‰ì…˜ '{name}' ìƒì„± ì™„ë£Œ")

    def upsert_chunks(self, collection_name: str, chunks: List[Dict[str, Any]], batch_size: int = 32):
        """ì²­í¬ ì—…ì„œíŠ¸ (ë°°ì¹˜ ì²˜ë¦¬)"""
        if not chunks:
            print("âŒ ì—…ë¡œë“œí•  ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        total = len(chunks)
        print(f"ğŸš€ ì´ {total}ê°œ ì²­í¬ ì—…ë¡œë“œ ì‹œì‘...")

        # í¬ì¸íŠ¸ ID ìƒì„±ì„ ìœ„í•œ ì˜¤í”„ì…‹ (ê¸°ì¡´ ë°ì´í„°ì™€ ì¶©ëŒ ë°©ì§€ í•„ìš”ì‹œ ì¡°ì •)
        start_id = 0

        for i in range(0, total, batch_size):
            batch = chunks[i: i + batch_size]
            texts = [c['text'] for c in batch]

            # ì„ë² ë”©
            embeddings = self.model.encode(
                texts, show_progress_bar=False, convert_to_numpy=True)

            points = []
            for idx, (chunk, vector) in enumerate(zip(batch, embeddings)):
                # ë©”íƒ€ë°ì´í„°ì— í…ìŠ¤íŠ¸ í¬í•¨ (í˜ì´ë¡œë“œ ì €ì¥ìš©)
                payload = chunk['metadata'].copy()
                payload['text'] = chunk['text']

                points.append(PointStruct(
                    id=start_id + i + idx,
                    vector=vector.tolist(),
                    payload=payload
                ))

            self.client.upsert(collection_name=collection_name, points=points)
            print(
                f"\rğŸ“¥ ì €ì¥ ì¤‘: {i + len(batch)}/{total} ({(i + len(batch))/total*100:.1f}%)", end='', flush=True)

        print(f"\nâœ… '{collection_name}' ì—…ë¡œë“œ ì™„ë£Œ!")

    def search(self, img_query: str, top_k: int = 3):
        """í…ŒìŠ¤íŠ¸ ê²€ìƒ‰"""
        vec = self.model.encode(img_query).tolist()
        hits = self.client.query_points(
            collection_name=COLLECTION_NAME, query=vec, limit=top_k).points
        return hits


# ============================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================
def main():
    print("=" * 60)
    print("âš–ï¸  ë²•ë ¹ ë°ì´í„° Qdrant ì—…ë¡œë“œ")
    print("=" * 60)

    # 1. ì „ì²˜ë¦¬ëœ íŒŒì¼ ë¡œë“œ
    if not PROCESSED_FILE.exists():
        print(f"âŒ ì „ì²˜ë¦¬ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {PROCESSED_FILE}")
        print("ğŸ’¡ ë¨¼ì € 'uv run a_team/scripts/preprocesser_ë²•ë ¹.py'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    print(f"ğŸ“‚ íŒŒì¼ ë¡œë“œ ì¤‘: {PROCESSED_FILE}")
    with open(PROCESSED_FILE, 'r', encoding='utf-8') as f:
        chunks = json.load(f)
    print(f"ğŸ“Š ë¡œë“œëœ ì²­í¬: {len(chunks)}ê°œ")

    # 2. Qdrant ì—°ê²°
    url = os.getenv("QDRANT_URL")
    key = os.getenv("QDRANT_API_KEY")

    if not url:
        # ë¡œì»¬ í´ë°±
        db = LegalVectorDB(host='localhost', port=6333)
    else:
        db = LegalVectorDB(url=url, api_key=key)

    # 3. ì—…ë¡œë“œ
    # ì£¼ì˜: recreate=Trueë¡œ í•˜ë©´ ê¸°ì¡´ ë°ì´í„° ë‚ ë¼ê°. í•„ìš”ì‹œ Falseë¡œ ë³€ê²½.
    # í•˜ì§€ë§Œ Clean êµ¬ì¶•ì„ ìœ„í•´ True ìœ ì§€ (ì‚¬ìš©ì ì˜ë„ì— ë”°ë¼ ì¡°ì • ê°€ëŠ¥)
    db.create_collection(COLLECTION_NAME, recreate=True)
    db.upsert_chunks(COLLECTION_NAME, chunks)

    # 4. ê²€ì¦
    print("\nğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸: 'í‡´ì§ê¸ˆ ì¤‘ê°„ì •ì‚°'")
    hits = db.search("í‡´ì§ê¸ˆ ì¤‘ê°„ì •ì‚°")
    for i, h in enumerate(hits, 1):
        meta = h.payload
        print(
            f"\n[{i}] {meta.get('law_name')} {meta.get('article_title')} (Score: {h.score:.3f})")
        print(f"    {meta.get('text')[:100]}...")


if __name__ == '__main__':
    main()
