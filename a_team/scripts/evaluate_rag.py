"""
ë…¸ë™ë²• RAG ì±—ë´‡ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸

Golden Datasetì„ ì‚¬ìš©í•˜ì—¬ baseline RAG ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.
Ragas ë©”íŠ¸ë¦­(Faithfulness, Answer Relevancy, Context Precision/Recall)ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

Usage:
    # ê¸°ë³¸ ì‹¤í–‰
    uv run a_team/scripts/evaluate_rag.py
    
    # ìƒ˜í”Œ ìˆ˜ ì§€ì • (í…ŒìŠ¤íŠ¸ìš©)
    uv run a_team/scripts/evaluate_rag.py --sample 10
    
    # ì»¤ìŠ¤í…€ ê³¨ë“ ì…‹ ê²½ë¡œ
    uv run a_team/scripts/evaluate_rag.py --golden-set path/to/golden_set.csv
"""

import os
import sys
import argparse
import warnings
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any

import pandas as pd
from dotenv import load_dotenv
from tqdm import tqdm

# Ragas í‰ê°€ ë©”íŠ¸ë¦­
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
)
from datasets import Dataset

# LangChain
from langchain_openai import ChatOpenAI

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
_SCRIPT_DIR = Path(__file__).parent
load_dotenv(dotenv_path=_SCRIPT_DIR / ".env")

# ê²½ê³  ë©”ì‹œì§€ í•„í„°ë§
warnings.filterwarnings("ignore", category=DeprecationWarning)


# ============================================================
# Golden Dataset ë¡œë“œ
# ============================================================
def load_golden_dataset(path: str) -> pd.DataFrame:
    """
    Golden Dataset CSV íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.

    Args:
        path: CSV íŒŒì¼ ê²½ë¡œ

    Returns:
        DataFrame with columns: user_input, reference (ground truth)
    """
    if not os.path.exists(path):
        raise FileNotFoundError(f"Golden Datasetì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}")

    df = pd.read_csv(path)

    # Ragas 0.4.x ì»¬ëŸ¼ëª… í™•ì¸ ë° ë§¤í•‘
    # ì˜ˆìƒ ì»¬ëŸ¼: user_input, reference, reference_contexts
    required_cols = []

    # ì§ˆë¬¸ ì»¬ëŸ¼ ì°¾ê¸°
    question_col = None
    for col in ['user_input', 'question', 'query']:
        if col in df.columns:
            question_col = col
            break

    if question_col is None:
        raise ValueError(f"ì§ˆë¬¸ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì»¬ëŸ¼: {list(df.columns)}")

    # ì •ë‹µ ì»¬ëŸ¼ ì°¾ê¸°
    answer_col = None
    for col in ['reference', 'ground_truth', 'answer', 'expected_answer']:
        if col in df.columns:
            answer_col = col
            break

    if answer_col is None:
        raise ValueError(f"ì •ë‹µ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì»¬ëŸ¼: {list(df.columns)}")

    # ì»¬ëŸ¼ëª… ì •ê·œí™”
    df = df.rename(columns={
        question_col: 'user_input',
        answer_col: 'reference'
    })

    print(f"âœ… Golden Dataset ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ ìƒ˜í”Œ")
    print(f"   ì§ˆë¬¸ ì»¬ëŸ¼: {question_col} â†’ user_input")
    print(f"   ì •ë‹µ ì»¬ëŸ¼: {answer_col} â†’ reference")

    return df


# ============================================================
# Baseline ëª¨ë¸ ì¶”ë¡ 
# ============================================================
def run_inference(questions: List[str], verbose: bool = True) -> List[Dict[str, Any]]:
    """
    Baseline RAG ëª¨ë¸ë¡œ ê° ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        questions: ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
        verbose: ì§„í–‰ ìƒí™© ì¶œë ¥ ì—¬ë¶€

    Returns:
        List of {answer: str, contexts: List[str]}
    """
    # baseline.pyì—ì„œ ì±—ë´‡ ì´ˆê¸°í™” í•¨ìˆ˜ ìž„í¬íŠ¸
    from baseline import initialize_rag_chatbot

    print("\nðŸ¤– Baseline ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
    chatbot = initialize_rag_chatbot()
    print("âœ… ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ\n")

    results = []

    iterator = tqdm(questions, desc="ì¶”ë¡  ì¤‘") if verbose else questions

    for question in iterator:
        try:
            # Agent ì‹¤í–‰ (intermediate_stepsë¡œ ê²€ìƒ‰ ê²°ê³¼ ì¶”ì¶œ)
            response = chatbot.invoke({
                "input": question,
                "chat_history": []
            })

            answer = response.get("output", "")

            # ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
            # AgentExecutorì˜ intermediate_stepsì—ì„œ tool í˜¸ì¶œ ê²°ê³¼ ì¶”ì¶œ
            contexts = []
            intermediate_steps = response.get("intermediate_steps", [])

            for step in intermediate_steps:
                if len(step) >= 2:
                    tool_output = step[1]
                    if isinstance(tool_output, str) and len(tool_output) > 0:
                        # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë¬¸ì„œ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
                        docs = tool_output.split("\n\n")
                        contexts.extend([doc.strip()
                                        for doc in docs if doc.strip()])

            # ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ëŒ€ì‹  ë‹µë³€ ì¼ë¶€ ì‚¬ìš© (fallback)
            if not contexts:
                contexts = ["(ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ)"]

            results.append({
                "answer": answer,
                "contexts": contexts
            })

        except Exception as e:
            print(f"\nâš ï¸ ì¶”ë¡  ì‹¤íŒ¨: {question[:50]}... - {e}")
            results.append({
                "answer": f"[ì˜¤ë¥˜] {str(e)}",
                "contexts": []
            })

    return results


# ============================================================
# Ragas í‰ê°€
# ============================================================
def evaluate_with_ragas(
    questions: List[str],
    answers: List[str],
    contexts: List[List[str]],
    references: List[str],
    llm_model: str = "gpt-4o"
) -> Dict[str, Any]:
    """
    Ragas ë©”íŠ¸ë¦­ìœ¼ë¡œ RAG ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.

    Args:
        questions: ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
        answers: ìƒì„±ëœ ë‹µë³€ ë¦¬ìŠ¤íŠ¸
        contexts: ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        references: ì •ë‹µ(Ground Truth) ë¦¬ìŠ¤íŠ¸
        llm_model: í‰ê°€ì— ì‚¬ìš©í•  LLM ëª¨ë¸

    Returns:
        í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    print("\nðŸ“Š Ragas í‰ê°€ ì‹œìž‘...")

    # Ragas Dataset ìƒì„±
    eval_dataset = Dataset.from_dict({
        "user_input": questions,
        "response": answers,
        "retrieved_contexts": contexts,
        "reference": references
    })

    # í‰ê°€ìš© LLM ì„¤ì •
    eval_llm = ChatOpenAI(model=llm_model, temperature=0)

    # ë©”íŠ¸ë¦­ ì •ì˜
    metrics = [
        faithfulness,
        answer_relevancy,
        context_precision,
        context_recall,
    ]

    # í‰ê°€ ì‹¤í–‰
    try:
        result = evaluate(
            dataset=eval_dataset,
            metrics=metrics,
            llm=eval_llm,
            raise_exceptions=False
        )

        print("âœ… Ragas í‰ê°€ ì™„ë£Œ")
        return result

    except Exception as e:
        print(f"âŒ Ragas í‰ê°€ ì‹¤íŒ¨: {e}")
        raise


# ============================================================
# ê²°ê³¼ ì €ìž¥
# ============================================================
def save_results(
    df: pd.DataFrame,
    ragas_result: Dict,
    output_path: str
):
    """
    í‰ê°€ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ìž¥í•©ë‹ˆë‹¤.

    Args:
        df: ì›ë³¸ ë°ì´í„°í”„ë ˆìž„ (ì§ˆë¬¸, ì •ë‹µ í¬í•¨)
        ragas_result: Ragas í‰ê°€ ê²°ê³¼
        output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
    """
    # Ragas ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
    result_df = ragas_result.to_pandas()

    # ì›ë³¸ ë°ì´í„°ì™€ ê²°í•©
    final_df = pd.concat(
        [df.reset_index(drop=True), result_df.reset_index(drop=True)], axis=1)

    # ì €ìž¥
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    final_df.to_csv(output_path, index=False, encoding='utf-8-sig')

    print(f"\nðŸ’¾ ê²°ê³¼ ì €ìž¥ ì™„ë£Œ: {output_path}")


# ============================================================
# ë©”ì¸ í•¨ìˆ˜
# ============================================================
def main():
    parser = argparse.ArgumentParser(description='ë…¸ë™ë²• RAG ì±—ë´‡ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸')
    parser.add_argument(
        '--golden-set',
        type=str,
        default='a_team/data/evaluation/labor_law_golden_set.csv',
        help='Golden Dataset CSV ê²½ë¡œ'
    )
    parser.add_argument(
        '--output',
        type=str,
        default=None,
        help='ê²°ê³¼ ì €ìž¥ ê²½ë¡œ (ê¸°ë³¸ê°’: golden_set í´ë”ì— ì €ìž¥)'
    )
    parser.add_argument(
        '--sample',
        type=int,
        default=0,
        help='í‰ê°€í•  ìƒ˜í”Œ ìˆ˜ (0ì´ë©´ ì „ì²´ í‰ê°€)'
    )
    parser.add_argument(
        '--eval-model',
        type=str,
        default='gpt-4o',
        help='Ragas í‰ê°€ì— ì‚¬ìš©í•  LLM ëª¨ë¸'
    )
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='ë°ì´í„° ë¡œë“œë§Œ í…ŒìŠ¤íŠ¸í•˜ê³  ì¢…ë£Œ'
    )
    args = parser.parse_args()

    # API Key í™•ì¸
    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return

    print("=" * 60)
    print("ðŸ›ï¸  ë…¸ë™ë²• RAG ì±—ë´‡ í‰ê°€ ì‹œìž‘")
    print("=" * 60)

    # 1. Golden Dataset ë¡œë“œ
    df = load_golden_dataset(args.golden_set)

    # ìƒ˜í”Œë§
    if args.sample > 0 and args.sample < len(df):
        df = df.sample(n=args.sample, random_state=42).reset_index(drop=True)
        print(f"âœ‚ï¸  {args.sample}ê°œ ìƒ˜í”Œë¡œ ì œí•œ")

    # Dry run ëª¨ë“œ
    if args.dry_run:
        print("\nðŸ§ª Dry Run ëª¨ë“œ: ë°ì´í„° ë¡œë“œë§Œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.")
        print(f"\nìƒ˜í”Œ ë°ì´í„°:")
        print(df.head(3).to_string())
        return

    # 2. Baseline ëª¨ë¸ ì¶”ë¡ 
    questions = df['user_input'].tolist()
    references = df['reference'].tolist()

    print(f"\nðŸ“ {len(questions)}ê°œ ì§ˆë¬¸ì— ëŒ€í•´ ì¶”ë¡  ì‹œìž‘...")
    inference_results = run_inference(questions)

    # ê²°ê³¼ ì¶”ì¶œ
    answers = [r['answer'] for r in inference_results]
    contexts = [r['contexts'] for r in inference_results]

    # DataFrameì— ì¶”ë¡  ê²°ê³¼ ì¶”ê°€
    df['generated_answer'] = answers
    df['retrieved_contexts'] = [str(c) for c in contexts]  # ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìžì—´ë¡œ

    # 3. Ragas í‰ê°€
    ragas_result = evaluate_with_ragas(
        questions=questions,
        answers=answers,
        contexts=contexts,
        references=references,
        llm_model=args.eval_model
    )

    # 4. ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 60)
    print("ðŸ“Š í‰ê°€ ê²°ê³¼ ìš”ì•½")
    print("=" * 60)

    for metric_name, score in ragas_result.items():
        if isinstance(score, (int, float)):
            print(f"  â€¢ {metric_name}: {score:.4f}")

    # 5. ê²°ê³¼ ì €ìž¥
    if args.output:
        output_path = args.output
    else:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = Path(args.golden_set).parent
        output_path = output_dir / f"evaluation_results_{timestamp}.csv"

    save_results(df, ragas_result, str(output_path))

    print("\n" + "=" * 60)
    print("âœ… í‰ê°€ ì™„ë£Œ!")
    print("=" * 60)


if __name__ == "__main__":
    main()
