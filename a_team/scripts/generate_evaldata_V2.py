"""
Qdrant Cloudì˜ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ RAG ì±—ë´‡ í‰ê°€ë¥¼ ìœ„í•œ Golden Set ìƒì„± ìŠ¤í¬ë¦½íŠ¸ (Ragas 0.4.x)

ìš”êµ¬ì‚¬í•­(ì»¤ìŠ¤í…€):
1) ì´ 20ê°œ
2) ë…¸ë™ë²• 10, ë¯¼ì‚¬ë²• 5, í˜•ì‚¬ë²• 5
3) ë…¸ë™ë²• ì§ˆë¬¸ì€ ë²•ë ¹ ì™¸ ë¬¸ì„œë„ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•  ìˆ˜ ìˆë„ë¡ ì§ˆë¬¸ ìƒì„±(ê°€ëŠ¥í•˜ë©´ í•´ë‹¹ í”Œë˜ê·¸ trueì¸ ì§ˆë¬¸ì„ ìš°ì„  ì„ íƒ)

êµ¬í˜„ ë°©ì‹:
- RAGASë¡œ ë¶„ì•¼ë³„ë¡œ ì¶©ë¶„íˆ í° í’€ì„ ìƒì„±
- LLMìœ¼ë¡œ (ë¶„ì•¼/ë…¸ë™-ë¹„ë²•ë ¹ì°¸ê³ ê°€ëŠ¥) ë¼ë²¨ë§
- ë¶„ì•¼ë³„ ëª©í‘œ ê°œìˆ˜ì— ë§ì¶° ìƒ˜í”Œë§
"""

import argparse
import os
import random
import warnings
from pathlib import Path
from typing import Dict, List, Tuple

import pandas as pd
from dotenv import load_dotenv
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from qdrant_client import QdrantClient
from ragas.run_config import RunConfig
from ragas.testset import TestsetGenerator

# .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# LangSmith íŠ¸ë ˆì´ì‹± ëª…ì‹œì  ë¹„í™œì„±í™” (í† í° ì ˆì•½)
os.environ["LANGCHAIN_TRACING_V2"] = "false"
os.environ["LANGCHAIN_TRACING"] = "false"

warnings.filterwarnings("ignore", category=DeprecationWarning)


def normalize_domain_label(s: str) -> str:
    s = (s or "").strip()
    if "ë…¸ë™" in s:
        return "ë…¸ë™ë²•"
    if "ë¯¼ì‚¬" in s:
        return "ë¯¼ì‚¬ë²•"
    if "í˜•ì‚¬" in s:
        return "í˜•ì‚¬ë²•"
    return "ê¸°íƒ€"


def parse_label_line(line: str) -> Tuple[str, bool]:
    """
    ê¸°ëŒ€ í˜•ì‹: "ë¶„ì•¼|ë…¸ë™-ë¹„ë²•ë ¹ì°¸ê³ ê°€ëŠ¥(yes/no)"
    ì˜ˆ: "ë…¸ë™ë²•|yes"
    """
    parts = [p.strip() for p in (line or "").split("|")]
    if len(parts) < 2:
        return ("ê¸°íƒ€", False)
    domain = normalize_domain_label(parts[0])
    ns = parts[1].lower()
    non_statute_ok = ns in ("yes", "y", "true", "1", "ê°€ëŠ¥")
    return (domain, non_statute_ok)


def build_labeler_llm(model_name: str) -> ChatOpenAI:
    return ChatOpenAI(model=model_name, temperature=0)


def reformat_answers(df: pd.DataFrame, llm: ChatOpenAI) -> pd.DataFrame:
    """
    RAGASê°€ ìƒì„±í•œ ë‹µë³€(reference)ì„ ì›í•˜ëŠ” í…œí”Œë¦¿ í˜•ì‹ìœ¼ë¡œ ì¬ì‘ì„±.
    í…œí”Œë¦¿:
    - "ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€: ..."
    - "ê´€ë ¨ ë²•ë ¹ ì¡°í•­: ..."
    - "ì¶”ê°€ ì„¤ëª…: ..."
    """
    prompt = ChatPromptTemplate.from_template("""ë„ˆëŠ” ë²•ë¥  QA ë°ì´í„°ì…‹ì˜ ë‹µë³€ì„ ì •í•´ì§„ í…œí”Œë¦¿ìœ¼ë¡œ ì¬ì‘ì„±í•˜ëŠ” ì—­í• ì´ë‹¤.
ì£¼ì–´ì§„ ì§ˆë¬¸ê³¼ ì›ë³¸ ë‹µë³€ì„ ì°¸ê³ í•˜ì—¬, ì•„ë˜ í…œí”Œë¦¿ í˜•ì‹ìœ¼ë¡œ ì¬ì‘ì„±í•´ë¼.
ì›ë³¸ ë‹µë³€ì˜ ë‚´ìš©ì„ ì¶©ì‹¤íˆ ë°˜ì˜í•˜ë˜, í…œí”Œë¦¿ì— ë§ê²Œ êµ¬ì¡°í™”í•´ë¼.

### í…œí”Œë¦¿:
- "ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€: (í•µì‹¬ ë‹µë³€ 1~2ë¬¸ì¥)"
- "ê´€ë ¨ ë²•ë ¹ ì¡°í•­: (ë²•ë ¹ëª… ë° ì¡°í•­ ë²ˆí˜¸. ì—¬ëŸ¬ ê°œì¼ ìˆ˜ ìˆìŒ)"
- "ì¶”ê°€ ì„¤ëª…: (ë³´ì¶© ì„¤ëª…, ì˜ˆì™¸ì‚¬í•­, ì£¼ì˜ì  ë“±. 2~4ë¬¸ì¥)"

### ì…ë ¥:
ì§ˆë¬¸: {question}

ì›ë³¸ ë‹µë³€:
{original_answer}

### ì¶œë ¥:
í…œí”Œë¦¿ í˜•ì‹ìœ¼ë¡œ ì¬ì‘ì„±ëœ ë‹µë³€ë§Œ ì¶œë ¥í•´ë¼. ë‹¤ë¥¸ ì„¤ëª…ì€ í•˜ì§€ ë§ˆë¼.""")

    def get_col(row, *names):
        for n in names:
            if n in row and pd.notna(row[n]):
                return row[n]
        return ""

    new_answers = []
    for _, row in df.iterrows():
        question = get_col(row, "user_input", "question")
        original = get_col(row, "reference", "ground_truth", "answer")

        if not original or not question:
            new_answers.append(original)
            continue

        chain = prompt | llm
        result = chain.invoke({
            "question": str(question)[:1000],
            "original_answer": str(original)[:2000]
        }).content.strip()
        new_answers.append(result)

    df = df.copy()
    # reference ì»´ëŸ¼ ì´ë¦„ í™•ì¸ í›„ ì—…ë°ì´íŠ¸
    if "reference" in df.columns:
        df["reference"] = new_answers
    elif "ground_truth" in df.columns:
        df["ground_truth"] = new_answers
    elif "answer" in df.columns:
        df["answer"] = new_answers
    else:
        df["reference"] = new_answers
    
    return df


def label_rows(df: pd.DataFrame, llm: ChatOpenAI) -> pd.DataFrame:
    """
    ê° rowì— ëŒ€í•´ (domain, labor_non_statute_ok) ë¼ë²¨ì„ ë¶€ì—¬.
    RAGAS DF ì»¬ëŸ¼ì´ ë²„ì „ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆì–´ ë„“ê²Œ ëŒ€ì‘.
    """

    prompt = ChatPromptTemplate.from_template("""ë„ˆëŠ” ë²•ë¥  QA í‰ê°€ ë°ì´í„° ë¼ë²¨ëŸ¬ë‹¤.
ì…ë ¥(ì§ˆë¬¸/ì •ë‹µ/ì»¨í…ìŠ¤íŠ¸ ì¼ë¶€)ì„ ë³´ê³  ë‹¤ìŒì„ íŒì •í•œë‹¤:

1) ë¶„ì•¼: ë…¸ë™ë²•/ë¯¼ì‚¬ë²•/í˜•ì‚¬ë²•/ê¸°íƒ€
2) (ë…¸ë™ë²•ì¸ ê²½ìš°) ë²•ë ¹ ì¡°ë¬¸ë§Œìœ¼ë¡œ ë‹µí•˜ê¸° ì–´ë µê³ , í–‰ì •í•´ì„/íŒë¡€/Q&A/íŒì •ì„ ë¡€ ë“±
   'ë²•ë ¹ ì™¸ ë¬¸ì„œ' ì°¸ê³ ê°€ í•„ìš”í•œ ì§ˆë¬¸ì´ë©´ yes, ì•„ë‹ˆë©´ no

ì¶œë ¥ì€ ë°˜ë“œì‹œ í•œ ì¤„ë¡œë§Œ, ë‹¤ìŒ í˜•ì‹:
ë¶„ì•¼|yes/no
ì˜ˆ: ë…¸ë™ë²•|yes

---
ì§ˆë¬¸:
{q}

ì •ë‹µ(ì°¸ê³ ):
{a}

ì»¨í…ìŠ¤íŠ¸(ë°œì·Œ):
{ctx}""")

    def get_col(row, *names):
        for n in names:
            if n in row and pd.notna(row[n]):
                return row[n]
        return ""

    domains = []
    non_statutes = []

    for _, row in df.iterrows():
        q = get_col(row, "user_input", "question")
        a = get_col(row, "reference", "ground_truth", "answer")
        ctx_val = get_col(row, "contexts", "context")
        # contextsê°€ listì¼ ìˆ˜ë„ ìˆì–´ì„œ í…ìŠ¤íŠ¸ë¡œ ì¶•ì•½
        if isinstance(ctx_val, list):
            ctx = "\n---\n".join([str(x) for x in ctx_val[:3]])
        else:
            ctx = str(ctx_val)[:1500]

        chain = prompt | llm
        out = chain.invoke({"q": str(q)[:800], "a": str(a)[:1200], "ctx": ctx}).content.strip()
        domain, non_statute_ok = parse_label_line(out)

        domains.append(domain)
        non_statutes.append(bool(non_statute_ok))

    df = df.copy()
    df["domain"] = domains
    df["labor_non_statute_ok"] = non_statutes
    return df


def sample_with_quota(df: pd.DataFrame, domain_targets: Dict[str, int]) -> pd.DataFrame:
    """
    domain_targets ì˜ˆ: {"ë…¸ë™ë²•":10, "ë¯¼ì‚¬ë²•":5, "í˜•ì‚¬ë²•":5}
    ê° ë„ë©”ì¸ë³„ ëª©í‘œ ê°œìˆ˜ë§Œí¼ ìƒ˜í”Œë§.
    ë…¸ë™ë²•ì€ labor_non_statute_ok == True ë¥¼ ìš°ì„  ì±„íƒ(ê°€ëŠ¥í•˜ë©´).
    """
    picked_frames = []

    for domain, n in domain_targets.items():
        domain_df = df[df["domain"] == domain].copy()

        if len(domain_df) == 0:
            print(f"âš ï¸  [{domain}] í•´ë‹¹ ë¶„ì•¼ ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤. ìŠ¤í‚µí•©ë‹ˆë‹¤.")
            continue

        # ë…¸ë™ë²•ì´ë©´ non_statute_ok ìš°ì„ ìˆœìœ„ ë¶€ì—¬
        if domain == "ë…¸ë™ë²•":
            domain_df["__priority"] = domain_df["labor_non_statute_ok"].apply(lambda x: 0 if x else 1)
            domain_df = domain_df.sort_values(["__priority"])

        # ëª©í‘œ ê°œìˆ˜ë§Œí¼ ìƒ˜í”Œë§
        take = min(len(domain_df), n)
        if take < n:
            print(f"âš ï¸  [{domain}] í•„ìš” {n}ê°œ, ë³´ìœ  {len(domain_df)}ê°œ â†’ {take}ê°œ ìƒ˜í”Œë§")
        
        sampled = domain_df.head(take)
        picked_frames.append(sampled)

    if not picked_frames:
        raise ValueError("ìƒ˜í”Œë§ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ìƒì„±ëœ í’€ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    out = pd.concat(picked_frames, ignore_index=True)
    
    expected = sum(domain_targets.values())
    if len(out) != expected:
        print(f"âš ï¸  ìµœì¢… ìƒ˜í”Œ ìˆ˜: {len(out)}ê°œ (ëª©í‘œ {expected}ê°œ)")
    
    return out


# --------------------------------
# Qdrantì—ì„œ ë¬¸ì„œ ë¡œë“œ (ìµœì í™” ë²„ì „)
# --------------------------------
def load_documents_from_qdrant_by_domain(
    collection_name: str = None,
    docs_per_domain: int = 500
) -> Dict[str, list]:
    """
    Qdrantì—ì„œ ë¶„ì•¼ë³„ë¡œ í•„ìš”í•œ ë¬¸ì„œë§Œ ìƒ˜í”Œë§í•´ì„œ ë¡œë“œ.
    ì „ì²´ ìŠ¤ìº” ëŒ€ì‹  ëœë¤ ìƒ˜í”Œë§ìœ¼ë¡œ ë¹ ë¥´ê²Œ ê°€ì ¸ì˜´.
    """
    qdrant_url = os.getenv("QDRANT_URL")
    qdrant_api_key = os.getenv("QDRANT_API_KEY")
    collection = collection_name or os.getenv("QDRANT_COLLECTION_NAME", "A-TEAM")

    print(f"ğŸ“‚ Qdrant DBì—ì„œ ë¶„ì•¼ë³„ ë¬¸ì„œ ìƒ˜í”Œë§ ì¤‘...")
    print(f"   Collection: {collection}")
    print(f"   ë¶„ì•¼ë³„ ìµœëŒ€: {docs_per_domain}ê°œ")

    if qdrant_url and qdrant_api_key:
        print(f"   URL: {qdrant_url[:30]}...")
        client = QdrantClient(
            url=qdrant_url,
            api_key=qdrant_api_key,
            timeout=60
        )
    else:
        print("   Local Docker: localhost:6333")
        client = QdrantClient(host="localhost", port=6333)

    try:
        collection_info = client.get_collection(collection_name=collection)
        total_points = collection_info.points_count
        print(f"   ì´ í¬ì¸íŠ¸ ìˆ˜: {total_points}")
    except Exception as e:
        raise ConnectionError(f"Qdrant ì»¬ë ‰ì…˜ '{collection}'ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")

    buckets = {"ë…¸ë™ë²•": [], "ë…¸ë™ë²•_ë²•ë ¹ì™¸": [], "ë¯¼ì‚¬ë²•": [], "í˜•ì‚¬ë²•": [], "ê¸°íƒ€": []}
    
    # ë‹¤ì–‘ì„± í™•ë³´ë¥¼ ìœ„í•œ ë¶„ì‚° ìƒ˜í”Œë§
    sample_size = min(docs_per_domain * 6, 5000)
    batch_size = 20  # ì‘ì€ ë°°ì¹˜ë¡œ ë‹¤ì–‘í•œ ìœ„ì¹˜ì—ì„œ ìƒ˜í”Œë§
    num_sampling_points = max(100, sample_size // batch_size)  # ìµœì†Œ 100ê°œ ìœ„ì¹˜
    
    print(f"\n   ğŸ“¥ ë¶„ì‚° ëœë¤ ìƒ˜í”Œë§ ì¤‘ ({sample_size}ê°œ ëª©í‘œ, {num_sampling_points}ê°œ ìœ„ì¹˜)")
    
    # ì „ì²´ ë²”ìœ„ë¥¼ ê· ë“± ë¶„í•  í›„ ê° êµ¬ê°„ì—ì„œ ëœë¤ ìƒ˜í”Œë§
    all_sampled = []
    
    if total_points > sample_size:
        # ì „ì²´ ë²”ìœ„ë¥¼ num_sampling_pointsê°œ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ„ê¸°
        segment_size = total_points // num_sampling_points
        
        for i in range(num_sampling_points):
            if len(all_sampled) >= sample_size:
                break
            
            # ê° êµ¬ê°„ ë‚´ì—ì„œ ëœë¤ ì˜¤í”„ì…‹ ì„ íƒ
            segment_start = i * segment_size
            segment_end = min((i + 1) * segment_size, total_points)
            
            if segment_end - segment_start < batch_size:
                random_offset = segment_start
            else:
                random_offset = random.randint(segment_start, max(segment_start, segment_end - batch_size))
            
            try:
                results = client.scroll(
                    collection_name=collection,
                    limit=batch_size,
                    offset=random_offset,
                    with_payload=True,
                    with_vectors=False
                )
                
                points, _ = results
                
                for point in points:
                    if len(all_sampled) >= sample_size:
                        break
                    
                    payload = point.payload or {}
                    text = payload.get("text", "")
                    
                    if text and len(text) > 30:
                        doc = Document(
                            page_content=text,
                            metadata={
                                "id": str(point.id),
                                "source": payload.get("source", ""),
                                "law_name": payload.get("law_name", ""),
                                "law_id": payload.get("law_id", ""),
                                "article_no": payload.get("article_no", ""),
                                "article_title": payload.get("article_title", ""),
                                "paragraph_no": payload.get("paragraph_no", ""),
                                "chunk_type": payload.get("chunk_type", ""),
                                "category": payload.get("category", ""),
                                "chunk_index": payload.get("chunk_index", 0),
                            }
                        )
                        all_sampled.append(doc)
            except Exception as e:
                # ì¼ë¶€ ì˜¤í”„ì…‹ì—ì„œ ì˜¤ë¥˜ ë°œìƒ ì‹œ ìŠ¤í‚µ
                continue
    else:
        # ì „ì²´ ë°ì´í„°ê°€ ìƒ˜í”Œ ì‚¬ì´ì¦ˆë³´ë‹¤ ì‘ìœ¼ë©´ ì „ì²´ ë¡œë“œ
        offset = None
        while True:
            results = client.scroll(
                collection_name=collection,
                limit=100,
                offset=offset,
                with_payload=True,
                with_vectors=False
            )
            
            points, next_offset = results
            if not points:
                break
                
            for point in points:
                payload = point.payload or {}
                text = payload.get("text", "")
                
                if text and len(text) > 30:
                    doc = Document(
                        page_content=text,
                        metadata={
                            "id": str(point.id),
                            "source": payload.get("source", ""),
                            "law_name": payload.get("law_name", ""),
                            "law_id": payload.get("law_id", ""),
                            "article_no": payload.get("article_no", ""),
                            "article_title": payload.get("article_title", ""),
                            "paragraph_no": payload.get("paragraph_no", ""),
                            "chunk_type": payload.get("chunk_type", ""),
                            "category": payload.get("category", ""),
                            "chunk_index": payload.get("chunk_index", 0),
                        }
                    )
                    all_sampled.append(doc)
            
            offset = next_offset
            if offset is None:
                break

    print(f"   âœ… {len(all_sampled)}ê°œ ë¬¸ì„œ ìƒ˜í”Œë§ ì™„ë£Œ")

    # ë¶„ì•¼ë³„ ë¶„ë¥˜
    # ë²•ë ¹ ì™¸ ë¬¸ì„œì˜ source ê°’ë“¤ (ëª¨ë‘ ë…¸ë™ë²• ê´€ë ¨)
    # interpretation: í–‰ì •í•´ì„, case_law: ì£¼ìš”íŒì •ì‚¬ë¡€, moel_qa: ê³ ìš©ë…¸ë™ë¶€QA, íŒì •ì„ ë¡€: ê²°ì •ì„ ë¡€
    non_statute_sources = {"interpretation", "case_law", "moel_qa", "íŒì •ì„ ë¡€"}
    
    def classify_domain(doc) -> str:
        meta = doc.metadata or {}
        # 1. ë²•ë ¹ ì—¬ë¶€: law_name(ë˜ëŠ” law_id)ì´ ìˆìœ¼ë©´ ë²•ë ¹
        if meta.get("law_name") or meta.get("law_id"):
            category = str(meta.get("category", ""))
            if category in buckets:
                return category
            return "ê¸°íƒ€"
        # 2. ë²•ë ¹ ì™¸ ë¬¸ì„œ: sourceë¡œ ë¶„ë¥˜
        src = str(meta.get("source", ""))
        if src in non_statute_sources:
            return "ë…¸ë™ë²•_ë²•ë ¹ì™¸"
        return "ê¸°íƒ€"

    # ì…”í”Œí•´ì„œ ìˆœì„œ ëœë¤í™” (ê°™ì€ ë²•ë¥ ì´ ì—°ì†ìœ¼ë¡œ ì˜¤ëŠ” ê²ƒ ë°©ì§€)
    random.shuffle(all_sampled)
    
    for doc in all_sampled:
        domain = classify_domain(doc)
        if len(buckets[domain]) < docs_per_domain:
            buckets[domain].append(doc)
    
    # ê° ë¶„ì•¼ ë‚´ì—ì„œë„ ë‹¤ì‹œ ì…”í”Œ (ë²•ë¥ ëª… ê¸°ì¤€ ë‹¤ì–‘ì„± í™•ë³´)
    for domain in buckets:
        random.shuffle(buckets[domain])

    print(f"\nğŸ“„ ë¶„ì•¼ë³„ ë¡œë“œ ì™„ë£Œ:")
    for k, v in buckets.items():
        if v:
            # í•´ë‹¹ ë¶„ì•¼ì˜ ë¬¸ì„œ ë‹¤ì–‘ì„± ì²´í¬ (ë²•ë ¹ + ë²•ë ¹ì™¸)
            law_names = set(doc.metadata.get("law_name") for doc in v if doc.metadata.get("law_name"))
            sources = set(doc.metadata.get("source", "") for doc in v)
            non_statute_count = sum(1 for doc in v if not doc.metadata.get("law_name"))
            print(f"   ğŸ“Œ {k}: {len(v)} docs (ë²•ë ¹ {len(law_names)}ì¢…ë¥˜, ë²•ë ¹ì™¸ {non_statute_count}ê°œ)")
        else:
            print(f"   ğŸ“Œ {k}: {len(v)} docs")

    return buckets


# ---------------------------------------------------------
# [ìˆ˜ì •] ì˜¨ë„ë¥¼ ê°•ì œë¡œ 1ë¡œ ê³ ì •í•˜ëŠ” ì»¤ìŠ¤í…€ LLM í´ë˜ìŠ¤
# ---------------------------------------------------------
class ForceTemperature1ChatOpenAI(ChatOpenAI):
    def _generate(self, messages, stop=None, run_manager=None, **kwargs):
        if 'temperature' in kwargs:
            kwargs['temperature'] = 1
        return super()._generate(messages, stop=stop, run_manager=run_manager, **kwargs)

    async def _agenerate(self, messages, stop=None, run_manager=None, **kwargs):
        if 'temperature' in kwargs:
            kwargs['temperature'] = 1
        return await super()._agenerate(messages, stop=stop, run_manager=run_manager, **kwargs)


def setup_generator(model_name: str = "gpt-4o-mini") -> TestsetGenerator:
    print(f"ğŸ¤– LLM ì„¤ì • ì¤‘(ìƒì„±): {model_name}")

    generator_llm = ForceTemperature1ChatOpenAI(
        model=model_name,
        temperature=1,
    )

    embeddings = OpenAIEmbeddings(model="text-embedding-3-large")

    generator = TestsetGenerator.from_langchain(
        llm=generator_llm,
        embedding_model=embeddings
    )

    print("âœ… TestsetGenerator ì„¤ì • ì™„ë£Œ\n")
    return generator


def generate_testset(generator: TestsetGenerator, documents: list, test_size: int) -> pd.DataFrame:
    run_config = RunConfig(
        max_retries=3,
        max_wait=60,
        max_workers=4,
        timeout=120,
        exception_types=(Exception,),
    )

    testset = generator.generate_with_langchain_docs(
        documents=documents,
        testset_size=test_size,
        raise_exceptions=False,
        run_config=run_config,
    )
    return testset.to_pandas()


def main():
    parser = argparse.ArgumentParser(description="RAGAS ê¸°ë°˜ Golden Set ìƒì„±(ì»¤ìŠ¤í…€ ì¿¼í„°/ë‚œì´ë„)")
    parser.add_argument('--collection', type=str, default=None)
    parser.add_argument('--docs-per-domain', type=int, default=500, help="ë¶„ì•¼ë³„ ìƒ˜í”Œë§í•  ë¬¸ì„œ ìˆ˜(ê¸°ë³¸ 500)")
    parser.add_argument('--model', type=str, default='gpt-4o-mini', help='ìƒì„±/ë¼ë²¨ë§ì— ì‚¬ìš©í•  LLM ëª¨ë¸')
    parser.add_argument('--output', type=str, default='golden_set_quota_20.json')
    parser.add_argument('--pool-mult', type=int, default=6, help="ë¶„ì•¼ë³„ ìƒì„± í’€ í¬ê¸° ë°°ìˆ˜(ê¸°ë³¸ 6ë°°)")
    args = parser.parse_args()

    if not os.environ.get("OPENAI_API_KEY"):
        print("âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        return

    print("=" * 60)
    print("ğŸ›ï¸  RAG í‰ê°€ìš© Golden Set ìƒì„± (RAGAS + ì¿¼í„° ìƒ˜í”Œë§)")
    print("=" * 60)

    # 1) ë¶„ì•¼ë³„ ë¬¸ì„œ ì§ì ‘ ìƒ˜í”Œë§ (ìµœì í™”)
    buckets = load_documents_from_qdrant_by_domain(
        collection_name=args.collection, 
        docs_per_domain=args.docs_per_domain
    )
    
    total_docs = sum(len(v) for v in buckets.values())
    if total_docs == 0:
        print("âŒ ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. Qdrant ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return

    # ëª©í‘œ ì¿¼í„°
    domain_targets = {"ë…¸ë™ë²•": 10, "ë¯¼ì‚¬ë²•": 5, "í˜•ì‚¬ë²•": 5}

    # 2) RAGAS generator ì„¤ì •
    generator = setup_generator(args.model)

    # 3) ë¶„ì•¼ë³„ë¡œ í’€ ìƒì„±(ì¶©ë¶„íˆ í¬ê²Œ)
    frames = []
    for domain, target_n in domain_targets.items():
        # ë…¸ë™ë²•ì€ ë²•ë ¹ + ë²•ë ¹ì™¸ í•©ì³ì„œ ì‚¬ìš©
        if domain == "ë…¸ë™ë²•":
            labor_law_docs = buckets.get("ë…¸ë™ë²•", [])
            labor_extra_docs = buckets.get("ë…¸ë™ë²•_ë²•ë ¹ì™¸", [])
            docs = labor_law_docs + labor_extra_docs
        else:
            docs = buckets.get(domain, [])
        
        if not docs:
            print(f"âš ï¸  '{domain}' ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ìŠ¤í‚µí•©ë‹ˆë‹¤.")
            continue

        # ë…¸ë™ë²•ì€ ë²•ë ¹ + ë²•ë ¹ì™¸ ë¬¸ì„œë¥¼ í•¨ê»˜ ì œê³µ (RAGASê°€ ìì—°ìŠ¤ëŸ½ê²Œ ì§ˆë¬¸ ìƒì„±)
        if domain == "ë…¸ë™ë²•":
            all_labor_docs = labor_law_docs + labor_extra_docs
            random.shuffle(all_labor_docs)
            docs_for_gen = all_labor_docs[:min(len(all_labor_docs), 300)]
        else:
            docs_for_gen = docs[:min(len(docs), 300)]

        pool_n = target_n * max(2, args.pool_mult)
        print(f"\nğŸ§ª [{domain}] í’€ ìƒì„±: {pool_n}ê°œ (ëª©í‘œ {target_n})")
        df_pool = generate_testset(generator, docs_for_gen, test_size=pool_n)
        df_pool["__generated_domain_hint"] = domain
        frames.append(df_pool)

    df_all = pd.concat(frames, ignore_index=True)
    print(f"\nâœ… ì „ì²´ í’€ ìƒì„± ì™„ë£Œ: {len(df_all)} rows")

    # 5) ë‹µë³€ í…œí”Œë¦¿ ì¬ì‘ì„±
    labeler = build_labeler_llm(args.model)
    print("\nğŸ“ ë‹µë³€ í…œí”Œë¦¿ ì¬ì‘ì„± ì¤‘...")
    df_all = reformat_answers(df_all, labeler)

    # 6) ë¼ë²¨ë§
    print("\nğŸ·ï¸  ë¼ë²¨ë§ ì¤‘(ë¶„ì•¼/ë…¸ë™-ë¹„ë²•ë ¹ì°¸ê³ )...")
    df_labeled = label_rows(df_all, labeler)

    # 7) ë¶„ì•¼ë³„ ìƒ˜í”Œë§
    print("\nğŸ¯ ìƒ˜í”Œë§ ì¤‘...")
    df_selected = sample_with_quota(df_labeled, domain_targets)

    # 8) ì €ì¥
    script_dir = Path(__file__).parent
    output_dir = script_dir.parent / "data" / "evaluation"
    output_dir.mkdir(parents=True, exist_ok=True)

    output_path = output_dir / args.output
    df_selected.to_json(output_path, orient='records', force_ascii=False, indent=2)

    print(f"\nğŸ’¾ ì €ì¥ ì™„ë£Œ: {output_path}")
    print("\nğŸ“Š ìµœì¢… ë¶„í¬:")
    print(df_selected["domain"].value_counts().to_string())
    print("\n(ë…¸ë™ë²•) ë¹„ë²•ë ¹ì°¸ê³ ê°€ëŠ¥ ê°œìˆ˜:", int(df_selected[df_selected["domain"] == "ë…¸ë™ë²•"]["labor_non_statute_ok"].sum()))


if __name__ == '__main__':
    main()