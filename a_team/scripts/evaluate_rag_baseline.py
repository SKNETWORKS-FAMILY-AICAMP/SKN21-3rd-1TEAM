"""
ë…¸ë™ë²• RAG ì±—ë´‡ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸

Golden Datasetì„ ì‚¬ìš©í•˜ì—¬ baseline RAG ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.
Ragas ë©”íŠ¸ë¦­(Faithfulness, Answer Relevancy, Context Precision/Recall)ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

Usage:
    # ê¸°ë³¸ ì‹¤í–‰
    uv run a_team/scripts/evaluate_rag.py
    
    # ìƒ˜í”Œ ìˆ˜ ì§€ì • (í…ŒìŠ¤íŠ¸ìš©)
    uv run a_team/scripts/evaluate_rag.py --sample 10
    
    # ì»¤ìŠ¤í…€ ê³¨ë“ ì…‹ ê²½ë¡œ
    uv run a_team/scripts/evaluate_rag.py --golden-set a_team/data/evaluation/golden_set_quota_20.json
"""

import os
import sys
import argparse
import warnings
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any

import pandas as pd
from dotenv import load_dotenv
from tqdm import tqdm

# Ragas í‰ê°€ ë©”íŠ¸ë¦­
from ragas import evaluate
from ragas.metrics import (
    Faithfulness,
    ResponseRelevancy,
    LLMContextPrecisionWithoutReference,
    LLMContextRecall,
)
from ragas.embeddings import LangchainEmbeddingsWrapper
from ragas.llms import LangchainLLMWrapper
from datasets import Dataset

# LangChain
from langchain_openai import ChatOpenAI
from langchain_huggingface import HuggingFaceEmbeddings

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
_SCRIPT_DIR = Path(__file__).parent
load_dotenv(dotenv_path=_SCRIPT_DIR / ".env")

# ê²½ê³  ë©”ì‹œì§€ í•„í„°ë§
warnings.filterwarnings("ignore", category=DeprecationWarning)


# ============================================================
# Golden Dataset ë¡œë“œ
# ============================================================
def load_golden_dataset(path: str) -> pd.DataFrame:
    """
    Golden Dataset JSON íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.

    Args:
        path: JSON íŒŒì¼ ê²½ë¡œ

    Returns:
        DataFrame with columns: user_input, reference (ground truth)
    """
    if not os.path.exists(path):
        raise FileNotFoundError(f"Golden Datasetì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}")

    df = pd.read_json(path)

    # Ragas 0.4.x ì»¬ëŸ¼ëª… í™•ì¸ ë° ë§¤í•‘
    # ì˜ˆìƒ ì»¬ëŸ¼: user_input, reference, reference_contexts
    required_cols = []

    # ì§ˆë¬¸ ì»¬ëŸ¼ ì°¾ê¸°
    question_col = None
    for col in ['user_input', 'question', 'query']:
        if col in df.columns:
            question_col = col
            break

    if question_col is None:
        raise ValueError(f"ì§ˆë¬¸ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì»¬ëŸ¼: {list(df.columns)}")

    # ì •ë‹µ ì»¬ëŸ¼ ì°¾ê¸°
    answer_col = None
    for col in ['reference', 'ground_truth', 'answer', 'expected_answer']:
        if col in df.columns:
            answer_col = col
            break

    if answer_col is None:
        raise ValueError(f"ì •ë‹µ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì»¬ëŸ¼: {list(df.columns)}")

    # ì»¬ëŸ¼ëª… ì •ê·œí™”
    df = df.rename(columns={
        question_col: 'user_input',
        answer_col: 'reference'
    })

    print(f"âœ… Golden Dataset ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ ìƒ˜í”Œ")
    print(f"   ì§ˆë¬¸ ì»¬ëŸ¼: {question_col} â†’ user_input")
    print(f"   ì •ë‹µ ì»¬ëŸ¼: {answer_col} â†’ reference")

    return df


# ============================================================
# Baseline ëª¨ë¸ ì¶”ë¡ 
# ============================================================
def run_inference(questions: List[str], verbose: bool = True) -> List[Dict[str, Any]]:
    """
    Baseline RAG ëª¨ë¸ë¡œ ê° ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        questions: ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
        verbose: ì§„í–‰ ìƒí™© ì¶œë ¥ ì—¬ë¶€

    Returns:
        List of {answer: str, contexts: List[str]}
    """
    # baseline.pyì—ì„œ ì±—ë´‡ ì´ˆê¸°í™” í•¨ìˆ˜ ìž„í¬íŠ¸
    from chatbot_baseline import initialize_rag_chatbot

    print("\nðŸ¤– Baseline ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
    chatbot = initialize_rag_chatbot()
    print("âœ… ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ\n")

    results = []

    iterator = tqdm(questions, desc="ì¶”ë¡  ì¤‘") if verbose else questions

    for i, question in enumerate(iterator):
        try:
            if verbose:
                print(f"\nðŸ” ì§ˆë¬¸ [{i+1}]: {question}")

            # Agent ì‹¤í–‰ (intermediate_stepsë¡œ ê²€ìƒ‰ ê²°ê³¼ ì¶”ì¶œ)
            response = chatbot.invoke({
                "input": question,
                "chat_history": []
            })

            answer = response.get("output", "")

            # ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
            # AgentExecutorì˜ intermediate_stepsì—ì„œ tool í˜¸ì¶œ ê²°ê³¼ ì¶”ì¶œ
            contexts = []
            intermediate_steps = response.get("intermediate_steps", [])

            for step in intermediate_steps:
                if len(step) >= 2:
                    tool_output = step[1]
                    if isinstance(tool_output, str) and len(tool_output) > 0:
                        # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë¬¸ì„œ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
                        docs = tool_output.split("\n\n")
                        contexts.extend([doc.strip()
                                        for doc in docs if doc.strip()])

            # ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ëŒ€ì‹  ë‹µë³€ ì¼ë¶€ ì‚¬ìš© (fallback)
            if not contexts:
                contexts = ["(ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ)"]

            results.append({
                "answer": answer,
                "contexts": contexts
            })

        except Exception as e:
            print(f"\nâš ï¸ ì¶”ë¡  ì‹¤íŒ¨: {question[:50]}... - {e}")
            results.append({
                "answer": f"[ì˜¤ë¥˜] {str(e)}",
                "contexts": []
            })

    return results


# ============================================================
# Ragas í‰ê°€
# ============================================================
def evaluate_with_ragas(
    questions: List[str],
    answers: List[str],
    contexts: List[List[str]],
    references: List[str],
    llm_model: str = "gpt-4o-mini",
    embedding_model: Any = None
) -> Dict[str, Any]:
    """
    Ragas ë©”íŠ¸ë¦­ìœ¼ë¡œ RAG ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.

    Args:
        questions: ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
        answers: ìƒì„±ëœ ë‹µë³€ ë¦¬ìŠ¤íŠ¸
        contexts: ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        references: ì •ë‹µ(Ground Truth) ë¦¬ìŠ¤íŠ¸
        llm_model: í‰ê°€ì— ì‚¬ìš©í•  LLM ëª¨ë¸
        embedding_model: í‰ê°€ì— ì‚¬ìš©í•  ìž„ë² ë”© ëª¨ë¸

    Returns:
        í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    print("\nðŸ“Š Ragas í‰ê°€ ì‹œìž‘...")

    # Ragas Dataset ìƒì„±
    eval_dataset = Dataset.from_dict({
        "user_input": questions,
        "response": answers,
        "retrieved_contexts": contexts,
        "reference": references
    })

    # í‰ê°€ìš© LLM ë° Embeddings ì„¤ì •
    eval_llm = LangchainLLMWrapper(ChatOpenAI(model=llm_model, temperature=0))
    eval_embeddings = LangchainEmbeddingsWrapper(
        embedding_model) if embedding_model else None

    # ë©”íŠ¸ë¦­ ì •ì˜ (Ragas 0.4.x class-based API)
    metrics = [
        Faithfulness(),
        ResponseRelevancy(),
        LLMContextPrecisionWithoutReference(),
        LLMContextRecall(),
    ]

    # í‰ê°€ ì‹¤í–‰
    try:
        result = evaluate(
            dataset=eval_dataset,
            metrics=metrics,
            llm=eval_llm,
            embeddings=eval_embeddings,
            raise_exceptions=False
        )

        print("âœ… Ragas í‰ê°€ ì™„ë£Œ")
        return result

    except Exception as e:
        print(f"âŒ Ragas í‰ê°€ ì‹¤íŒ¨: {e}")
        raise


# ============================================================
# ê²°ê³¼ ì €ìž¥
# ============================================================
def save_results(
    df: pd.DataFrame,
    ragas_result: Dict,
    output_path: str
):
    """
    í‰ê°€ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ìž¥í•©ë‹ˆë‹¤.
    êµ¬ì¡°: { "summary": {metrics...}, "results": [records...] }

    Args:
        df: ì›ë³¸ ë°ì´í„°í”„ë ˆìž„ (ì§ˆë¬¸, ì •ë‹µ í¬í•¨)
        ragas_result: Ragas í‰ê°€ ê²°ê³¼
        output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
    """
    import json

    # Ragas ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
    result_df = ragas_result.to_pandas()

    # ì¤‘ë³µ ì»¬ëŸ¼ ì œê±° (ì›ë³¸ dfì— ì´ë¯¸ ìžˆëŠ” ì»¬ëŸ¼ì€ result_dfì—ì„œ ì œì™¸)
    cols_to_use = result_df.columns.difference(df.columns)

    # ì›ë³¸ ë°ì´í„°ì™€ ê²°í•©
    final_df = pd.concat(
        [df.reset_index(drop=True),
         result_df[cols_to_use].reset_index(drop=True)],
        axis=1
    )

    # ìš”ì•½ ì •ë³´ ìƒì„± (í‰ê·  ì ìˆ˜)
    summary = {
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "metrics": {}
    }

    numeric_cols = result_df.select_dtypes(include=['number']).columns
    for col in numeric_cols:
        summary["metrics"][col] = float(result_df[col].mean())

    # ìµœì¢… ì €ìž¥ ë°ì´í„° êµ¬ì¡°
    output_data = {
        "summary": summary,
        "results": final_df.to_dict(orient='records')
    }

    # ì €ìž¥
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)

    print(f"\nðŸ’¾ ê²°ê³¼ ì €ìž¥ ì™„ë£Œ: {output_path}")
    print(f"   (ìƒë‹¨ summary í¬í•¨)")


# ============================================================
# ë©”ì¸ í•¨ìˆ˜
# ============================================================
def main():
    parser = argparse.ArgumentParser(description='ë…¸ë™ë²• RAG ì±—ë´‡ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸')
    parser.add_argument(
        '--golden-set',
        type=str,
        default='a_team/data/evaluation/golden_set_quota_10.json',
        help='Golden Dataset JSON ê²½ë¡œ'
    )
    parser.add_argument(
        '--output',
        type=str,
        default=None,
        help='ê²°ê³¼ ì €ìž¥ ê²½ë¡œ (ê¸°ë³¸ê°’: golden_set í´ë”ì— ì €ìž¥)'
    )
    parser.add_argument(
        '--sample',
        type=int,
        default=0,
        help='í‰ê°€í•  ìƒ˜í”Œ ìˆ˜ (0ì´ë©´ ì „ì²´ í‰ê°€)'
    )
    parser.add_argument(
        '--eval-model',
        type=str,
        default='gpt-4o-mini',
        help='Ragas í‰ê°€ì— ì‚¬ìš©í•  LLM ëª¨ë¸'
    )
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='ë°ì´í„° ë¡œë“œë§Œ í…ŒìŠ¤íŠ¸í•˜ê³  ì¢…ë£Œ'
    )
    args = parser.parse_args()

    # API Key í™•ì¸
    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return

    print("=" * 60)
    print("ðŸ›ï¸  ë…¸ë™ë²• RAG ì±—ë´‡ í‰ê°€ ì‹œìž‘")
    print("=" * 60)

    # 1. Golden Dataset ë¡œë“œ
    df = load_golden_dataset(args.golden_set)

    # ìƒ˜í”Œë§
    if args.sample > 0 and args.sample < len(df):
        df = df.sample(n=args.sample, random_state=42).reset_index(drop=True)
        print(f"âœ‚ï¸  {args.sample}ê°œ ìƒ˜í”Œë¡œ ì œí•œ")

    # Dry run ëª¨ë“œ
    if args.dry_run:
        print("\nðŸ§ª Dry Run ëª¨ë“œ: ë°ì´í„° ë¡œë“œë§Œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.")
        print(f"\nìƒ˜í”Œ ë°ì´í„°:")
        print(df.head(3).to_string())
        return

    # 2. Baseline ëª¨ë¸ ì¶”ë¡ 
    questions = df['user_input'].tolist()
    references = df['reference'].tolist()

    print(f"\nðŸ“ {len(questions)}ê°œ ì§ˆë¬¸ì— ëŒ€í•´ ì¶”ë¡  ì‹œìž‘...")
    inference_results = run_inference(questions)

    # ê²°ê³¼ ì¶”ì¶œ
    answers = [r['answer'] for r in inference_results]
    contexts = [r['contexts'] for r in inference_results]

    # DataFrameì— ì¶”ë¡  ê²°ê³¼ ì¶”ê°€
    df['generated_answer'] = answers
    df['retrieved_contexts'] = [str(c) for c in contexts]  # ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìžì—´ë¡œ

    # 3. ìž„ë² ë”© ëª¨ë¸ ë¡œë“œ (Qwen) - Ragas í‰ê°€ìš©
    print(f"\nðŸš€ í‰ê°€ìš© ìž„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘ (Qwen/Qwen3-Embedding-0.6B)...")
    embeddings = HuggingFaceEmbeddings(
        model_name="Qwen/Qwen3-Embedding-0.6B",
        model_kwargs={'trust_remote_code': True},
        encode_kwargs={'normalize_embeddings': True}
    )

    # 4. Ragas í‰ê°€
    ragas_result = evaluate_with_ragas(
        questions=questions,
        answers=answers,
        contexts=contexts,
        references=references,
        llm_model=args.eval_model,
        embedding_model=embeddings
    )

    # 4. ê²°ê³¼ ì €ìž¥ (ì¶œë ¥ ì „ì— ë¨¼ì € ì €ìž¥!)
    if args.output:
        output_path = args.output
    else:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = Path(args.golden_set).parent
        output_path = output_dir / f"evaluation_results_{timestamp}.json"

    try:
        save_results(df, ragas_result, str(output_path))
    except Exception as e:
        print(f"âš ï¸ ê²°ê³¼ ì €ìž¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # 5. ê²°ê³¼ ì¶œë ¥ (DataFrame ì‚¬ìš©)
    print("\n" + "=" * 60)
    print("ðŸ“Š í‰ê°€ ê²°ê³¼ ìš”ì•½")
    print("=" * 60)

    try:
        # Ragas ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜í•˜ì—¬ í‰ê·  ê³„ì‚°
        result_df = ragas_result.to_pandas()
        numeric_cols = result_df.select_dtypes(include=['number']).columns

        for col in numeric_cols:
            avg_score = result_df[col].mean()
            print(f"  â€¢ {col}: {avg_score:.4f}")

    except Exception as e:
        print(f"âš ï¸ ê²°ê³¼ ì¶œë ¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ë°ì´í„°ëŠ” ì €ìž¥ë¨): {e}")

    print("\n" + "=" * 60)
    print("âœ… í‰ê°€ ì™„ë£Œ!")
    print("=" * 60)


if __name__ == "__main__":
    main()
