################################################
# A-TEAM ë²•ë¥  RAG ì±—ë´‡ (LangGraph V5)
# - ê²€ìƒ‰ ë‹¤ì¤‘ ì¿¼ë¦¬ + rerank + ë…¸ë™ë²• ë¹„ë²•ë ¹ ë¬¸ì„œ ê°€ì¤‘ì¹˜
# - ê·¼ê±° ìŠ¤ë‹ˆí«ì„ ì •ëˆëœ bulletë¡œ ì „ë‹¬í•´ ì¸ìš© ê°•ì œ
# - Top-K ì†Œí­ ìƒí–¥, ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ
################################################

import os
import re
import warnings
from pathlib import Path
from typing import Annotated, TypedDict, Sequence, Optional, List, Literal, Any
from dotenv import load_dotenv

from qdrant_client import QdrantClient
from langchain_qdrant import QdrantVectorStore
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_openai import ChatOpenAI
from langchain_core.documents import Document, BaseDocumentCompressor
from langchain_community.tools import TavilySearchResults
from pydantic import BaseModel, Field
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages

_DOTENV_PATH = Path(__file__).with_name(".env")
load_dotenv(dotenv_path=_DOTENV_PATH)


class AgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]
    user_query: str
    query_analysis: Optional[dict]
    retrieved_docs: Optional[List[Document]]
    case_law_results: Optional[List[dict]]
    generated_answer: Optional[str]
    next_action: Optional[str]


class JinaReranker(BaseDocumentCompressor):
    model_name: str = "jinaai/jina-reranker-v2-base-multilingual"
    top_n: int = 6
    model: Any = None
    tokenizer: Any = None

    class Config:
        arbitrary_types_allowed = True
        extra = "allow"

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)
        self.model = AutoModelForSequenceClassification.from_pretrained(
            self.model_name, trust_remote_code=True, dtype="auto"
        )
        self.model.eval()

    def compress_documents(self, documents: Sequence[Document], query: str, callbacks: Optional[Any] = None) -> Sequence[Document]:
        if not documents:
            return []

        pairs = [[query, doc.page_content] for doc in documents]
        with torch.no_grad():
            inputs = self.tokenizer(pairs, padding=True, truncation=True, return_tensors="pt", max_length=512)
            scores = self.model(**inputs).logits.squeeze(-1).float().cpu()
            scores = torch.sigmoid(scores).tolist()
            if not isinstance(scores, list):
                scores = [scores]

        top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[: self.top_n]
        final_docs = []
        for i in top_indices:
            doc = documents[i]
            doc.metadata["relevance_score"] = scores[i]
            final_docs.append(doc)
        return final_docs


class QueryAnalysis(BaseModel):
    category: str = Field(description="ë²•ë¥  ë¶„ì•¼: ë…¸ë™ë²•, í˜•ì‚¬ë²•, ë¯¼ì‚¬ë²•, ê¸°íƒ€ ì¤‘ í•˜ë‚˜")
    needs_clarification: bool = Field(default=False, description="ì§ˆë¬¸ì´ ê·¹ë„ë¡œ ëª¨í˜¸í•˜ì—¬ ë‹µë³€ ë¶ˆê°€ëŠ¥í•œì§€")
    needs_case_law: bool = Field(default=False, description="ëŒ€ë²•ì› íŒë¡€ ê²€ìƒ‰ì´ í•„ìš”í•œì§€")
    clarification_question: str = Field(default="", description="ëª…í™•í™” í•„ìš” ì‹œ ì‚¬ìš©ìì—ê²Œ ë¬¼ì–´ë³¼ ì§ˆë¬¸")


def create_analyze_query_node(llm: ChatOpenAI):
    structured_llm = llm.with_structured_output(QueryAnalysis)
    analyze_prompt = ChatPromptTemplate.from_messages([
        (
            "system",
            """ë‹¹ì‹ ì€ ë²•ë¥  ì§ˆë¬¸ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

1. category: ì§ˆë¬¸ì˜ ë²•ë¥  ë¶„ì•¼
   - "ë…¸ë™ë²•": ê·¼ë¡œê¸°ì¤€ë²•, ì„ê¸ˆ, í‡´ì§ê¸ˆ, í•´ê³ , ì‚°ì¬, ì£¼íœ´ìˆ˜ë‹¹ ë“±
   - "í˜•ì‚¬ë²•": ë²”ì£„, í˜•ë²Œ, ìˆ˜ì‚¬, ì¬íŒ, ê³ ì†Œ/ê³ ë°œ ë“±
   - "ë¯¼ì‚¬ë²•": ê³„ì•½, ì†í•´ë°°ìƒ, ì†Œìœ ê¶Œ, ì±„ê¶Œ ë“±
   - "ê¸°íƒ€": ìœ„ ì¹´í…Œê³ ë¦¬ì— ì†í•˜ì§€ ì•ŠëŠ” ë²•ë¥  ì§ˆë¬¸

2. needs_clarification: ì§ˆë¬¸ì´ ê·¹ë„ë¡œ ëª¨í˜¸í•˜ì—¬ ì–´ë–¤ ë‹µë³€ë„ ë¶ˆê°€ëŠ¥í•œì§€ (true/false)
3. needs_case_law: ëŒ€ë²•ì› íŒë¡€ê°€ í•„ìš”í•œì§€ (true/false)
4. clarification_question: needs_clarificationì´ trueì¼ ë•Œë§Œ ì‘ì„±""",
        ),
        ("human", "{query}"),
    ])

    def analyze_query(state: AgentState) -> AgentState:
        query = state["user_query"]
        print("ğŸ” [ì§ˆë¬¸ ë¶„ì„ ì¤‘...]")
        chain = analyze_prompt | structured_llm
        analysis: QueryAnalysis = chain.invoke({"query": query})
        print(f"ğŸ“‹ [ë¶„ì„ ê²°ê³¼] ë¶„ì•¼: {analysis.category} / íŒë¡€ í•„ìš”: {'ì˜ˆ' if analysis.needs_case_law else 'ì•„ë‹ˆì˜¤'}")
        return {"query_analysis": analysis.model_dump()}

    return analyze_query


def create_clarify_node(llm: ChatOpenAI):
    def request_clarification(state: AgentState) -> AgentState:
        analysis = state.get("query_analysis", {})
        clarification_q = analysis.get("clarification_question", "") or "ì§ˆë¬¸ì„ ì¢€ ë” êµ¬ì²´ì ìœ¼ë¡œ ì•Œë ¤ì£¼ì„¸ìš”. ìƒí™©, ìƒëŒ€ë°©, ìŸì , ì›í•˜ëŠ” ê²°ê³¼ë¥¼ ì ì–´ì£¼ì‹œë©´ ë” ì •í™•íˆ ë‹µë³€ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        print("â“ [ëª…í™•í™” ìš”ì²­]")
        answer = f"""ì•ˆë…•í•˜ì„¸ìš”! ì§ˆë¬¸ì„ ë” ì´í•´í•˜ê¸° ìœ„í•´ ëª‡ ê°€ì§€ë¥¼ í™•ì¸í•˜ê³  ì‹¶ì–´ìš”.

{clarification_q}

ìœ„ ë‚´ìš©ì„ í¬í•¨í•´ ë‹¤ì‹œ ì•Œë ¤ì£¼ì‹œë©´ ë” ì •í™•íˆ ë„ì›€ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ğŸ˜Š"""
        return {"generated_answer": answer, "next_action": "end"}

    return request_clarification


# ----------------------------
# ê²€ìƒ‰ ê´€ë ¨ í—¬í¼
# ----------------------------
NON_STATUTE_SOURCES = {"interpretation", "case_law", "moel_qa", "íŒì •ì„ ë¡€"}


def expand_queries(query: str) -> List[str]:
    variants = {query.strip()}
    # ì¡°ì‚¬/ë¶ˆìš©ì–´ ì¼ë¶€ ì œê±° ì‹œë„
    compact = re.sub(r"[\s]+", " ", query).strip()
    variants.add(compact)
    # ê´„í˜¸/ìŠ¬ë˜ì‹œ ì œê±° ë²„ì „
    variants.add(re.sub(r"[()\[\]/]", " ", compact))
    # ì˜ì–´ ì§ˆë¬¸ ëŒ€ì‘: í•œêµ­ì–´ ë²ˆì—­ íŒíŠ¸ê°€ ì—†ë‹¤ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    return [v for v in variants if v]


def dedup_documents(docs: List[Document]) -> List[Document]:
    seen = set()
    unique = []
    for doc in docs:
        key = doc.metadata.get("id") or (doc.metadata.get("source"), doc.metadata.get("law_name"), doc.metadata.get("article_no"), doc.page_content[:80])
        if key in seen:
            continue
        seen.add(key)
        unique.append(doc)
    return unique


def boost_non_statute_score(doc: Document, boost: float = 0.15) -> float:
    score = doc.metadata.get("relevance_score", 0.0)
    if str(doc.metadata.get("source", "")) in NON_STATUTE_SOURCES:
        score += boost
    return score


def format_context_snippets(docs: List[Document], max_docs: int = 5, max_chars: int = 500) -> str:
    parts = []
    for i, doc in enumerate(docs[:max_docs], 1):
        meta = doc.metadata or {}
        law_name = meta.get("law_name", "")
        article = meta.get("article_no", "")
        title = meta.get("article_title") or meta.get("title", "")
        source = meta.get("source", "")
        snippet = doc.page_content[: max_chars].strip()
        header = f"[ê·¼ê±° {i}]"
        if law_name:
            header += f" {law_name}"
            if article:
                header += f" ì œ{article}ì¡°"
        if title:
            header += f" - {title}"
        if source and not law_name:
            header += f" ({source})"
        parts.append(f"{header}\n{snippet}\n")
    return "\n".join(parts) if parts else "(ê´€ë ¨ ë²•ë ¹ ë¬¸ì„œê°€ ê²€ìƒ‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤)"


def create_search_node(vectorstore: QdrantVectorStore):
    def search_documents(state: AgentState) -> AgentState:
        query = state["user_query"]
        print(f"ğŸ” [ë²•ë ¹ ê²€ìƒ‰] ì¿¼ë¦¬: {query[:50]}...")

        variants = expand_queries(query)
        all_docs: List[Document] = []
        for q in variants:
            try:
                res = vectorstore.similarity_search_with_score(q, k=12)
                all_docs.extend([doc for doc, score in res])
            except Exception as e:
                print(f"âš ï¸  [ê²€ìƒ‰ ì˜¤ë¥˜] {e}")

        all_docs = dedup_documents(all_docs)
        if not all_docs:
            print("âš ï¸  [ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ]")
            return {"retrieved_docs": []}

        try:
            reranker = JinaReranker(top_n=6)
            reranked = reranker.compress_documents(all_docs, query)
            if reranked:
                # ë…¸ë™ë²• ë¹„ë²•ë ¹ ë¬¸ì„œ ê°€ì¤‘ì¹˜ ë¶€ì—¬ í›„ ì¬ì •ë ¬
                reranked = sorted(
                    reranked,
                    key=lambda d: boost_non_statute_score(d),
                    reverse=True,
                )
                docs = reranked[:6]
                print(f"âœ… [ë¦¬ë­í‚¹ ì™„ë£Œ] {len(docs)}ê°œ ë¬¸ì„œ ì„ ë³„")
            else:
                docs = all_docs[:6]
                print("âš ï¸  [ë¦¬ë­í‚¹ ê²°ê³¼ ì—†ìŒ] ì›ë³¸ ìƒìœ„ 6ê°œ ì‚¬ìš©")
        except Exception as e:
            print(f"âš ï¸  [ë¦¬ë­í‚¹ ì˜¤ë¥˜] {e}")
            docs = all_docs[:6]

        for i, d in enumerate(docs, 1):
            print(f"   [{i}] score={d.metadata.get('relevance_score', 0):.4f} | {d.page_content[:40]}...")

        return {"retrieved_docs": docs}

    return search_documents


def create_case_law_search_node(llm: ChatOpenAI):
    def search_case_law(state: AgentState) -> AgentState:
        query = state["user_query"]
        analysis = state.get("query_analysis", {})
        category = analysis.get("category", "ê¸°íƒ€")

        print("âš–ï¸  [íŒë¡€ ê²€ìƒ‰] ëŒ€ë²•ì› íŒë¡€ ì›¹ ê²€ìƒ‰ ì¤‘...")
        tavily_api_key = os.getenv("TAVILY_API_KEY")
        if not tavily_api_key:
            print("âš ï¸  [íŒë¡€ ê²€ìƒ‰ ìŠ¤í‚µ] TAVILY_API_KEY ë¯¸ì„¤ì •")
            return {"case_law_results": []}

        try:
            search_tool = TavilySearchResults(
                max_results=3,
                search_depth="advanced",
                include_answer=True,
                include_raw_content=False,
            )
            search_query = f"ëŒ€ë²•ì› íŒë¡€ {category} {query}"
            results = search_tool.invoke({"query": search_query})
            case_laws = []
            for r in results:
                case_laws.append(
                    {
                        "title": r.get("title", ""),
                        "url": r.get("url", ""),
                        "content": r.get("content", "")[:400],
                    }
                )
            print(f"âœ… [íŒë¡€ ê²€ìƒ‰ ì™„ë£Œ] {len(case_laws)}ê±´")
            return {"case_law_results": case_laws}
        except Exception as e:
            print(f"âš ï¸  [íŒë¡€ ê²€ìƒ‰ ì˜¤ë¥˜] {e}")
            return {"case_law_results": []}

    return search_case_law


def create_generate_node(llm: ChatOpenAI):
    answer_prompt = ChatPromptTemplate.from_messages([
        (
            "system",
            """ë‹¹ì‹ ì€ ë²•ë¥  ì „ë¬¸ AI 'A-TEAM ë´‡'ì…ë‹ˆë‹¤.
- ê²€ìƒ‰ëœ ê·¼ê±°ë¥¼ ì¸ìš©í•˜ì—¬ ë‹µë³€í•©ë‹ˆë‹¤.
- ë‹µë³€ êµ¬ì¡°: ğŸ“Œ ê²°ë¡  â†’ ğŸ“– ë²•ì  ê·¼ê±° â†’ ğŸ’¡ ì¶”ê°€ ì„¤ëª…
- ê·¼ê±°ë§ˆë‹¤ [ë²•ë ¹ëª… ì œNì¡°], [íŒë¡€: ì œëª©] í˜•íƒœë¡œ í‘œê¸°í•˜ê³ , ì¡´ì¬í•˜ëŠ” ê·¼ê±°ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
- ë¶ˆí™•ì‹¤í•˜ë©´ ì¶”ì¸¡í•˜ì§€ ë§ê³  í•œê³„ë¥¼ ëª…ì‹œí•©ë‹ˆë‹¤.
- í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€í•©ë‹ˆë‹¤.""",
        ),
        (
            "human",
            """ì§ˆë¬¸ ë¶„ì•¼: {category}
ì‚¬ìš©ì ì§ˆë¬¸: {query}

ğŸ“š ê·¼ê±° ìŠ¤ë‹ˆí«:
{context}

âš–ï¸ ê´€ë ¨ íŒë¡€:
{case_law}

ìœ„ ê·¼ê±°ë¥¼ ì¸ìš©í•´ ë‹µë³€í•˜ì„¸ìš”. ê° ë‹¨ë½ì— ê·¼ê±°ë¥¼ ë¶™ì´ê³ , ê·¼ê±°ê°€ ì—†ìœ¼ë©´ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ì„¸ìš”.""",
        ),
    ])

    def generate_answer(state: AgentState) -> AgentState:
        query = state["user_query"]
        analysis = state.get("query_analysis", {})
        category = analysis.get("category", "ê¸°íƒ€")
        docs = state.get("retrieved_docs", []) or []
        case_laws = state.get("case_law_results", []) or []

        print("ğŸ’¬ [ë‹µë³€ ìƒì„± ì¤‘...]")

        context = format_context_snippets(docs, max_docs=5, max_chars=500)

        if case_laws:
            case_parts = []
            for i, case in enumerate(case_laws, 1):
                case_parts.append(f"[íŒë¡€ {i}] {case.get('title','')}: {case.get('content','')}")
            case_law_context = "\n".join(case_parts)
        else:
            case_law_context = "(ê´€ë ¨ íŒë¡€ ì •ë³´ ì—†ìŒ)"

        if not docs and not case_laws:
            answer = """ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ê·¼ê±°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•˜ê±°ë‚˜ ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”. ë³µì¡í•œ ì‚¬ì•ˆì´ë©´ ì „ë¬¸ ë²•ë¥  ìƒë‹´ì„ ê¶Œì¥ë“œë¦½ë‹ˆë‹¤."""
        else:
            chain = answer_prompt | llm
            response = chain.invoke(
                {
                    "category": category,
                    "query": query,
                    "context": context,
                    "case_law": case_law_context,
                }
            )
            answer = response.content

        print("âœ… [ë‹µë³€ ìƒì„± ì™„ë£Œ]")
        return {"generated_answer": answer}

    return generate_answer


def route_after_analysis(state: AgentState) -> Literal["clarify", "search"]:
    analysis = state.get("query_analysis", {})
    if analysis.get("needs_clarification", False):
        return "clarify"
    return "search"


def route_after_search(state: AgentState) -> Literal["case_law_search", "generate"]:
    analysis = state.get("query_analysis", {})
    if analysis.get("needs_case_law", False):
        return "case_law_search"
    return "generate"


def initialize_resources():
    COLLECTION_NAME = os.getenv("QDRANT_COLLECTION_NAME")
    QDRANT_URL = os.getenv("QDRANT_URL")
    QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")
    if not QDRANT_API_KEY:
        raise ValueError("QDRANT_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")

    print("ğŸ”§ ì„¤ì • ë¡œë“œ ì™„ë£Œ")
    print("\nğŸš€ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘ (Qwen/Qwen3-Embedding-0.6B)...")
    embeddings = HuggingFaceEmbeddings(
        model_name="Qwen/Qwen3-Embedding-0.6B",
        model_kwargs={"trust_remote_code": True},
        encode_kwargs={"normalize_embeddings": True},
    )
    print("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

    print("\nğŸ“¡ Qdrant ì—°ê²° ì¤‘...")
    warnings.filterwarnings("ignore", message="Api key is used with an insecure connection")
    client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY, timeout=30, prefer_grpc=False)
    print("âœ… Qdrant ì—°ê²° ì™„ë£Œ")

    print("\nğŸ—‚ï¸  ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” ì¤‘...")
    vectorstore = QdrantVectorStore(
        client=client,
        collection_name=COLLECTION_NAME,
        embedding=embeddings,
        content_payload_key="text",
    )
    print("âœ… ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” ì™„ë£Œ")
    return {"embeddings": embeddings, "vectorstore": vectorstore}


def initialize_langgraph_chatbot():
    resources = initialize_resources()
    vectorstore = resources["vectorstore"]

    print("\nğŸ¤– LLM ì„¤ì • ì¤‘...")
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0, streaming=True)
    print("âœ… LLM ì„¤ì • ì™„ë£Œ")

    print("\nâš™ï¸  LangGraph ë…¸ë“œ ìƒì„± ì¤‘...")
    analyze_node = create_analyze_query_node(llm)
    clarify_node = create_clarify_node(llm)
    search_node = create_search_node(vectorstore)
    case_law_node = create_case_law_search_node(llm)
    generate_node = create_generate_node(llm)
    print("âœ… ë…¸ë“œ ìƒì„± ì™„ë£Œ")

    workflow = StateGraph(AgentState)
    workflow.add_node("analyze", analyze_node)
    workflow.add_node("clarify", clarify_node)
    workflow.add_node("search", search_node)
    workflow.add_node("case_law_search", case_law_node)
    workflow.add_node("generate", generate_node)

    workflow.set_entry_point("analyze")
    workflow.add_conditional_edges("analyze", route_after_analysis, {"clarify": "clarify", "search": "search"})
    workflow.add_edge("clarify", END)
    workflow.add_conditional_edges("search", route_after_search, {"case_law_search": "case_law_search", "generate": "generate"})
    workflow.add_edge("case_law_search", "generate")
    workflow.add_edge("generate", END)

    graph = workflow.compile()
    print("âœ… LangGraph êµ¬ì„± ì™„ë£Œ")
    return graph


def main():
    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ ì˜¤ë¥˜: OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    if not os.getenv("TAVILY_API_KEY"):
        print("âš ï¸  ê²½ê³ : TAVILY_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. íŒë¡€ ê²€ìƒ‰ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.\n")

    try:
        print("\n" + "=" * 60)
        print("ğŸš€ A-TEAM ë²•ë¥  RAG ì±—ë´‡ (LangGraph V5) ì´ˆê¸°í™”")
        print("=" * 60 + "\n")

        graph = initialize_langgraph_chatbot()

        print("\n" + "=" * 60)
        print("âœ… ğŸ¤– A-TEAM ë²•ë¥  ì±—ë´‡ ì¤€ë¹„ ì™„ë£Œ (V5)")
        print("=" * 60)
        print("\nì‚¬ìš© ë°©ë²•: ë…¸ë™ë²•/í˜•ì‚¬ë²•/ë¯¼ì‚¬ë²• ì§ˆë¬¸ì— ë‹µë³€, íŒë¡€ í•„ìš” ì‹œ ì›¹ ê²€ìƒ‰, ëª¨í˜¸í•˜ë©´ ëª…í™•í™” ìš”ì²­")
        print("'exit', 'quit', 'ì¢…ë£Œ'ë¡œ ì¢…ë£Œí•©ë‹ˆë‹¤.\n")

        while True:
            try:
                user_input = input("ğŸ‘¤ User >> ").strip()
                if user_input.lower() in ["exit", "quit", "ì¢…ë£Œ", "q"]:
                    print("\nğŸ‘‹ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!")
                    break
                if not user_input:
                    print("âŒ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.\n")
                    continue

                initial_state = {
                    "messages": [HumanMessage(content=user_input)],
                    "user_query": user_input,
                    "query_analysis": None,
                    "retrieved_docs": None,
                    "case_law_results": None,
                    "generated_answer": None,
                    "next_action": None,
                }

                print("\n" + "-" * 60)
                print("ğŸ”„ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘...")
                print("-" * 60 + "\n")

                result = graph.invoke(initial_state)
                answer = result.get("generated_answer", "")
                if answer:
                    print("\n" + "=" * 60)
                    print("ğŸ¤– AI ë‹µë³€:")
                    print("=" * 60)
                    print(f"\n{answer}\n")
                    print("=" * 60 + "\n")
                else:
                    print("\nâš ï¸ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n")

            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!")
                break
            except Exception as e:
                print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
                print("ğŸ’¡ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.\n")
                import traceback
                traceback.print_exc()

    except Exception as e:
        print(f"\nâŒ ì±—ë´‡ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        raise


if __name__ == "__main__":
    main()
