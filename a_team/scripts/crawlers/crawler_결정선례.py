"""
ì¤‘ì•™ë¶€ì²˜ 1ì°¨ í•´ì„ (ê²°ì •ì„ ë¡€) í¬ë¡¤ëŸ¬ (2ë‹¨ê³„ ì „ëµ - ìˆ˜ì •ì™„ë£Œ)
1ë‹¨ê³„: ì „ì²´ í˜ì´ì§€(ì•½ 9000ê±´/180í˜ì´ì§€ ì¶”ì •) ID ì „ìˆ˜ ìˆ˜ì§‘
2ë‹¨ê³„: ìˆ˜ì§‘ëœ ID ê¸°ë°˜ ìƒì„¸ì •ë³´ ì¶”ì¶œ
"""

import json
import os
import time
import re
import random
from datetime import datetime
from pathlib import Path
from playwright.sync_api import sync_playwright

# ============================================================
# ì„¤ì •
# ============================================================
SCRIPT_DIR = Path(__file__).parent
DATA_DIR = SCRIPT_DIR / '..' / 'data'
RAW_DIR = DATA_DIR / 'raw'
LIST_FILE = RAW_DIR / 'rd_ë²•ë ¹ì™¸_ê²°ì •ì„ ë¡€_list.json'
OUTPUT_FILE = RAW_DIR / 'rd_ë²•ë ¹ì™¸_ê²°ì •ì„ ë¡€.json'

# íƒ€ê²Ÿ URL (ì¤‘ì•™ë¶€ì²˜ 1ì°¨ í•´ì„, ì „ì²´ ë¶€ì²˜)
TARGET_URL = "https://www.law.go.kr/LSW/cgmExpcSc.do?menuId=11&subMenuId=729&tabMenuId=733&upperOfiClsCd=M&ofiClsCd=350101"

# ============================================================
# ìœ í‹¸ë¦¬í‹°
# ============================================================


def save_json(data, filepath: Path):
    filepath.parent.mkdir(parents=True, exist_ok=True)
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def load_json(filepath: Path):
    if not filepath.exists():
        return []
    with open(filepath, 'r', encoding='utf-8') as f:
        return json.load(f)

# ============================================================
# Phase 1: ì „ì²´ ëª©ë¡(ID) ìˆ˜ì§‘
# ============================================================


def collect_ids():
    print("ğŸš€ [1ë‹¨ê³„] ì „ì²´ ëª©ë¡ ID ìˆ˜ì§‘ ì‹œì‘...")

    # ì´ì–´í•˜ê¸° ì§€ì›
    collected_items = load_json(LIST_FILE)
    visited_ids = {item['item_id']
                   for item in collected_items if 'item_id' in item}

    # ë§ˆì§€ë§‰ìœ¼ë¡œ ìˆ˜ì§‘í•œ í˜ì´ì§€ ì°¾ê¸° (ì´ì–´í•˜ê¸°ìš©)
    start_page = 1
    if collected_items:
        max_collected_page = max((item.get('page', 1)
                                 for item in collected_items), default=1)
        start_page = max_collected_page
        print(f"  â†ª ê¸°ì¡´ ë°ì´í„°: {len(collected_items)}ê°œ, {start_page}í˜ì´ì§€ë¶€í„° íƒìƒ‰ ì¬ê°œ")
    else:
        print(f"  â†ª ì‹ ê·œ ìˆ˜ì§‘ ì‹œì‘")

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context()
        page = context.new_page()

        print(f"  ğŸ“„ í˜ì´ì§€ ë¡œë“œ: {TARGET_URL}")
        try:
            page.goto(TARGET_URL, timeout=60000)
            page.wait_for_selector('.tbl_wrap table', timeout=30000)
        except Exception as e:
            print(f"  âŒ ì´ˆê¸° ì ‘ì† ì‹¤íŒ¨: {e}")
            return

        current_page = 1  # fnSetPage ì—ëŸ¬ íšŒí”¼ë¥¼ ìœ„í•´ 1ë¶€í„° ì‹œì‘ (ì¤‘ë³µì€ ìŠ¤í‚µë¨)
        # start_page ë¡œì§ ì œê±°

        last_first_item_title = None
        consecutive_duplicates = 0

        while True:
            print(f"\nğŸ“‘ [Page {current_page}] ìŠ¤ìº” ì¤‘...", end=" ", flush=True)

            try:
                page.wait_for_selector(
                    '.tbl_wrap table tbody tr td.s_tit a', timeout=10000)
            except:
                print("âš ï¸ í…Œì´ë¸” ë¡œë”© íƒ€ì„ì•„ì›ƒ (ë°ì´í„° ì—†ìŒ/ëì¼ ìˆ˜ ìˆìŒ)")
                break

            rows = page.locator('.tbl_wrap table tbody tr td.s_tit a').all()
            if not rows:
                print("âš ï¸ ê²Œì‹œê¸€ ì—†ìŒ. ì¢…ë£Œ.")
                break

            new_in_page = 0

            # í˜„ì¬ í˜ì´ì§€ ì²«ë²ˆì§¸ ì•„ì´í…œ í™•ì¸
            current_first_title = rows[0].inner_text().strip()
            if last_first_item_title == current_first_title:
                consecutive_duplicates += 1
                if consecutive_duplicates >= 2:
                    print(f"ğŸ í˜ì´ì§€ ë³€í™” ì—†ìŒ ({consecutive_duplicates}íšŒ). ìˆ˜ì§‘ ì¢…ë£Œ.")
                    break
            else:
                consecutive_duplicates = 0
            last_first_item_title = current_first_title

            for link in rows:
                try:
                    title_full = link.inner_text().strip()
                    onclick = link.get_attribute('onclick')

                    item_id = ""
                    if onclick:
                        match = re.search(
                            r"(?:lsEmpViewWideAll|cgmExpcView)\('(\d+)'", onclick)
                        if match:
                            item_id = match.group(1)

                    if item_id and item_id not in visited_ids:
                        item = {
                            "item_id": item_id,
                            "title_full": title_full,
                            "onclick": onclick,
                            "page": current_page,
                            "collected_at": datetime.now().isoformat()
                        }
                        collected_items.append(item)
                        visited_ids.add(item_id)
                        new_in_page += 1
                except Exception:
                    pass

            print(
                f"ì™„ë£Œ ({new_in_page}ê±´ ì‹ ê·œ / ì „ì²´ {len(collected_items)}ê±´) | ì˜ˆ: {current_first_title[:15]}...")
            save_json(collected_items, LIST_FILE)

            # ë‹¤ìŒ í˜ì´ì§€ ì´ë™
            next_page = current_page + 1

            # í˜ì´ì§€ ì´ë™ ì‹¤í–‰ (Click ë°©ì‹)
            try:
                # í˜ì´ì§• ì˜ì—­ì˜ ëª¨ë“  ë§í¬ ê°€ì ¸ì˜¤ê¸°
                paging_links = page.locator(".paging a").all()
                navigated = False

                # print(f"  ğŸ” í˜ì´ì§€ íƒìƒ‰ ì¤‘... (ì°¾ëŠ” í˜ì´ì§€: {next_page})")

                # 1. ìˆ«ì ë²„íŠ¼ ì°¾ê¸°
                for link in paging_links:
                    text = link.inner_text().strip()
                    if text == str(next_page):
                        # print(f"  ğŸ‘‰ ìˆ«ì ë²„íŠ¼({next_page}) í´ë¦­")
                        link.evaluate("el => el.click()")
                        navigated = True
                        break

                # 2. ìˆ«ìë¥¼ ëª» ì°¾ì•˜ìœ¼ë©´ 'ë‹¤ìŒ' ë²„íŠ¼ ì°¾ê¸°
                if not navigated:
                    for link in paging_links:
                        # ì´ë¯¸ì§€ í™•ì¸
                        img = link.locator("img")
                        if img.count() > 0:
                            alt = img.get_attribute("alt")
                            if alt and "ë‹¤ìŒ" in alt:
                                print(f"  â© 'ë‹¤ìŒ' ì´ë¯¸ì§€ ë²„íŠ¼ í´ë¦­")
                                link.evaluate("el => el.click()")
                                navigated = True
                                break
                        # í´ë˜ìŠ¤ í™•ì¸
                        elif link.get_attribute("class") == "next":
                            print(f"  â© 'next' í´ë˜ìŠ¤ ë²„íŠ¼ í´ë¦­")
                            link.evaluate("el => el.click()")
                            navigated = True
                            break

                if navigated:
                    try:
                        # ë¡œë”© ëŒ€ê¸°
                        page.wait_for_load_state('networkidle', timeout=10000)
                        time.sleep(1.5)
                    except:
                        time.sleep(2.0)
                    current_page += 1
                else:
                    print(f"ğŸ ë‹¤ìŒ í˜ì´ì§€({next_page}) ì—°ê²° ê³ ë¦¬ ì—†ìŒ. ìˆ˜ì§‘ ì¢…ë£Œ.")
                    break

            except Exception as e:
                print(f"âš ï¸ í˜ì´ì§€ ì´ë™ ì¤‘ ì—ëŸ¬: {e}")
                break

            if current_page % 10 == 0:
                print(f"  ğŸ’¤ 10í˜ì´ì§€ ë‹¨ìœ„ íœ´ì‹...")
                time.sleep(1)

        browser.close()

    print(f"\nâœ… [1ë‹¨ê³„] ID ìˆ˜ì§‘ ì™„ë£Œ. ì´ {len(collected_items)}ê°œ ì €ì¥ë¨.")
    return collected_items

# ============================================================
# Phase 2: ìƒì„¸ ìˆ˜ì§‘
# ============================================================


def extract_content(page, header_id, stop_tag='H4'):
    return page.evaluate(f"""() => {{
        const header = document.querySelector('#{header_id}');
        if (!header) return "";
        const texts = [];
        let sib = header.nextElementSibling;
        while (sib && sib.tagName !== '{stop_tag}') {{
            if (sib.innerText && (sib.tagName === 'P' || sib.tagName === 'DIV')) {{
                texts.push(sib.innerText.trim());
            }}
            sib = sib.nextElementSibling;
        }}
        return texts.join('\\n');
    }}""")


def crawl_details():
    print("\nğŸš€ [2ë‹¨ê³„] ìƒì„¸ ë°ì´í„° í¬ë¡¤ë§ ì‹œì‘...")

    id_list = load_json(LIST_FILE)
    if not id_list:
        print("âŒ 1ë‹¨ê³„ ë°ì´í„°(IDëª©ë¡)ê°€ ì—†ìŠµë‹ˆë‹¤. collect_ids()ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    completed_data = load_json(OUTPUT_FILE)
    completed_ids = {item['item_id']
                     for item in completed_data if 'item_id' in item}

    total_count = len(id_list)
    targets = [item for item in id_list if item['item_id'] not in completed_ids]

    print(f"  ğŸ“ ì´ í•­ëª©: {total_count}ê°œ")
    print(f"  âœ… ì™„ë£Œë¨: {len(completed_ids)}ê°œ")
    print(f"  â–¶ï¸ ë‚¨ì€ì‘ì—…: {len(targets)}ê°œ")

    if not targets:
        print("ğŸ‰ ëª¨ë“  ë°ì´í„° ìˆ˜ì§‘ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return

    # Phase 2ëŠ” title ë™ê¸°í™”ê°€ í•µì‹¬ì´ë¯€ë¡œ, ì—„ê²©í•œ ì²´í¬ë¥¼ ìœ„í•´ title ì •ê·œí™” í•¨ìˆ˜ ì •ì˜
    def normalize_title(t):
        return re.sub(r'\s+', '', t).strip()

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context()
        page = context.new_page()

        print(f"  ğŸ“„ ë² ì´ìŠ¤ í˜ì´ì§€ ë¡œë“œ...")
        try:
            page.goto(TARGET_URL, timeout=60000)
            page.wait_for_selector('.tbl_wrap table', timeout=30000)
        except Exception as e:
            print(f"âŒ ë² ì´ìŠ¤ í˜ì´ì§€ ì ‘ì† ë¶ˆê°€: {e}")
            return

        for i, item in enumerate(targets):
            item_id = item['item_id']
            title = item['title_full']

            progress = f"[{i+1}/{len(targets)}]"
            print(
                f"{progress} ID:{item_id} | {title[:20]}...", end="", flush=True)

            retries = 2
            success = False

            while retries > 0:
                try:
                    clean_js = item['onclick'].replace(
                        'return false', '').strip()
                    if clean_js.endswith(';'):
                        clean_js = clean_js[:-1]

                    page.evaluate(clean_js)

                    # DOM ë³€ê²½ ëŒ€ê¸° (AJAX)
                    try:
                        # 1. ìŠ¤í”¼ë„ˆë‚˜ ì˜¤ë²„ë ˆì´ê°€ ì‚¬ë¼ì§€ê¸¸ ëŒ€ê¸° (ìˆë‹¤ë©´)
                        # 2. #contentBodyê°€ ë³´ì´ê¸¸ ëŒ€ê¸°
                        # 3. ì¤‘ìš”: í´ë¦­ ì „ì˜ í…ìŠ¤íŠ¸ì™€ ë‹¬ë¼ì¡ŒëŠ”ì§€ í™•ì¸ì€ ì–´ë ¤ìš°ë¯€ë¡œ,
                        #    evaluate ì§í›„ ì•½ê°„ì˜ sleepì„ ì£¼ê³ , rpl(ë‹µë³€) IDê°€ ë¡œë“œë˜ê¸°ë¥¼ ëŒ€ê¸°
                        time.sleep(0.5)
                        page.wait_for_selector(
                            '#contentBody', state='visible', timeout=5000)
                        page.wait_for_selector(
                            '#rpl', state='attached', timeout=5000)  # ë‹µë³€ ì˜ì—­ ì¡´ì¬ í™•ì¸
                    except:
                        pass

                    try:
                        page.wait_for_selector('#contentBody', timeout=5000)
                    except:
                        if retries == 1:
                            page.goto(
                                f"{TARGET_URL}#licCgmExpc{item_id}_350101")
                            page.wait_for_selector(
                                '#contentBody', timeout=5000)
                        else:
                            raise Exception("Content load timeout")

                    # --------------------------------------------------------
                    # [ì¤‘ìš”] Title ê²€ì¦ìœ¼ë¡œ í˜ì´ì§€ ê°±ì‹  ì—¬ë¶€ í™•ì¸
                    # --------------------------------------------------------
                    page_title = ""
                    try:
                        # í˜ì´ì§€ ë‚´ ì‹¤ì œ ì œëª© ìš”ì†Œ (h4 ë“±) êµ¬ì¡°ì— ë”°ë¼ ìˆ˜ì • í•„ìš”
                        # ê²°ì •ì„ ë¡€ í˜ì´ì§€ êµ¬ì¡°ìƒ #contentBody h3 ë˜ëŠ” h4 ë“±ì— ì œëª©ì´ ìˆì„ ìˆ˜ ìˆìŒ
                        # ì—¬ê¸°ì„œëŠ” .tit_view ë˜ëŠ” input[name="title"] ë“± í™•ì¸ í•„ìš”í•˜ì§€ë§Œ
                        # 2ë‹¨ê³„ ë¦¬ìŠ¤íŠ¸ì—ì„œ í´ë¦­ ì‹œ, ë³¸ë¬¸ ìƒë‹¨ íƒ€ì´í‹€ì´ ë°”ë€ŒëŠ”ì§€ í™•ì¸.

                        # law.go.kr êµ¬ì¡°ìƒ ë³¸ë¬¸ íƒ€ì´í‹€ IDê°€ ëª…í™•ì¹˜ ì•Šì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ
                        # inqGst(ì§ˆì˜) ë‚´ìš©ì´ ë¹„ì–´ìˆì§€ ì•Šì€ì§€ ìš°ì„  ì²´í¬í•˜ê³ ,
                        # ê°€ëŠ¥í•˜ë‹¤ë©´ item['title_full']ê³¼ ìœ ì‚¬í•œ í…ìŠ¤íŠ¸ê°€ ìˆëŠ”ì§€ í™•ì¸.
                        pass
                    except:
                        pass

                    # ì œëª©ì—ì„œ Agency, Date ì¶”ì¶œ (ì •ê·œì‹)
                    # ì˜ˆ: "ìœ¡ì•„íœ´ì§ ê¸‰ì—¬ ... [ê³ ìš©ë…¸ë™ë¶€, 2025.08.06.]"
                    real_title = title
                    agency = "Unknown"
                    date = "Unknown"

                    match = re.search(
                        r'^(.*?)\s*\[([^,]+),\s*([\d.]+)\]$', title)
                    if match:
                        real_title_only = match.group(1).strip()
                        agency = match.group(2).strip()
                        date = match.group(3).strip()
                    else:
                        real_title_only = title

                    # ë³¸ë¬¸ ì¶”ì¶œ
                    q_text = extract_content(page, 'inqGst')
                    a_text = extract_content(page, 'rpl')

                    # [ê²€ì¦] ì§ˆì˜ë‚˜ ë‹µë³€ ì¤‘ í•˜ë‚˜ëŠ” ë°˜ë“œì‹œ ìˆì–´ì•¼ í•¨.
                    # ë˜í•œ, ë§Œì•½ ì´ì „ í˜ì´ì§€ì˜ ë‚´ìš©ì´ ë‚¨ì•„ìˆëŠ”ì§€ í™•ì¸í•´ì•¼ í•¨.
                    # (ì—¬ê¸°ì„œëŠ” q_text, a_textê°€ ë¹„ì–´ìˆìœ¼ë©´ ë¡œë”© ì‹¤íŒ¨ë¡œ ê°„ì£¼)
                    if not q_text and not a_text:
                        raise Exception("Empty content (q_text & a_text)")

                    # [ê²€ì¦ 2] ë³¸ë¬¸ ë‚´ìš©ì´ ì´ì „ ì•„ì´í…œê³¼ ë™ì¼í•œì§€ ì²´í¬ (Stale Element)
                    # ê°„ë‹¨íˆ í…ìŠ¤íŠ¸ ê¸¸ì´ ë“±ìœ¼ë¡œ ë¹„êµí•˜ê±°ë‚˜ í•´ì‹œë¥¼ ì“¸ ìˆ˜ ì—†ìœ¼ë‹ˆ,
                    # ì—¬ê¸°ì„œëŠ” 'ë¡œë”© ëŒ€ê¸°'ë¥¼ ë¯¿ë˜, ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ì˜ì‹¬.
                    if len(q_text) < 5 and len(a_text) < 5:
                        raise Exception("Content too short (Example: null)")

                    # --------------------------------------------------------

                    laws = []
                    try:
                        laws = page.evaluate("""() => {
                            const lHeader = document.querySelector('#conLs');
                            if (!lHeader) return [];
                            const res = [];
                            let sib = lHeader.nextElementSibling;
                            while (sib) {
                                if (sib.tagName === 'P') {
                                    sib.querySelectorAll('a').forEach(l => {
                                        res.push({text: l.innerText.trim(), onclick: l.getAttribute('onclick')});
                                    });
                                }
                                if (sib.tagName === 'H4') break;
                                sib = sib.nextElementSibling;
                            }
                            return res;
                        }""")
                    except:
                        pass

                    data_obj = {
                        "item_id": item_id,
                        "title": real_title,
                        "agency": agency,
                        "date": date,
                        "question": q_text,
                        "answer": a_text,
                        "related_laws": laws,
                        "url": page.url,
                        "crawled_at": datetime.now().isoformat()
                    }

                    completed_data.append(data_obj)
                    save_json(completed_data, OUTPUT_FILE)

                    print(" -> âœ… ì„±ê³µ")
                    success = True
                    break

                except Exception as e:
                    retries -= 1
                    print(f" -> âš ï¸ ì‹¤íŒ¨(ì¬ì‹œë„{retries}): {e}", end="")
                    try:
                        page.goto(TARGET_URL)
                        page.wait_for_selector(
                            '.tbl_wrap table', timeout=10000)
                    except:
                        pass

            if not success:
                print(" -> âŒ ìµœì¢… ì‹¤íŒ¨ (ê±´ë„ˆëœ€)")

            try:
                if page.locator('#btnList').is_visible():
                    page.click('#btnList')
                    page.wait_for_selector('.tbl_wrap table', timeout=5000)
                else:
                    page.go_back()
            except:
                pass

            time.sleep(0.1)

        browser.close()


if __name__ == "__main__":
    # Phase 1: ID ìˆ˜ì§‘ ìš°ì„  (ì‚¬ìš©ì ìš”ì²­: 9000ê°œ ì „ìˆ˜ ìˆ˜ì§‘)
    # collect_ids()

    # Phase 2: ìƒì„¸ ìˆ˜ì§‘ (Phase 1 ì™„ë£Œ í›„ í•„ìš” ì‹œ ì£¼ì„ í•´ì œ)
    crawl_details()
