################################################
# A-TEAM ë²•ë¥  RAG ì±—ë´‡ (LangGraph V7)
# V7 ì‹ ê·œ ê¸°ëŠ¥:
# - ì§ˆë¬¸ ì˜ë„ ë¶„ì„ (6ê°œ ìœ í˜•: ë²•ë ¹ì¡°íšŒ, ì ˆì°¨ë¬¸ì˜, ìƒí™©íŒë‹¨, ê¶Œë¦¬í™•ì¸, ë¶„ìŸí•´ê²°, ì¼ë°˜ìƒë‹´)
# - ê²€ìƒ‰ ì „ëµ ê²°ì • (ë²•ë ¹ìš°ì„ , í–‰ì •í•´ì„ìš°ì„ , íŒë¡€í•„ìˆ˜, ì¢…í•©ê²€ìƒ‰)
# ê¸°ì¡´ ê¸°ëŠ¥: Hybrid Retriever, Query Expansion, Generator-Critic Light
# ì‘ì„±ì: SKN 3-1íŒ€ A-TEAM
# ì‘ì„±ì¼: 2026-01-08
################################################

from langgraph.graph.message import add_messages
from langgraph.graph import StateGraph, END
from typing import Any
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
from pydantic import BaseModel, Field
from langchain_community.retrievers import BM25Retriever
from langchain_core.documents import Document, BaseDocumentCompressor
from langchain_openai import ChatOpenAI
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_qdrant import QdrantVectorStore
from qdrant_client import QdrantClient
from dotenv import load_dotenv
from typing import Annotated, TypedDict, Sequence, Optional, List, Literal
from pathlib import Path
import warnings
import os

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ: ì‹¤í–‰ ìœ„ì¹˜(CWD)ì™€ ë¬´ê´€í•˜ê²Œ ì´ íŒŒì¼ê³¼ ê°™ì€ í´ë”ì˜ .envë¥¼ ì‚¬ìš©
_DOTENV_PATH = Path(__file__).with_name(".env")
load_dotenv(dotenv_path=_DOTENV_PATH)


# ===========================
# State ì •ì˜
# ===========================
class AgentState(TypedDict):
    """LangGraph Agentì˜ ìƒíƒœë¥¼ ì •ì˜í•˜ëŠ” TypedDict"""
    # ëŒ€í™” íˆìŠ¤í† ë¦¬
    messages: Annotated[Sequence[BaseMessage], add_messages]
    # ì‚¬ìš©ì ì§ˆë¬¸
    user_query: str
    # ì§ˆë¬¸ ë¶„ì„ ê²°ê³¼
    # {category, needs_clarification, needs_case_law, clarification_question}
    query_analysis: Optional[dict]
    # ê²€ìƒ‰ ê²°ê³¼ (Document ë¦¬ìŠ¤íŠ¸)
    retrieved_docs: Optional[List[Document]]
    # ìƒì„±ëœ ë‹µë³€
    generated_answer: Optional[str]
    # í˜„ì¬ ë¼ìš°íŒ… ê²°ì •
    next_action: Optional[str]
    # [V5] ë‹µë³€ í‰ê°€ ê²°ê³¼ (Generator-Critic)
    evaluation_result: Optional[dict]
    # [V5] ì¬ê²€ìƒ‰ ì‹œë„ íšŸìˆ˜ (ë¬´í•œ ë£¨í”„ ë°©ì§€)
    retry_count: Optional[int]


# ===========================
# Reranker ì •ì˜
# ===========================
class JinaReranker(BaseDocumentCompressor):
    model_name: str = "jinaai/jina-reranker-v2-base-multilingual"
    top_n: int = 7
    model: Any = None
    tokenizer: Any = None

    class Config:
        arbitrary_types_allowed = True
        extra = "allow"

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name, trust_remote_code=True)
        self.model = AutoModelForSequenceClassification.from_pretrained(
            self.model_name, trust_remote_code=True, dtype="auto"
        )
        self.model.eval()

    def compress_documents(
        self, documents: Sequence[Document], query: str, callbacks: Optional[Any] = None
    ) -> Sequence[Document]:
        if not documents:
            return []

        pairs = [[query, doc.page_content] for doc in documents]

        with torch.no_grad():
            inputs = self.tokenizer(
                pairs,
                padding=True,
                truncation=True,
                return_tensors="pt",
                max_length=512
            )
            # Sigmoid ì ìš©í•˜ì—¬ 0~1 ì‚¬ì´ í™•ë¥ ë¡œ ë³€í™˜
            scores = self.model(**inputs).logits.squeeze(-1).float().cpu()
            scores = torch.sigmoid(scores).tolist()
            if not isinstance(scores, list):
                scores = [scores]

        # Sort and select top_n
        top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[
            :self.top_n]

        final_docs = []
        for i in top_indices:
            doc = documents[i]
            doc.metadata["relevance_score"] = scores[i]
            final_docs.append(doc)

        return final_docs


# ===========================
# ë…¸ë“œ í•¨ìˆ˜ ì •ì˜ (LangGraph ì˜ì—­)
# ===========================

# Pydantic ëª¨ë¸: Query Expansion ê²°ê³¼
class ExpandedQuery(BaseModel):
    """ê²€ìƒ‰ ì¿¼ë¦¬ í™•ì¥ ê²°ê³¼"""
    original_query: str = Field(description="ì›ë³¸ ì‚¬ìš©ì ì§ˆë¬¸")
    search_keywords: List[str] = Field(description="í•µì‹¬ ê²€ìƒ‰ í‚¤ì›Œë“œ (3-5ê°œ)")
    legal_terms: List[str] = Field(
        description="ê´€ë ¨ ë²•ë¥  ìš©ì–´ ë° ì¡°í•­ëª… (ì˜ˆ: ê·¼ë¡œê¸°ì¤€ë²• ì œ23ì¡°)")
    synonyms: List[str] = Field(description="ë™ì˜ì–´ ë° ìœ ì‚¬ í‘œí˜„ (2-3ê°œ)")
    expanded_query: str = Field(description="í™•ì¥ëœ ê²€ìƒ‰ ì¿¼ë¦¬ (ì›ë³¸ + í‚¤ì›Œë“œ ì¡°í•©)")


def create_query_expander(llm: ChatOpenAI):
    """Query Expansion í•¨ìˆ˜ ìƒì„± - ë²•ë¥  ë„ë©”ì¸ íŠ¹í™”"""

    structured_llm = llm.with_structured_output(ExpandedQuery)

    expansion_prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ í•œêµ­ ë²•ë¥  ê²€ìƒ‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì‚¬ìš©ìì˜ ë²•ë¥  ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ë²¡í„° ê²€ìƒ‰ê³¼ í‚¤ì›Œë“œ ê²€ìƒ‰ì— ìµœì í™”ëœ ì¿¼ë¦¬ë¡œ í™•ì¥í•©ë‹ˆë‹¤.

## ëª©í‘œ
ë²•ë¥  ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ìµœëŒ€í•œ ë§ì´ ê²€ìƒ‰í•  ìˆ˜ ìˆë„ë¡ ì¿¼ë¦¬ë¥¼ í™•ì¥í•©ë‹ˆë‹¤.

## í™•ì¥ ì „ëµ
1. **í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ**: ì§ˆë¬¸ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ë²•ë¥  ê°œë… 3-5ê°œ ì¶”ì¶œ
2. **ë²•ë¥  ìš©ì–´ ë§¤í•‘**: ì¼ìƒ í‘œí˜„ì„ ë²•ë¥  ìš©ì–´ë¡œ ë³€í™˜ (ì˜ˆ: "ì›”ê¸‰" â†’ "ì„ê¸ˆ", "ì˜ë¦¼" â†’ "í•´ê³ ")
3. **ê´€ë ¨ ì¡°í•­ ì¶”ë¡ **: í•´ë‹¹ ë¶„ì•¼ì˜ ëŒ€í‘œ ë²•ë ¹ëª…ê³¼ ì¡°í•­ ì¶”ì • (ì˜ˆ: "ì£¼íœ´ìˆ˜ë‹¹" â†’ "ê·¼ë¡œê¸°ì¤€ë²• ì œ55ì¡°")
4. **ë™ì˜ì–´ í™•ì¥**: ê²€ìƒ‰ ë²”ìœ„ë¥¼ ë„“íˆê¸° ìœ„í•œ ìœ ì‚¬ í‘œí˜„ ì¶”ê°€

## ë²•ë¥  ë¶„ì•¼ë³„ ì£¼ìš” í‚¤ì›Œë“œ
- ë…¸ë™ë²•: ê·¼ë¡œê¸°ì¤€ë²•, ì„ê¸ˆ, í‡´ì§ê¸ˆ, í•´ê³ , ì‚°ì¬, ì£¼íœ´ìˆ˜ë‹¹, ì—°ì°¨, ê·¼ë¡œê³„ì•½
- í˜•ì‚¬ë²•: í˜•ë²•, í˜•ì‚¬ì†Œì†¡ë²•, ê³ ì†Œ, ê³ ë°œ, ê¸°ì†Œ, êµ¬ì†, ê³µì†Œì‹œíš¨
- ë¯¼ì‚¬ë²•: ë¯¼ë²•, ê³„ì•½, ì†í•´ë°°ìƒ, ì†Œìœ ê¶Œ, ì±„ê¶Œ, ë¬¼ê¶Œ, ë¶ˆë²•í–‰ìœ„

## ì¶œë ¥ ê·œì¹™
- expanded_queryëŠ” ì›ë³¸ ì§ˆë¬¸ + í•µì‹¬ í‚¤ì›Œë“œ + ê´€ë ¨ ë²•ë ¹ëª…ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì¡°í•©
- ê²€ìƒ‰ì— ë¶ˆí•„ìš”í•œ ì¡°ì‚¬, ì–´ë¯¸ëŠ” ì œê±°
- ìµœëŒ€ 100ì ì´ë‚´ë¡œ ì••ì¶•"""),
        ("human", "{query}")
    ])

    def expand_query(query: str) -> ExpandedQuery:
        """ì§ˆë¬¸ì„ ê²€ìƒ‰ì— ìµœì í™”ëœ í˜•íƒœë¡œ í™•ì¥"""
        try:
            chain = expansion_prompt | structured_llm
            result: ExpandedQuery = chain.invoke({"query": query})
            return result
        except Exception as e:
            # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì¿¼ë¦¬ ê·¸ëŒ€ë¡œ ë°˜í™˜
            return ExpandedQuery(
                original_query=query,
                search_keywords=[],
                legal_terms=[],
                synonyms=[],
                expanded_query=query
            )

    return expand_query


# Pydantic ëª¨ë¸: ì§ˆë¬¸ ë¶„ì„ ê²°ê³¼ (V7 - ì˜ë„ ë¶„ì„ + ê²€ìƒ‰ ì „ëµ)
class QueryAnalysis(BaseModel):
    """LLMì´ ë°˜í™˜í•  ì§ˆë¬¸ ë¶„ì„ ê²°ê³¼ - V7 í™•ì¥"""
    # ê¸°ë³¸ ë¶„ë¥˜
    category: str = Field(description="ë²•ë¥  ë¶„ì•¼: ë…¸ë™ë²•, í˜•ì‚¬ë²•, ë¯¼ì‚¬ë²•, ê¸°íƒ€ ì¤‘ í•˜ë‚˜")
    needs_clarification: bool = Field(
        default=False, description="ì§ˆë¬¸ì´ ê·¹ë„ë¡œ ëª¨í˜¸í•˜ì—¬ ë‹µë³€ ë¶ˆê°€ëŠ¥í•œì§€")
    needs_case_law: bool = Field(default=False, description="ëŒ€ë²•ì› íŒë¡€ ê²€ìƒ‰ì´ í•„ìš”í•œì§€")
    clarification_question: str = Field(
        default="", description="ëª…í™•í™” í•„ìš” ì‹œ ì‚¬ìš©ìì—ê²Œ ë¬¼ì–´ë³¼ ì§ˆë¬¸")

    # [V7] ì§ˆë¬¸ ì˜ë„ ë¶„ì„
    intent_type: str = Field(
        description="ì§ˆë¬¸ ì˜ë„: ë²•ë ¹ì¡°íšŒ, ì ˆì°¨ë¬¸ì˜, ìƒí™©íŒë‹¨, ê¶Œë¦¬í™•ì¸, ë¶„ìŸí•´ê²°, ì¼ë°˜ìƒë‹´ ì¤‘ í•˜ë‚˜")
    user_situation: str = Field(
        default="", description="ì‚¬ìš©ìê°€ ì²˜í•œ ìƒí™© 1-2ë¬¸ì¥ ìš”ì•½")
    core_question: str = Field(
        default="", description="ì§ˆë¬¸ì˜ í•µì‹¬ (í•œ ë¬¸ì¥ìœ¼ë¡œ ì¶”ì¶œ)")

    # [V7] ê²€ìƒ‰ ì „ëµ
    search_strategy: str = Field(
        description="ê²€ìƒ‰ ì „ëµ: ë²•ë ¹ìš°ì„ , í–‰ì •í•´ì„ìš°ì„ , íŒë¡€í•„ìˆ˜, ì¢…í•©ê²€ìƒ‰ ì¤‘ í•˜ë‚˜")
    target_doc_types: List[str] = Field(
        default_factory=list,
        description="ê²€ìƒ‰í•  ë¬¸ì„œ íƒ€ì… ë¦¬ìŠ¤íŠ¸: ë²•, ì‹œí–‰ë ¹, ì‹œí–‰ê·œì¹™, í–‰ì •í•´ì„, íŒì •ì„ ë¡€ ì¤‘ ì„ íƒ")
    related_laws: List[str] = Field(
        default_factory=list,
        description="ì˜ˆìƒ ê´€ë ¨ ë²•ë¥ ëª… (ì˜ˆ: ê·¼ë¡œê¸°ì¤€ë²•, ì‚°ì—…ì¬í•´ë³´ìƒë³´í—˜ë²•)")


def create_analyze_query_node(llm: ChatOpenAI):
    """ë…¸ë“œ 1: ì§ˆë¬¸ ë¶„ì„ (V7 - ì˜ë„ ë¶„ì„ + ê²€ìƒ‰ ì „ëµ)"""

    # Structured Outputì„ ìœ„í•œ LLM
    structured_llm = llm.with_structured_output(QueryAnalysis)

    analyze_prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ ë²•ë¥  ì§ˆë¬¸ì„ ì‹¬ì¸µ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì˜ë„ë¥¼ íŒŒì•…í•˜ê³  ìµœì ì˜ ê²€ìƒ‰ ì „ëµì„ ê²°ì •í•©ë‹ˆë‹¤.

## 1. ê¸°ë³¸ ë¶„ë¥˜
**category** (ë²•ë¥  ë¶„ì•¼):
- "ë…¸ë™ë²•": ê·¼ë¡œê¸°ì¤€ë²•, ì„ê¸ˆ, í‡´ì§ê¸ˆ, í•´ê³ , ì‚°ì¬, ì£¼íœ´ìˆ˜ë‹¹, ì—°ì°¨íœ´ê°€, ê·¼ë¡œê³„ì•½ ë“±
- "í˜•ì‚¬ë²•": ë²”ì£„, í˜•ë²Œ, ìˆ˜ì‚¬, ì¬íŒ, ê³ ì†Œ/ê³ ë°œ, í˜•ì‚¬ì†Œì†¡ ë“±
- "ë¯¼ì‚¬ë²•": ê³„ì•½, ì†í•´ë°°ìƒ, ì†Œìœ ê¶Œ, ì±„ê¶Œ, ë¶ˆë²•í–‰ìœ„, ë¯¼ì‚¬ì†Œì†¡ ë“±
- "ê¸°íƒ€": ìœ„ ì¹´í…Œê³ ë¦¬ì— ì†í•˜ì§€ ì•ŠëŠ” ë²•ë¥  ì§ˆë¬¸

**needs_clarification**: 
- true: 1~2ë‹¨ì–´ë§Œ ìˆì–´ ì–´ë–¤ ë‹µë³€ë„ ë¶ˆê°€ëŠ¥í•œ ê²½ìš° ("ë²•ë¥ ", "ê³„ì•½", "ë„ì™€ì¤˜")
- false: ìƒí™©ì´ ì¡°ê¸ˆì´ë¼ë„ ì„¤ëª…ë˜ì–´ ìˆìœ¼ë©´ ë‹µë³€ ê°€ëŠ¥

**needs_case_law**: 
- true: "íŒë¡€", "íŒê²°", "ëŒ€ë²•ì›" ì–¸ê¸‰ ë˜ëŠ” ë²•ì  í•´ì„ì´ í•„ìš”í•œ ìŸì 
- false: ë‹¨ìˆœ ë²•ë ¹ ì¡°íšŒ, ì ˆì°¨ ë¬¸ì˜

## 2. ì§ˆë¬¸ ì˜ë„ ë¶„ì„ (intent_type)
ì‚¬ìš©ìê°€ ë¬´ì—‡ì„ ì›í•˜ëŠ”ì§€ íŒŒì•…:
- **"ë²•ë ¹ì¡°íšŒ"**: íŠ¹ì • ë²•ë ¹, ì¡°í•­, ê·œì •ì˜ ë‚´ìš©ì„ ì•Œê³  ì‹¶ìŒ (ì˜ˆ: "ê·¼ë¡œê¸°ì¤€ë²• ì œ23ì¡°ê°€ ë­ì•¼?")
- **"ì ˆì°¨ë¬¸ì˜"**: ì‹ ì²­, ì ‘ìˆ˜, ì²˜ë¦¬ ì ˆì°¨ë¥¼ ì•Œê³  ì‹¶ìŒ (ì˜ˆ: "ì‚°ì¬ ì‹ ì²­ ì–´ë–»ê²Œ í•´?")
- **"ìƒí™©íŒë‹¨"**: ìì‹ ì˜ ìƒí™©ì´ ë²•ì ìœ¼ë¡œ ì–´ë–¤ ìƒíƒœì¸ì§€ íŒë‹¨ ìš”ì²­ (ì˜ˆ: "ì´ê²Œ ë¶€ë‹¹í•´ê³ ì•¼?")
- **"ê¶Œë¦¬í™•ì¸"**: ìì‹ ì—ê²Œ ì–´ë–¤ ê¶Œë¦¬ê°€ ìˆëŠ”ì§€ í™•ì¸ (ì˜ˆ: "í‡´ì§ê¸ˆ ë°›ì„ ìˆ˜ ìˆì–´?")
- **"ë¶„ìŸí•´ê²°"**: ê°ˆë“±/ë¶„ìŸ ìƒí™©ì—ì„œ í•´ê²° ë°©ë²• ë¬¸ì˜ (ì˜ˆ: "ì‚¬ì¥ì´ ì„ê¸ˆ ì•ˆ ì¤˜ ì–´ë–»ê²Œ í•´?")
- **"ì¼ë°˜ìƒë‹´"**: ìœ„ì— í•´ë‹¹í•˜ì§€ ì•ŠëŠ” ì¼ë°˜ì  ë²•ë¥  ì§ˆë¬¸

## 3. ìƒí™© ë¶„ì„
**user_situation**: ì‚¬ìš©ìê°€ ì²˜í•œ ìƒí™©ì„ 1-2ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½
**core_question**: ì§ˆë¬¸ì˜ í•µì‹¬ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ì¶”ì¶œ

## 4. ê²€ìƒ‰ ì „ëµ ê²°ì • (search_strategy)
ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¼ ìµœì ì˜ ê²€ìƒ‰ ì „ëµ ì„ íƒ:
- **"ë²•ë ¹ìš°ì„ "**: ë²•ë ¹ì¡°íšŒ, ê¶Œë¦¬í™•ì¸ â†’ ë²• ì¡°ë¬¸ì´ ê°€ì¥ ì¤‘ìš”
- **"í–‰ì •í•´ì„ìš°ì„ "**: ì ˆì°¨ë¬¸ì˜ â†’ í–‰ì •í•´ì„/ì‹œí–‰ê·œì¹™ì´ ì‹¤ë¬´ì 
- **"íŒë¡€í•„ìˆ˜"**: ìƒí™©íŒë‹¨ì—ì„œ ìŸì ì´ ìˆê±°ë‚˜ needs_case_lawê°€ true
- **"ì¢…í•©ê²€ìƒ‰"**: ë¶„ìŸí•´ê²°, ë³µí•©ì  ì§ˆë¬¸ â†’ ë‹¤ì–‘í•œ ë¬¸ì„œ í•„ìš”

## 5. ë¬¸ì„œ íƒ€ì… ì¶”ì²œ (target_doc_types)
ê²€ìƒ‰í•  ë¬¸ì„œ íƒ€ì… ì„ íƒ (ë³µìˆ˜ ì„ íƒ ê°€ëŠ¥):
- "ë²•": ê¸°ë³¸ ë²•ë¥  (ê·¼ë¡œê¸°ì¤€ë²•, í˜•ë²• ë“±)
- "ì‹œí–‰ë ¹": ëŒ€í†µë ¹ë ¹ (ë²•ì˜ ì„¸ë¶€ ì‹œí–‰ì‚¬í•­)
- "ì‹œí–‰ê·œì¹™": ë¶€ë ¹ (ì ˆì°¨, ì„œì‹, ê¸°ì¤€)
- "í–‰ì •í•´ì„": ê³ ìš©ë…¸ë™ë¶€ ë“± í–‰ì •ê¸°ê´€ í•´ì„
- "íŒì •ì„ ë¡€": ë…¸ë™ìœ„ì›íšŒ ë“± íŒì • ì‚¬ë¡€

## 6. ê´€ë ¨ ë²•ë¥  ì¶”ë¡  (related_laws)
ì§ˆë¬¸ì—ì„œ ì˜ˆìƒë˜ëŠ” ê´€ë ¨ ë²•ë¥ ëª… ë‚˜ì—´ (ì˜ˆ: ["ê·¼ë¡œê¸°ì¤€ë²•", "ì‚°ì—…ì¬í•´ë³´ìƒë³´í—˜ë²•"])"""),
        ("human", "{query}")
    ])

    def analyze_query(state: AgentState) -> AgentState:
        """ì§ˆë¬¸ ë¶„ì„ ë…¸ë“œ: ì˜ë„ íŒŒì•… + ê²€ìƒ‰ ì „ëµ ê²°ì •"""
        query = state["user_query"]

        print(f"ğŸ” [ì§ˆë¬¸ ì‹¬ì¸µ ë¶„ì„ ì¤‘...]")

        chain = analyze_prompt | structured_llm
        analysis: QueryAnalysis = chain.invoke({"query": query})

        # ë¶„ì„ ê²°ê³¼ ìƒì„¸ ì¶œë ¥
        print(f"")
        print(f"ğŸ“‹ [ë¶„ì„ ê²°ê³¼]")
        print(f"   ğŸ“‚ ë¶„ì•¼: {analysis.category}")
        print(f"   ğŸ¯ ì˜ë„: {analysis.intent_type}")
        print(
            f"   ğŸ’­ ìƒí™©: {analysis.user_situation[:50]}..." if analysis.user_situation else "   ğŸ’­ ìƒí™©: (ì—†ìŒ)")
        print(f"   â“ í•µì‹¬ ì§ˆë¬¸: {analysis.core_question}")
        print(f"   ğŸ” ê²€ìƒ‰ ì „ëµ: {analysis.search_strategy}")
        print(f"   ğŸ“‘ ëŒ€ìƒ ë¬¸ì„œ: {', '.join(analysis.target_doc_types)}")
        print(f"   ğŸ“š ê´€ë ¨ ë²•ë¥ : {', '.join(analysis.related_laws)}")
        print(f"   ëª…í™•í™” í•„ìš”: {'ì˜ˆ' if analysis.needs_clarification else 'ì•„ë‹ˆì˜¤'}")
        print(f"   íŒë¡€ í•„ìš”: {'ì˜ˆ' if analysis.needs_case_law else 'ì•„ë‹ˆì˜¤'}")
        print(f"")

        return {
            "query_analysis": analysis.model_dump()
        }

    return analyze_query


def create_clarify_node(llm: ChatOpenAI):
    """ë…¸ë“œ 2: ì‚¬ìš©ìì—ê²Œ ëª…í™•í™” ìš”ì²­"""

    def request_clarification(state: AgentState) -> AgentState:
        """ëª…í™•í™” ìš”ì²­ ë…¸ë“œ: ëª¨í˜¸í•œ ì§ˆë¬¸ì— ëŒ€í•´ êµ¬ì²´ì ì¸ ì •ë³´ ìš”ì²­"""
        analysis = state.get("query_analysis", {})
        clarification_q = analysis.get("clarification_question", "")

        if not clarification_q:
            # ê¸°ë³¸ ëª…í™•í™” ì§ˆë¬¸
            clarification_q = "ì§ˆë¬¸ì„ ì¢€ ë” êµ¬ì²´ì ìœ¼ë¡œ í•´ì£¼ì‹œê² ì–´ìš”? ì–´ë–¤ ìƒí™©ì¸ì§€, ë¬´ì—‡ì´ ê¶ê¸ˆí•˜ì‹ ì§€ ìì„¸íˆ ì•Œë ¤ì£¼ì‹œë©´ ë” ì •í™•í•œ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤."

        print(f"â“ [ëª…í™•í™” ìš”ì²­]")

        # ì¹œì ˆí•œ í˜•ì‹ìœ¼ë¡œ ë‹µë³€ êµ¬ì„±
        answer = f"""ì•ˆë…•í•˜ì„¸ìš”! ì§ˆë¬¸ì„ ì˜ ì´í•´í•˜ê¸° ìœ„í•´ ëª‡ ê°€ì§€ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.

{clarification_q}

ìœ„ ë‚´ìš©ì„ í¬í•¨í•´ì„œ ë‹¤ì‹œ ì§ˆë¬¸í•´ ì£¼ì‹œë©´, ë” ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ğŸ˜Š"""

        return {
            "generated_answer": answer,
            "next_action": "end"
        }

    return request_clarification


def create_search_node(vectorstore: QdrantVectorStore,
                       bm25_retriever: Optional[BM25Retriever] = None,
                       query_expander=None):
    """ë…¸ë“œ 3: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (V7 - ì˜ë„ ê¸°ë°˜ í•„í„°ë§ + ë²•ë¥  ë¶€ìŠ¤íŒ…)"""

    # Rerankerë¥¼ í•œ ë²ˆë§Œ ìƒì„± (ì„±ëŠ¥ ìµœì í™”)
    _reranker = JinaReranker(top_n=7)  # V7: top_nì„ 7ë¡œ ì¦ê°€ (í•„í„°ë§ í›„ 5ê°œ ìœ ì§€)

    def get_doc_type(law_name: str) -> str:
        """ë²•ë ¹ëª…ì—ì„œ ë¬¸ì„œ íƒ€ì… ì¶”ë¡ """
        if 'ì‹œí–‰ê·œì¹™' in law_name:
            return 'ì‹œí–‰ê·œì¹™'
        elif 'ì‹œí–‰ë ¹' in law_name:
            return 'ì‹œí–‰ë ¹'
        elif law_name:  # ê¸°ë³¸ ë²•ë¥ 
            return 'ë²•'
        return 'ê¸°íƒ€'

    def search_documents(state: AgentState) -> AgentState:
        """ê²€ìƒ‰ ì‹¤í–‰ ë…¸ë“œ: V7 - ì˜ë„ ê¸°ë°˜ í•„í„°ë§ + ë²•ë¥  ë¶€ìŠ¤íŒ…"""
        original_query = state["user_query"]

        # [V7] ë¶„ì„ ê²°ê³¼ì—ì„œ ê²€ìƒ‰ ì „ëµ ì¶”ì¶œ
        analysis = state.get("query_analysis", {})
        search_strategy = analysis.get("search_strategy", "ì¢…í•©ê²€ìƒ‰")
        target_doc_types = analysis.get("target_doc_types", [])
        related_laws = analysis.get("related_laws", [])

        print(f"ğŸ¯ [V7 ê²€ìƒ‰ ì „ëµ] {search_strategy}")
        if target_doc_types:
            print(f"   ğŸ“‘ ëŒ€ìƒ ë¬¸ì„œ: {', '.join(target_doc_types)}")
        if related_laws:
            print(f"   ğŸ“š ê´€ë ¨ ë²•ë¥ : {', '.join(related_laws)}")

        # Query Expansion ì ìš©
        if query_expander is not None:
            print(f"ğŸ” [Query Expansion] ì¿¼ë¦¬ í™•ì¥ ì¤‘...")
            try:
                expanded = query_expander(original_query)
                search_query = expanded.expanded_query
                print(f"   ğŸ“ ì›ë³¸: {original_query[:40]}...")
                print(f"   ğŸ”„ í™•ì¥: {search_query[:60]}...")
                if expanded.legal_terms:
                    print(f"   ğŸ“‹ ë²•ë¥  ìš©ì–´: {', '.join(expanded.legal_terms[:3])}")
            except Exception as e:
                print(f"   âš ï¸  Query Expansion ì‹¤íŒ¨: {e}")
                search_query = original_query
        else:
            search_query = original_query

        print(f"ğŸ” [í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰] ì¿¼ë¦¬: {search_query[:50]}...")

        all_docs = []

        # 1. Vector Search (ìœ ì‚¬ë„ ê¸°ë°˜) - í™•ì¥ëœ ì¿¼ë¦¬ ì‚¬ìš©
        print(f"   ğŸ“Š [Vector Search] ì‹¤í–‰ ì¤‘...")
        try:
            vector_results = vectorstore.similarity_search_with_score(
                search_query, k=15)
            vector_docs = [doc for doc, score in vector_results]
            print(f"   âœ… Vector Search: {len(vector_docs)}ê°œ ë¬¸ì„œ ê²€ìƒ‰")

            # Vector ê²€ìƒ‰ ê²°ê³¼ì— source í‘œì‹œ
            for doc in vector_docs:
                doc.metadata["search_source"] = "vector"
            all_docs.extend(vector_docs)
        except Exception as e:
            print(f"   âš ï¸  Vector Search ì˜¤ë¥˜: {e}")

        # 2. BM25 Search (í‚¤ì›Œë“œ ê¸°ë°˜) - í™•ì¥ëœ ì¿¼ë¦¬ ì‚¬ìš©
        if bm25_retriever is not None:
            print(f"   ğŸ“ [BM25 Search] ì‹¤í–‰ ì¤‘...")
            try:
                bm25_docs = bm25_retriever.invoke(search_query)
                print(f"   âœ… BM25 Search: {len(bm25_docs)}ê°œ ë¬¸ì„œ ê²€ìƒ‰")

                # BM25 ê²€ìƒ‰ ê²°ê³¼ì— source í‘œì‹œ
                for doc in bm25_docs:
                    doc.metadata["search_source"] = "bm25"
                all_docs.extend(bm25_docs)
            except Exception as e:
                print(f"   âš ï¸  BM25 Search ì˜¤ë¥˜: {e}")
        else:
            print(f"   âš ï¸  BM25 Retriever ë¯¸ì„¤ì • (Vector Searchë§Œ ì‚¬ìš©)")

        # 3. ì¤‘ë³µ ì œê±° (page_content ê¸°ì¤€)
        seen_contents = set()
        unique_docs = []
        for doc in all_docs:
            content_hash = hash(doc.page_content[:200])  # ì• 200ìë¡œ ì¤‘ë³µ ì²´í¬
            if content_hash not in seen_contents:
                seen_contents.add(content_hash)
                unique_docs.append(doc)

        print(f"   ğŸ”„ ì¤‘ë³µ ì œê±° í›„: {len(unique_docs)}ê°œ ë¬¸ì„œ")

        # ìœ ì‚¬ë„ ì„ê³„ê°’ ì„¤ì • (ì´ ì ìˆ˜ ë¯¸ë§Œì€ ê´€ë ¨ ì—†ëŠ” ë¬¸ì„œë¡œ íŒë‹¨)
        RELEVANCE_THRESHOLD = 0.2

        if unique_docs:
            # 4. ë¦¬ë­í‚¹ (Jina Reranker) - ì›ë³¸ ì¿¼ë¦¬ë¡œ ë¦¬ë­í‚¹ (ì˜ë¯¸ ë³´ì¡´)
            print(f"ğŸ”„ [ë¦¬ë­í‚¹] Jina Rerankerë¡œ ìƒìœ„ ë¬¸ì„œ ì„ ë³„ ì¤‘...")
            try:
                reranked_docs = _reranker.compress_documents(
                    unique_docs, original_query)

                if reranked_docs:
                    # [V7] 5. ê´€ë ¨ ë²•ë¥  ë¶€ìŠ¤íŒ… - related_lawsì— í•´ë‹¹í•˜ëŠ” ë¬¸ì„œ ì ìˆ˜ ìƒí–¥
                    if related_laws:
                        print(f"   ğŸš€ [ë²•ë¥  ë¶€ìŠ¤íŒ…] ê´€ë ¨ ë²•ë¥  ë¬¸ì„œ ì ìˆ˜ ìƒí–¥...")
                        for doc in reranked_docs:
                            law_name = doc.metadata.get('law_name', '')
                            for rel_law in related_laws:
                                if rel_law in law_name:
                                    original_score = doc.metadata.get(
                                        'relevance_score', 0)
                                    boosted_score = min(
                                        1.0, original_score + 0.1)
                                    doc.metadata['relevance_score'] = boosted_score
                                    doc.metadata['boosted'] = True
                                    print(
                                        f"      â†‘ {law_name}: {original_score:.3f} â†’ {boosted_score:.3f}")
                                    break

                    # 6. ìœ ì‚¬ë„ ì„ê³„ê°’ í•„í„°ë§ - ë‚®ì€ ì ìˆ˜ ë¬¸ì„œ ì œì™¸
                    filtered_docs = []
                    for doc in reranked_docs:
                        score = doc.metadata.get('relevance_score', 0)
                        if score >= RELEVANCE_THRESHOLD:
                            filtered_docs.append(doc)

                    # [V7] 7. ë¬¸ì„œ íƒ€ì… í•„í„°ë§ ì œê±°ë¨ - ë¦¬ë­ì»¤ ì ìˆ˜ ìˆœì„œ ìœ ì§€

                    print(
                        f"âœ… [ë¦¬ë­í‚¹ ì™„ë£Œ] {len(reranked_docs)}ê°œ â†’ {len(filtered_docs)}ê°œ (ì„ê³„ê°’ {RELEVANCE_THRESHOLD} ì´ìƒ)")
                    for i, doc in enumerate(filtered_docs[:5], 1):
                        source = doc.metadata.get('search_source', 'unknown')
                        doc_type = doc.metadata.get('doc_type', '')
                        boosted = "â¬†ï¸" if doc.metadata.get('boosted') else ""
                        print(
                            f"   [{i}] ì ìˆ˜: {doc.metadata.get('relevance_score', 0):.4f} {boosted} | {doc_type} | {doc.page_content[:30]}...")

                    if filtered_docs:
                        docs = filtered_docs[:5]  # ìµœì¢… ìƒìœ„ 5ê°œ
                    else:
                        # ì„ê³„ê°’ í†µê³¼ ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ (ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ)
                        print(
                            f"âš ï¸  [ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ] ëª¨ë“  ë¬¸ì„œì˜ ìœ ì‚¬ë„ê°€ {RELEVANCE_THRESHOLD} ë¯¸ë§Œì…ë‹ˆë‹¤")
                        docs = []
                else:
                    print(f"âš ï¸  [ë¦¬ë­í‚¹ ê²°ê³¼ ì—†ìŒ] ì›ë³¸ ê²€ìƒ‰ ê²°ê³¼ ì‚¬ìš© (ìƒìœ„ 5ê°œ)")
                    docs = unique_docs[:5]
            except Exception as e:
                print(f"âš ï¸  [ë¦¬ë­í‚¹ ì˜¤ë¥˜] {e}")
                print(f"   ì›ë³¸ ê²€ìƒ‰ ê²°ê³¼ ì‚¬ìš© (ìƒìœ„ 5ê°œ)")
                docs = unique_docs[:5]

            if docs:
                print(f"âœ… [í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì™„ë£Œ] {len(docs)}ê°œ ê´€ë ¨ ë¬¸ì„œ")
            else:
                print(f"âš ï¸  [ê²€ìƒ‰ ì™„ë£Œ] ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ")
        else:
            docs = []
            print(f"âš ï¸  [ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ]")

        return {
            "retrieved_docs": docs
        }

    return search_documents


def create_generate_node(llm: ChatOpenAI):
    """ë…¸ë“œ 4: ìµœì¢… ë‹µë³€ ìƒì„± (DB ê²€ìƒ‰ ê²°ê³¼ë§Œ ì‚¬ìš©)"""

    answer_prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ ë²•ë¥  ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ 'A-TEAM ë´‡'ì…ë‹ˆë‹¤.

ì—­í• :
- ê²€ìƒ‰ëœ ë²•ë¥  ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•©ë‹ˆë‹¤.
- ë²•ë ¹ëª…, ì¡°í•­ ë“± êµ¬ì²´ì ì¸ ê·¼ê±°ë¥¼ ì œì‹œí•©ë‹ˆë‹¤.
- ë²•ë¥  ìš©ì–´ëŠ” ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª…í•©ë‹ˆë‹¤.

ë‹µë³€ ì‘ì„± ê·œì¹™:
1. ê²€ìƒ‰ëœ ìë£Œë¥¼ ê·¼ê±°ë¡œ ë‹µë³€í•˜ì„¸ìš”.
2. ë‹µë³€ êµ¬ì¡°: ğŸ“Œ ê²°ë¡  â†’ ğŸ“– ë²•ì  ê·¼ê±° â†’ ğŸ’¡ ì¶”ê°€ ì„¤ëª…
3. ê´€ë ¨ ë²•ë ¹ê³¼ ì¡°í•­ì„ [ë²•ë ¹ëª… ì œXì¡°]ì²˜ëŸ¼ ëª…ì‹œí•˜ì„¸ìš”.
4. í™•ì‹¤í•˜ì§€ ì•Šì€ ë‚´ìš©ì€ "~ë¡œ í•´ì„ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤" ë“±ìœ¼ë¡œ ì‹ ì¤‘í•˜ê²Œ í‘œí˜„í•˜ì„¸ìš”.
5. ê²€ìƒ‰ëœ ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.
6. ì „ë¬¸ ë²•ë¥  ìƒë‹´ì´ í•„ìš”í•œ ê²½ìš° ì•ˆë‚´í•˜ì„¸ìš”.
7. í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”."""),
        ("human", """ì‚¬ìš©ì ì§ˆë¬¸: {query}

ğŸ“š ê²€ìƒ‰ëœ ë²•ë ¹/ë¬¸ì„œ:
{context}

{case_law_notice}

ìœ„ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.""")
    ])

    def generate_answer(state: AgentState) -> AgentState:
        """ë‹µë³€ ìƒì„± ë…¸ë“œ: ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ë‹µë³€ ìƒì„±"""
        query = state["user_query"]
        docs = state.get("retrieved_docs", [])
        analysis = state.get("query_analysis", {})
        needs_case_law = analysis.get("needs_case_law", False)

        print(f"ğŸ’¬ [ë‹µë³€ ìƒì„± ì¤‘...]")

        # ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…
        if docs:
            context_parts = []
            for i, doc in enumerate(docs, 1):
                metadata = doc.metadata
                law_name = metadata.get("law_name", "")
                article = metadata.get("article_no", "")
                title = metadata.get(
                    "article_title", "") or metadata.get("title", "")
                content = doc.page_content[:800]

                header = f"[ë¬¸ì„œ {i}]"
                if law_name:
                    header += f" {law_name}"
                    if article:
                        header += f" ì œ{article}ì¡°"
                if title:
                    header += f" - {title}"

                context_parts.append(f"{header}\n{content}\n")

            context = "\n".join(context_parts)
        else:
            context = "(ê´€ë ¨ ë²•ë ¹ ë¬¸ì„œê°€ ê²€ìƒ‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤)"

        # íŒë¡€ í•„ìš” ì—¬ë¶€ ì•ˆë‚´ (DBì— íŒë¡€ê°€ ì—†ìœ¼ë¯€ë¡œ ì•ˆë‚´)
        if needs_case_law:
            case_law_notice = "âš ï¸ ì°¸ê³ : ì‚¬ìš©ìê°€ íŒë¡€ ì •ë³´ë¥¼ ìš”ì²­í–ˆìœ¼ë‚˜, í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ì— í•´ë‹¹ íŒë¡€ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ëŒ€ë²•ì› ì¢…í•©ë²•ë¥ ì •ë³´(https://glaw.scourt.go.kr)ì—ì„œ ì§ì ‘ ê²€ìƒ‰í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
        else:
            case_law_notice = ""

        # ê²€ìƒ‰ ê²°ê³¼ê°€ ì „í˜€ ì—†ëŠ” ê²½ìš°
        if not docs:
            answer = """ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë²•ë¥  ì •ë³´ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.

ë‹¤ìŒê³¼ ê°™ì´ ì‹œë„í•´ ë³´ì‹œê² ì–´ìš”?
1. ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš” (ì˜ˆ: ìƒí™©, ê´€ë ¨ ë²•ë ¹ ë“±)
2. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ì§ˆë¬¸í•´ ë³´ì„¸ìš”
3. ë³µì¡í•œ ì‚¬ì•ˆì˜ ê²½ìš° ì „ë¬¸ ë²•ë¥  ìƒë‹´ì„ ê¶Œì¥ë“œë¦½ë‹ˆë‹¤.

ğŸ“Œ ì°¸ê³  ì‚¬ì´íŠ¸:
- ë²•ì œì²˜ êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„°: https://law.go.kr
- ëŒ€ë²•ì› ì¢…í•©ë²•ë¥ ì •ë³´: https://glaw.scourt.go.kr"""
        else:
            # LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±
            chain = answer_prompt | llm
            response = chain.invoke({
                "query": query,
                "context": context,
                "case_law_notice": case_law_notice
            })
            answer = response.content

        print(f"âœ… [ë‹µë³€ ìƒì„± ì™„ë£Œ]")

        return {
            "generated_answer": answer
        }

    return generate_answer


# Pydantic ëª¨ë¸: ë‹µë³€ í‰ê°€ ê²°ê³¼ (Generator-Critic Light)
class AnswerEvaluation(BaseModel):
    """LLMì´ ë°˜í™˜í•  ë‹µë³€ í‰ê°€ ê²°ê³¼"""
    has_legal_basis: bool = Field(description="ë‹µë³€ì— ë²•ì  ê·¼ê±°(ë²•ë ¹, ì¡°í•­)ê°€ ëª…ì‹œë˜ì–´ ìˆëŠ”ê°€")
    cites_retrieved_docs: bool = Field(description="ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©ì„ ì‹¤ì œë¡œ ì¸ìš©í–ˆëŠ”ê°€")
    is_relevant: bool = Field(description="ë‹µë³€ì´ ì§ˆë¬¸ì— ì ì ˆíˆ ëŒ€ì‘í•˜ëŠ”ê°€")
    needs_more_search: bool = Field(description="ë” ë‚˜ì€ ë‹µë³€ì„ ìœ„í•´ ì¶”ê°€ ê²€ìƒ‰ì´ í•„ìš”í•œê°€")
    quality_score: int = Field(description="ë‹µë³€ í’ˆì§ˆ ì ìˆ˜ (1-5, 5ê°€ ìµœê³ )")
    improvement_suggestion: str = Field(
        default="", description="ê°œì„ ì´ í•„ìš”í•œ ê²½ìš° êµ¬ì²´ì  ì œì•ˆ")


def create_evaluate_node(llm: ChatOpenAI):
    """ë…¸ë“œ 6: ë‹µë³€ í’ˆì§ˆ í‰ê°€ (Generator-Critic Light)"""

    structured_llm = llm.with_structured_output(AnswerEvaluation)

    evaluate_prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ ë²•ë¥  ë‹µë³€ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ë¹„í‰ê°€(Critic)ì…ë‹ˆë‹¤.

## í‰ê°€ ê¸°ì¤€
1. **has_legal_basis**: ë‹µë³€ì— ë²•ë ¹ëª…, ì¡°í•­ ë²ˆí˜¸, íŒë¡€ ë²ˆí˜¸ ë“± êµ¬ì²´ì  ë²•ì  ê·¼ê±°ê°€ ìˆëŠ”ê°€
2. **cites_retrieved_docs**: ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ë‚´ìš©ì´ ë‹µë³€ì— ì‹¤ì œë¡œ ë°˜ì˜ë˜ì—ˆëŠ”ê°€
3. **is_relevant**: ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ë‹µí•˜ê³  ìˆëŠ”ê°€
4. **needs_more_search**: ê²€ìƒ‰ ê²°ê³¼ê°€ ë¶€ì¡±í•˜ì—¬ ì¶”ê°€ ê²€ìƒ‰ì´ í•„ìš”í•œê°€
   - true: ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì§ˆë¬¸ê³¼ ê´€ë ¨ ì—†ê±°ë‚˜, ë‹µë³€ì— "ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤" ë“±ì´ í¬í•¨ëœ ê²½ìš°
   - false: ì¶©ë¶„í•œ ê·¼ê±°ê°€ ìˆê±°ë‚˜, ì´ë¯¸ ìµœì„ ì˜ ë‹µë³€ì¸ ê²½ìš°
5. **quality_score**: 1-5ì  (1: ë§¤ìš° ë¶€ì¡±, 3: ë³´í†µ, 5: ë§¤ìš° ìš°ìˆ˜)

## íŒë‹¨ ì›ì¹™
- ë²•ë¥  ë‹µë³€ì€ ì •í™•ì„±ì´ ìƒëª…ì…ë‹ˆë‹¤. ê·¼ê±° ì—†ëŠ” ë‹µë³€ì€ ë‚®ì€ ì ìˆ˜ë¥¼ ë°›ìŠµë‹ˆë‹¤.
- ë‹¨, ë²•ë ¹DBì— í•´ë‹¹ ì •ë³´ê°€ ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì–´ë„ í•©ë¦¬ì  ë‹µë³€ì´ë©´ ì¸ì •í•©ë‹ˆë‹¤.
- quality_score 3ì  ì´ìƒì´ë©´ í†µê³¼, 2ì  ì´í•˜ë©´ ì¬ê²€ìƒ‰ì„ ê¶Œì¥í•©ë‹ˆë‹¤."""),
        ("human", """## ì‚¬ìš©ì ì§ˆë¬¸
{query}

## ê²€ìƒ‰ëœ ë¬¸ì„œ (ìš”ì•½)
{context_summary}

## ìƒì„±ëœ ë‹µë³€
{answer}

ìœ„ ë‹µë³€ì„ í‰ê°€í•´ì£¼ì„¸ìš”.""")
    ])

    def evaluate_answer(state: AgentState) -> AgentState:
        """ë‹µë³€ í‰ê°€ ë…¸ë“œ: ìƒì„±ëœ ë‹µë³€ì˜ í’ˆì§ˆì„ LLMìœ¼ë¡œ í‰ê°€"""
        query = state["user_query"]
        answer = state.get("generated_answer", "")
        docs = state.get("retrieved_docs", [])
        retry_count = state.get("retry_count", 0) or 0

        print(f"ğŸ” [ë‹µë³€ í‰ê°€ ì¤‘...] (ì‹œë„ {retry_count + 1}íšŒ)")

        # ê²€ìƒ‰ëœ ë¬¸ì„œ ìš”ì•½ ìƒì„±
        if docs:
            context_summary = "\n".join([
                f"- {doc.metadata.get('law_name', 'ë¬¸ì„œ')} {doc.metadata.get('article_no', '')}: {doc.page_content[:100]}..."
                for doc in docs[:5]
            ])
        else:
            context_summary = "(ê²€ìƒ‰ëœ ë¬¸ì„œ ì—†ìŒ)"

        # LLMìœ¼ë¡œ í‰ê°€
        chain = evaluate_prompt | structured_llm
        evaluation: AnswerEvaluation = chain.invoke({
            "query": query,
            "context_summary": context_summary,
            "answer": answer
        })

        print(f"ğŸ“Š [í‰ê°€ ê²°ê³¼]")
        print(f"   ë²•ì  ê·¼ê±°: {'âœ…' if evaluation.has_legal_basis else 'âŒ'}")
        print(f"   ë¬¸ì„œ ì¸ìš©: {'âœ…' if evaluation.cites_retrieved_docs else 'âŒ'}")
        print(f"   ë‹µë³€ ì í•©: {'âœ…' if evaluation.is_relevant else 'âŒ'}")
        print(
            f"   í’ˆì§ˆ ì ìˆ˜: {'â­' * evaluation.quality_score} ({evaluation.quality_score}/5)")
        if evaluation.needs_more_search:
            print(f"   âš ï¸  ì¶”ê°€ ê²€ìƒ‰ ê¶Œì¥: {evaluation.improvement_suggestion}")

        return {
            "evaluation_result": evaluation.model_dump(),
            "retry_count": retry_count + 1
        }

    return evaluate_answer


def route_after_evaluation(state: AgentState) -> Literal["search", "end"]:
    """í‰ê°€ í›„ ë¼ìš°íŒ…: ì¬ê²€ìƒ‰ í•„ìš” ì—¬ë¶€ì— ë”°ë¼ ë¶„ê¸°"""
    evaluation = state.get("evaluation_result", {})
    retry_count = state.get("retry_count", 0) or 0

    needs_more_search = evaluation.get("needs_more_search", False)
    quality_score = evaluation.get("quality_score", 3)

    # ë¬´í•œ ë£¨í”„ ë°©ì§€: ìµœëŒ€ 1íšŒë§Œ ì¬ì‹œë„
    if retry_count >= 2:
        print(f"âš ï¸  [ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ë„ë‹¬] í˜„ì¬ ë‹µë³€ ì‚¬ìš©")
        return "end"

    # í’ˆì§ˆ ì ìˆ˜ê°€ 2ì  ì´í•˜ì´ê³  ì¶”ê°€ ê²€ìƒ‰ì´ í•„ìš”í•˜ë©´ ì¬ê²€ìƒ‰
    if needs_more_search and quality_score <= 2:
        print(f"ğŸ”„ [ì¬ê²€ìƒ‰ ê²°ì •] í’ˆì§ˆ í–¥ìƒì„ ìœ„í•´ ë‹¤ì‹œ ê²€ìƒ‰í•©ë‹ˆë‹¤")
        return "search"

    print(f"âœ… [í‰ê°€ í†µê³¼] ë‹µë³€ í’ˆì§ˆ ì¶©ë¶„")
    return "end"


# ===========================
# ë¼ìš°íŒ… í•¨ìˆ˜ (ì¡°ê±´ë¶€ ë¶„ê¸°)
# ===========================

def route_after_analysis(state: AgentState) -> Literal["clarify", "search"]:
    """ë¶„ì„ í›„ ë¼ìš°íŒ…: ëª…í™•í™” í•„ìš” ì—¬ë¶€ì— ë”°ë¼ ë¶„ê¸°"""
    analysis = state.get("query_analysis", {})
    needs_clarification = analysis.get("needs_clarification", False)

    if needs_clarification:
        return "clarify"
    else:
        return "search"


# ===========================
# ì‚¬ì „ ì¤€ë¹„ ì˜ì—­: ë¦¬ì†ŒìŠ¤ ì´ˆê¸°í™”
# ===========================
def initialize_resources():
    """ì„ë² ë”© ëª¨ë¸, ë²¡í„°ìŠ¤í† ì–´, BM25 Retriever ì´ˆê¸°í™”"""

    # 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
    COLLECTION_NAME = os.getenv("QDRANT_COLLECTION_NAME")
    QDRANT_URL = os.getenv("QDRANT_URL")
    QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")

    if not QDRANT_API_KEY:
        raise ValueError("QDRANT_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")

    print(f"ğŸ”§ ì„¤ì • ë¡œë“œ ì™„ë£Œ")

    # 2. ì„ë² ë”© ëª¨ë¸ ì„¤ì •
    print(f"\nğŸš€ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘ (Qwen/Qwen3-Embedding-0.6B)...")
    embeddings = HuggingFaceEmbeddings(
        model_name="Qwen/Qwen3-Embedding-0.6B",
        model_kwargs={'trust_remote_code': True},
        encode_kwargs={'normalize_embeddings': True}
    )
    print("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

    # 3. Qdrant í´ë¼ì´ì–¸íŠ¸ ì—°ê²°
    print(f"\nğŸ“¡ Qdrant ì—°ê²° ì¤‘...")
    warnings.filterwarnings(
        'ignore', message='Api key is used with an insecure connection')

    client = QdrantClient(
        url=QDRANT_URL,
        api_key=QDRANT_API_KEY,
        timeout=30,
        prefer_grpc=False)
    print("âœ… Qdrant ì—°ê²° ì™„ë£Œ")

    # 4. ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
    print(f"\nğŸ—‚ï¸  ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” ì¤‘...")
    print("   (ì»¬ë ‰ì…˜ ê²€ì¦ ì¤‘... ë„¤íŠ¸ì›Œí¬ ìƒíƒœì— ë”°ë¼ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    vectorstore = QdrantVectorStore(
        client=client,
        collection_name=COLLECTION_NAME,
        embedding=embeddings,
        content_payload_key="text"
    )
    print("âœ… ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” ì™„ë£Œ")

    # 5. BM25 Retriever ì´ˆê¸°í™”
    print(f"\nğŸ“ BM25 Retriever ì´ˆê¸°í™” ì¤‘...")
    bm25_retriever = None
    try:
        # ì „ì²´ ë¬¸ì„œ ìˆ˜ í™•ì¸
        collection_info = client.get_collection(COLLECTION_NAME)
        total_points = collection_info.points_count
        print(f"   ì»¬ë ‰ì…˜ ë‚´ ì „ì²´ ë¬¸ì„œ: {total_points}ê°œ")

        # ë¬¸ì„œ ë¡œë“œ (BM25ìš© - ìµœëŒ€ 2000ê°œ ìƒ˜í”Œë§)
        sample_size = min(2000, total_points)
        scroll_result = client.scroll(
            collection_name=COLLECTION_NAME,
            limit=sample_size,
            with_payload=True,
            with_vectors=False
        )

        bm25_docs = []
        for point in scroll_result[0]:
            payload = point.payload
            text = payload.get("text", "")
            if text:
                doc = Document(
                    page_content=text,
                    metadata={k: v for k, v in payload.items() if k != "text"}
                )
                bm25_docs.append(doc)

        if bm25_docs:
            bm25_retriever = BM25Retriever.from_documents(bm25_docs, k=15)
            print(f"âœ… BM25 Retriever ì´ˆê¸°í™” ì™„ë£Œ ({len(bm25_docs)}ê°œ ë¬¸ì„œ ì¸ë±ì‹±)")
        else:
            print("âš ï¸  BM25ìš© ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. Vector Searchë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤.")

    except Exception as e:
        print(f"âš ï¸  BM25 Retriever ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        print("   Vector Searchë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤.")

    return {
        "embeddings": embeddings,
        "vectorstore": vectorstore,
        "bm25_retriever": bm25_retriever
    }


# ===========================
# LangGraph ì´ˆê¸°í™”
# ===========================
def initialize_langgraph_chatbot():
    """LangGraph ê¸°ë°˜ RAG ì±—ë´‡ ì´ˆê¸°í™” (ì¡°ê±´ë¶€ ë¶„ê¸° í¬í•¨, í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ + Query Expansion)"""

    # ì‚¬ì „ ì¤€ë¹„: ë¦¬ì†ŒìŠ¤ ì´ˆê¸°í™”
    resources = initialize_resources()
    vectorstore = resources["vectorstore"]
    bm25_retriever = resources.get("bm25_retriever")  # BM25 Retriever

    # LLM ì„¤ì •
    print(f"\nğŸ¤– LLM ì„¤ì • ì¤‘...")
    llm = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0,
        streaming=True
    )
    print("âœ… LLM ì„¤ì • ì™„ë£Œ")

    # Query Expander ìƒì„±
    print(f"\nğŸ”„ Query Expander ì´ˆê¸°í™” ì¤‘...")
    query_expander = create_query_expander(llm)
    print("âœ… Query Expander ì´ˆê¸°í™” ì™„ë£Œ")

    # ë…¸ë“œ ìƒì„±
    print(f"\nâš™ï¸  LangGraph ë…¸ë“œ ìƒì„± ì¤‘...")
    analyze_node = create_analyze_query_node(llm)
    clarify_node = create_clarify_node(llm)
    # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë…¸ë“œ (Vector + BM25 + Query Expansion + ìœ ì‚¬ë„ í•„í„°ë§)
    search_node = create_search_node(
        vectorstore, bm25_retriever, query_expander)
    generate_node = create_generate_node(llm)
    # [V5] ë‹µë³€ í‰ê°€ ë…¸ë“œ (Generator-Critic Light)
    evaluate_node = create_evaluate_node(llm)
    print("âœ… ë…¸ë“œ ìƒì„± ì™„ë£Œ (5ê°œ: analyze â†’ search â†’ generate â†’ evaluate)")

    # StateGraph êµ¬ì„±
    print(f"\nğŸ”— LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„± ì¤‘...")
    workflow = StateGraph(AgentState)

    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("analyze", analyze_node)
    workflow.add_node("clarify", clarify_node)
    workflow.add_node("search", search_node)
    workflow.add_node("generate", generate_node)
    workflow.add_node("evaluate", evaluate_node)

    # ì—£ì§€ ì¶”ê°€
    workflow.set_entry_point("analyze")

    # ì¡°ê±´ë¶€ ë¶„ê¸° 1: ë¶„ì„ í›„ â†’ ëª…í™•í™” í•„ìš”? â†’ clarify / search
    workflow.add_conditional_edges(
        "analyze",
        route_after_analysis,
        {
            "clarify": "clarify",
            "search": "search"
        }
    )

    # clarifyëŠ” ë°”ë¡œ ì¢…ë£Œ
    workflow.add_edge("clarify", END)

    # ê²€ìƒ‰ í›„ â†’ ë‹µë³€ ìƒì„± (íŒë¡€ ì›¹ê²€ìƒ‰ ì œê±°, ë°”ë¡œ generateë¡œ)
    workflow.add_edge("search", "generate")

    # [V5] ë‹µë³€ ìƒì„± í›„ â†’ í‰ê°€ (Generator-Critic Light)
    workflow.add_edge("generate", "evaluate")

    # [V5] ì¡°ê±´ë¶€ ë¶„ê¸° 2: í‰ê°€ í›„ â†’ ì¬ê²€ìƒ‰ í•„ìš”? â†’ search / END
    workflow.add_conditional_edges(
        "evaluate",
        route_after_evaluation,
        {
            "search": "search",
            "end": END
        }
    )

    # ê·¸ë˜í”„ ì»´íŒŒì¼
    graph = workflow.compile()
    print("âœ… LangGraph êµ¬ì„± ì™„ë£Œ (Generator-Critic Light í¬í•¨)")

    return graph


# ===========================
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ===========================
def main():
    """LangGraph RAG ì±—ë´‡ ì‹¤í–‰ ë©”ì¸ í•¨ìˆ˜"""

    # API Key í™•ì¸
    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ ì˜¤ë¥˜: OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ .env íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
        return

    # Tavily API Key í™•ì¸ (ê²½ê³ ë§Œ)
    if not os.getenv("TAVILY_API_KEY"):
        print("âš ï¸  ê²½ê³ : TAVILY_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   íŒë¡€ ì›¹ ê²€ìƒ‰ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.\n")

    try:
        # ì±—ë´‡ ì´ˆê¸°í™”
        print("\n" + "="*60)
        print("ğŸš€ A-TEAM ë²•ë¥  RAG ì±—ë´‡ (LangGraph V5) ì´ˆê¸°í™” ì‹œì‘")
        print("="*60 + "\n")

        graph = initialize_langgraph_chatbot()

        print("\n" + "="*60)
        print("âœ… ğŸ¤– A-TEAM ë²•ë¥  ì±—ë´‡ ì¤€ë¹„ ì™„ë£Œ!")
        print("="*60)
        print("\nğŸ’¡ ì‚¬ìš© ë°©ë²•:")
        print("  - ë…¸ë™ë²•, í˜•ì‚¬ë²•, ë¯¼ì‚¬ë²• ê´€ë ¨ ì§ˆë¬¸ì— ì‘ë‹µí•©ë‹ˆë‹¤.")
        print("  - ì§ˆë¬¸ì´ ëª¨í˜¸í•˜ë©´ êµ¬ì²´í™”ë¥¼ ìš”ì²­í•©ë‹ˆë‹¤.")
        print("  - 'exit', 'quit', 'ì¢…ë£Œ'ë¥¼ ì…ë ¥í•˜ë©´ ì¢…ë£Œë©ë‹ˆë‹¤")
        print("\nğŸ“Š ì›Œí¬í”Œë¡œìš° (V5 - Generator-Critic Light):")
        print("  â”Œâ”€ ì§ˆë¬¸ ë¶„ì„ â”€â”¬â”€ [ëª¨í˜¸í•¨] â†’ ëª…í™•í™” ìš”ì²­ â†’ ì¢…ë£Œ")
        print("  â”‚            â””â”€ [ëª…í™•í•¨] â†’ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ â†’ ë‹µë³€ ìƒì„± â†’ í‰ê°€")
        print("  â”‚                           (ìœ ì‚¬ë„ 0.3 ë¯¸ë§Œ í•„í„°ë§)     â†“")
        print(
            "  â”‚                                           [í’ˆì§ˆ ë¶€ì¡±] â†“   â†“ [í†µê³¼]")
        print("  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì¬ê²€ìƒ‰ â†â”€â”€â”€â”˜   ì¢…ë£Œ")
        print("="*60 + "\n")

        # ëŒ€í™” ë£¨í”„
        while True:
            try:
                # ì‚¬ìš©ì ì…ë ¥
                user_input = input("ğŸ‘¤ User >> ").strip()

                # ì¢…ë£Œ ëª…ë ¹ í™•ì¸
                if user_input.lower() in ["exit", "quit", "ì¢…ë£Œ", "q"]:
                    print("\nğŸ‘‹ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!")
                    break

                # ë¹ˆ ì…ë ¥ ì²´í¬
                if not user_input:
                    print("âŒ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.\n")
                    continue

                # ì´ˆê¸° ìƒíƒœ ì„¤ì •
                initial_state = {
                    "messages": [HumanMessage(content=user_input)],
                    "user_query": user_input,
                    "query_analysis": None,
                    "retrieved_docs": None,
                    "generated_answer": None,
                    "next_action": None,
                    "evaluation_result": None,
                    "retry_count": 0
                }

                # ê·¸ë˜í”„ ì‹¤í–‰
                print("\n" + "-"*60)
                print("ğŸ”„ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘...")
                print("-"*60 + "\n")

                result = graph.invoke(initial_state)

                # ìµœì¢… ë‹µë³€ ì¶œë ¥
                answer = result.get("generated_answer", "")
                if answer:
                    print("\n" + "="*60)
                    print("ğŸ¤– AI ë‹µë³€:")
                    print("="*60)
                    print(f"\n{answer}\n")
                    print("="*60 + "\n")
                else:
                    print("\nâš ï¸ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n")

            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!")
                break
            except Exception as e:
                print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
                print("ğŸ’¡ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.\n")
                import traceback
                traceback.print_exc()

    except Exception as e:
        print(f"\nâŒ ì±—ë´‡ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        print("ğŸ’¡ ì„¤ì •ì„ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        import traceback
        traceback.print_exc()
        raise


if __name__ == "__main__":
    main()
