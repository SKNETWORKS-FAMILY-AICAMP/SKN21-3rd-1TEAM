################################################
# A-TEAM ë²•ë¥  RAG ì±—ë´‡ (LangGraph V8)
# V8 ë¦¬íŒ©í† ë§:
# - @dataclass Configë¡œ ì„¤ì • ë¶„ë¦¬
# - ê³„ì¸µí™”ëœ êµ¬ì¡°: Infrastructure â†’ Logic â†’ Execution
# - ì½”ë“œ ê°€ë…ì„± í–¥ìƒ
# ê¸°ì¡´ ê¸°ëŠ¥: ì§ˆë¬¸ ì˜ë„ ë¶„ì„, Hybrid Retriever, Query Expansion, Generator-Critic
# ì‘ì„±ì: SKN 3-1íŒ€ A-TEAM
# ì‘ì„±ì¼: 2026-01-08
################################################

import os
import sys
import logging
import warnings
from pathlib import Path
from dataclasses import dataclass, field
from typing import (
    Annotated, TypedDict, Sequence, Optional, List, Literal, Any
)

# Third-party
import torch
from dotenv import load_dotenv
from pydantic import BaseModel, Field
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# LangChain Core
from langchain_core.documents import Document, BaseDocumentCompressor
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_huggingface import HuggingFaceEmbeddings

# LangChain Retrievers
from langchain_community.retrievers import BM25Retriever

# Qdrant
from langchain_qdrant import QdrantVectorStore
from qdrant_client import QdrantClient

# LangGraph
from langgraph.graph.message import add_messages
from langgraph.graph import StateGraph, END


# ============================================================
# [SECTION 1] Configuration - ëª¨ë“  ì„¤ì •ê°’ì„ í•œ ê³³ì—ì„œ ê´€ë¦¬
# ============================================================
@dataclass
class Config:
    """Application Configuration (dataclass)

    ëª¨ë“  í•˜ë“œì½”ë”©ëœ ê°’ì„ ì´ê³³ì—ì„œ ê´€ë¦¬í•©ë‹ˆë‹¤.
    ë³€ê²½ì´ í•„ìš”í•œ ê²½ìš° ì´ í´ë˜ìŠ¤ë§Œ ìˆ˜ì •í•˜ë©´ ë©ë‹ˆë‹¤.
    """

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # [1] Models - ì‚¬ìš©í•  ëª¨ë¸ ì„¤ì •
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    LLM_MODEL: str = "gpt-4o-mini"
    LLM_TEMPERATURE: float = 0.0
    EMBEDDING_MODEL: str = "Qwen/Qwen3-Embedding-0.6B"
    RERANKER_MODEL: str = "jinaai/jina-reranker-v2-base-multilingual"

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # [2] RAG Settings - ê²€ìƒ‰ ë° ì²˜ë¦¬ ì„¤ì •
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    VECTOR_DIM: int = 1024
    TOP_K_VECTOR: int = 15
    TOP_K_BM25: int = 15
    TOP_K_RERANK: int = 7
    TOP_K_FINAL: int = 5
    RELEVANCE_THRESHOLD: float = 0.2
    BM25_SAMPLE_SIZE: int = 2000
    MAX_RETRY: int = 2

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # [3] Qdrant - ë²¡í„° DB ì„¤ì •
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    QDRANT_TIMEOUT: int = 30

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # [4] PROMPTS - ë…¸ë“œë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    # --- [ë…¸ë“œ: Query Expansion] ì¿¼ë¦¬ í™•ì¥ìš© í”„ë¡¬í”„íŠ¸ ---
    PROMPT_QUERY_EXPANSION: str = """ë‹¹ì‹ ì€ í•œêµ­ ë²•ë¥  ê²€ìƒ‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ë²•ë¥  ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ë²¡í„° ê²€ìƒ‰ê³¼ í‚¤ì›Œë“œ ê²€ìƒ‰ì— ìµœì í™”ëœ ì¿¼ë¦¬ë¡œ í™•ì¥í•©ë‹ˆë‹¤.

## í™•ì¥ ì „ëµ
1. í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ: ì§ˆë¬¸ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ë²•ë¥  ê°œë… 3-5ê°œ ì¶”ì¶œ
2. ë²•ë¥  ìš©ì–´ ë§¤í•‘: ì¼ìƒ í‘œí˜„ì„ ë²•ë¥  ìš©ì–´ë¡œ ë³€í™˜
3. ê´€ë ¨ ì¡°í•­ ì¶”ë¡ : í•´ë‹¹ ë¶„ì•¼ì˜ ëŒ€í‘œ ë²•ë ¹ëª…ê³¼ ì¡°í•­ ì¶”ì •
4. ë™ì˜ì–´ í™•ì¥: ê²€ìƒ‰ ë²”ìœ„ë¥¼ ë„“íˆê¸° ìœ„í•œ ìœ ì‚¬ í‘œí˜„ ì¶”ê°€

## ì¶œë ¥ ê·œì¹™
- expanded_queryëŠ” ì›ë³¸ ì§ˆë¬¸ + í•µì‹¬ í‚¤ì›Œë“œ + ê´€ë ¨ ë²•ë ¹ëª…ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì¡°í•©
- ìµœëŒ€ 100ì ì´ë‚´ë¡œ ì••ì¶•"""

    # --- [ë…¸ë“œ: Analyze] ì§ˆë¬¸ ë¶„ì„ìš© í”„ë¡¬í”„íŠ¸ ---
    PROMPT_ANALYZE: str = """ë‹¹ì‹ ì€ ë²•ë¥  ì§ˆë¬¸ì„ ì‹¬ì¸µ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

## ë¶„ë¥˜
- category: ë…¸ë™ë²•, í˜•ì‚¬ë²•, ë¯¼ì‚¬ë²•, ê¸°íƒ€
- intent_type: ë²•ë ¹ì¡°íšŒ, ì ˆì°¨ë¬¸ì˜, ìƒí™©íŒë‹¨, ê¶Œë¦¬í™•ì¸, ë¶„ìŸí•´ê²°, ì¼ë°˜ìƒë‹´
- search_strategy: ë²•ë ¹ìš°ì„ , í–‰ì •í•´ì„ìš°ì„ , íŒë¡€í•„ìˆ˜, ì¢…í•©ê²€ìƒ‰
- target_doc_types: ë²•, ì‹œí–‰ë ¹, ì‹œí–‰ê·œì¹™, í–‰ì •í•´ì„, íŒì •ì„ ë¡€

## ê·œì¹™
- needs_clarification: 1~2ë‹¨ì–´ë§Œ ìˆì–´ ë‹µë³€ ë¶ˆê°€ëŠ¥í•œ ê²½ìš°ì—ë§Œ true
- needs_case_law: íŒë¡€ ì–¸ê¸‰ ë˜ëŠ” ë²•ì  í•´ì„ ìŸì ì´ ìˆëŠ” ê²½ìš° true"""

    # --- [ë…¸ë“œ: Generate] ë‹µë³€ ìƒì„±ìš© í”„ë¡¬í”„íŠ¸ ---
    PROMPT_GENERATE: str = """ë‹¹ì‹ ì€ ë²•ë¥  ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ 'A-TEAM ë´‡'ì…ë‹ˆë‹¤.

ì—­í• :
- ê²€ìƒ‰ëœ ë²•ë¥  ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•©ë‹ˆë‹¤.
- ë²•ë ¹ëª…, ì¡°í•­ ë“± êµ¬ì²´ì ì¸ ê·¼ê±°ë¥¼ ì œì‹œí•©ë‹ˆë‹¤.
- ë²•ë¥  ìš©ì–´ëŠ” ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª…í•©ë‹ˆë‹¤.

ë‹µë³€ ì‘ì„± ê·œì¹™:
1. ê²€ìƒ‰ëœ ìë£Œë¥¼ ê·¼ê±°ë¡œ ë‹µë³€í•˜ì„¸ìš”.
2. ë‹µë³€ êµ¬ì¡°: ğŸ“Œ ê²°ë¡  â†’ ğŸ“– ë²•ì  ê·¼ê±° â†’ ğŸ’¡ ì¶”ê°€ ì„¤ëª…
3. ê´€ë ¨ ë²•ë ¹ê³¼ ì¡°í•­ì„ [ë²•ë ¹ëª… ì œXì¡°]ì²˜ëŸ¼ ëª…ì‹œí•˜ì„¸ìš”.
4. í™•ì‹¤í•˜ì§€ ì•Šì€ ë‚´ìš©ì€ "~ë¡œ í•´ì„ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤" ë“±ìœ¼ë¡œ ì‹ ì¤‘í•˜ê²Œ í‘œí˜„í•˜ì„¸ìš”.
5. ê²€ìƒ‰ëœ ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.
6. ì „ë¬¸ ë²•ë¥  ìƒë‹´ì´ í•„ìš”í•œ ê²½ìš° ì•ˆë‚´í•˜ì„¸ìš”.
7. í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”."""

    # --- [ë…¸ë“œ: Evaluate] ë‹µë³€ í‰ê°€ìš© í”„ë¡¬í”„íŠ¸ ---
    PROMPT_EVALUATE: str = """ë‹¹ì‹ ì€ ë²•ë¥  ë‹µë³€ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ë¹„í‰ê°€ì…ë‹ˆë‹¤.

## í‰ê°€ ê¸°ì¤€
1. has_legal_basis: ë²•ë ¹ëª…, ì¡°í•­ ë²ˆí˜¸ ë“± êµ¬ì²´ì  ë²•ì  ê·¼ê±° ìˆëŠ”ê°€
2. cites_retrieved_docs: ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©ì´ ë°˜ì˜ë˜ì—ˆëŠ”ê°€
3. is_relevant: ì§ˆë¬¸ì— ì§ì ‘ ë‹µí•˜ëŠ”ê°€
4. needs_more_search: ê²€ìƒ‰ ê²°ê³¼ ë¶€ì¡±í•˜ì—¬ ì¶”ê°€ ê²€ìƒ‰ í•„ìš”í•œê°€
5. quality_score: 1-5ì 

## ì›ì¹™
- í’ˆì§ˆ 3ì  ì´ìƒì´ë©´ í†µê³¼, 2ì  ì´í•˜ë©´ ì¬ê²€ìƒ‰ ê¶Œì¥"""

    # --- [ë…¸ë“œ: Clarify] ëª…í™•í™” ìš”ì²­ í…œí”Œë¦¿ ---
    TEMPLATE_CLARIFY: str = """ì•ˆë…•í•˜ì„¸ìš”! ì§ˆë¬¸ì„ ì˜ ì´í•´í•˜ê¸° ìœ„í•´ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.

{clarification_question}

ìœ„ ë‚´ìš©ì„ í¬í•¨í•´ì„œ ë‹¤ì‹œ ì§ˆë¬¸í•´ ì£¼ì‹œë©´, ë” ì •í™•í•œ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ğŸ˜Š"""

    # --- [ë…¸ë“œ: Generate] ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ ì‹œ ë‹µë³€ ---
    TEMPLATE_NO_RESULTS: str = """ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ë²•ë¥  ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.

ë‹¤ìŒê³¼ ê°™ì´ ì‹œë„í•´ ë³´ì„¸ìš”:
1. ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±
2. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ì§ˆë¬¸
3. ì „ë¬¸ ë²•ë¥  ìƒë‹´ ê¶Œì¥

ğŸ“Œ ì°¸ê³ : https://law.go.kr"""


# ============================================================
# [SECTION 2] Logging Setup
# ============================================================
logging.basicConfig(
    format="%(asctime)s [%(levelname)s] %(name)s - %(message)s",
    level=logging.INFO,
    handlers=[logging.StreamHandler(sys.stdout)],
)
logger = logging.getLogger("LegalRAG-V8")


# ============================================================
# [SECTION 3] State Definition - LangGraph ìƒíƒœ ì •ì˜
# ============================================================
class AgentState(TypedDict):
    """LangGraph Agentì˜ ìƒíƒœ"""
    messages: Annotated[Sequence[BaseMessage], add_messages]
    user_query: str
    query_analysis: Optional[dict]
    retrieved_docs: Optional[List[Document]]
    generated_answer: Optional[str]
    next_action: Optional[str]
    evaluation_result: Optional[dict]
    retry_count: Optional[int]


# ============================================================
# [SECTION 4] Reranker - ì»¤ìŠ¤í…€ Jina Reranker Wrapper
# ============================================================
class JinaReranker(BaseDocumentCompressor):
    """Jina Reranker Wrapper for LangChain"""
    model_name: str = "jinaai/jina-reranker-v2-base-multilingual"
    top_n: int = 7
    model: Any = None
    tokenizer: Any = None

    class Config:
        arbitrary_types_allowed = True
        extra = "allow"

    def __init__(self, model_name: str = None, top_n: int = None, **kwargs):
        super().__init__(**kwargs)
        if model_name:
            self.model_name = model_name
        if top_n:
            self.top_n = top_n

        logger.info(f"Loading Reranker: {self.model_name}")
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name, trust_remote_code=True)
        self.model = AutoModelForSequenceClassification.from_pretrained(
            self.model_name, trust_remote_code=True, torch_dtype="auto"
        )
        self.model.eval()
        logger.info("Reranker loaded successfully")

    def compress_documents(
        self, documents: Sequence[Document], query: str, callbacks: Optional[Any] = None
    ) -> Sequence[Document]:
        if not documents:
            return []

        pairs = [[query, doc.page_content] for doc in documents]

        with torch.no_grad():
            inputs = self.tokenizer(
                pairs, padding=True, truncation=True,
                return_tensors="pt", max_length=512
            )
            scores = self.model(**inputs).logits.squeeze(-1).float().cpu()
            scores = torch.sigmoid(scores).tolist()
            if not isinstance(scores, list):
                scores = [scores]

        # Sort and select top_n
        top_indices = sorted(
            range(len(scores)), key=lambda i: scores[i], reverse=True
        )[:self.top_n]

        final_docs = []
        for i in top_indices:
            doc = documents[i]
            doc.metadata["relevance_score"] = scores[i]
            final_docs.append(doc)

        return final_docs


# ============================================================
# [SECTION 5] Pydantic Schemas - LLM êµ¬ì¡°í™”ëœ ì¶œë ¥ìš©
# ============================================================
class ExpandedQuery(BaseModel):
    """ê²€ìƒ‰ ì¿¼ë¦¬ í™•ì¥ ê²°ê³¼"""
    original_query: str = Field(description="ì›ë³¸ ì‚¬ìš©ì ì§ˆë¬¸")
    search_keywords: List[str] = Field(description="í•µì‹¬ ê²€ìƒ‰ í‚¤ì›Œë“œ (3-5ê°œ)")
    legal_terms: List[str] = Field(description="ê´€ë ¨ ë²•ë¥  ìš©ì–´ ë° ì¡°í•­ëª…")
    synonyms: List[str] = Field(description="ë™ì˜ì–´ ë° ìœ ì‚¬ í‘œí˜„ (2-3ê°œ)")
    expanded_query: str = Field(description="í™•ì¥ëœ ê²€ìƒ‰ ì¿¼ë¦¬")


class QueryAnalysis(BaseModel):
    """ì§ˆë¬¸ ë¶„ì„ ê²°ê³¼"""
    category: str = Field(description="ë²•ë¥  ë¶„ì•¼: ë…¸ë™ë²•, í˜•ì‚¬ë²•, ë¯¼ì‚¬ë²•, ê¸°íƒ€")
    needs_clarification: bool = Field(default=False, description="ì§ˆë¬¸ ëª¨í˜¸ ì—¬ë¶€")
    needs_case_law: bool = Field(default=False, description="íŒë¡€ ê²€ìƒ‰ í•„ìš” ì—¬ë¶€")
    clarification_question: str = Field(default="", description="ëª…í™•í™” ì§ˆë¬¸")
    intent_type: str = Field(
        description="ì§ˆë¬¸ ì˜ë„: ë²•ë ¹ì¡°íšŒ, ì ˆì°¨ë¬¸ì˜, ìƒí™©íŒë‹¨, ê¶Œë¦¬í™•ì¸, ë¶„ìŸí•´ê²°, ì¼ë°˜ìƒë‹´")
    user_situation: str = Field(default="", description="ì‚¬ìš©ì ìƒí™© ìš”ì•½")
    core_question: str = Field(default="", description="í•µì‹¬ ì§ˆë¬¸")
    search_strategy: str = Field(description="ê²€ìƒ‰ ì „ëµ: ë²•ë ¹ìš°ì„ , í–‰ì •í•´ì„ìš°ì„ , íŒë¡€í•„ìˆ˜, ì¢…í•©ê²€ìƒ‰")
    target_doc_types: List[str] = Field(
        default_factory=list, description="ê²€ìƒ‰ ëŒ€ìƒ ë¬¸ì„œ íƒ€ì…")
    related_laws: List[str] = Field(default_factory=list, description="ê´€ë ¨ ë²•ë¥ ëª…")


class AnswerEvaluation(BaseModel):
    """ë‹µë³€ í‰ê°€ ê²°ê³¼"""
    has_legal_basis: bool = Field(description="ë²•ì  ê·¼ê±° ëª…ì‹œ ì—¬ë¶€")
    cites_retrieved_docs: bool = Field(description="ê²€ìƒ‰ ë¬¸ì„œ ì¸ìš© ì—¬ë¶€")
    is_relevant: bool = Field(description="ë‹µë³€ ì í•©ì„±")
    needs_more_search: bool = Field(description="ì¶”ê°€ ê²€ìƒ‰ í•„ìš” ì—¬ë¶€")
    quality_score: int = Field(description="í’ˆì§ˆ ì ìˆ˜ (1-5)")
    improvement_suggestion: str = Field(default="", description="ê°œì„  ì œì•ˆ")


# ============================================================
# [SECTION 6] Infrastructure Layer - ì™¸ë¶€ ë¦¬ì†ŒìŠ¤ ì—°ê²°
# ============================================================
class VectorStoreManager:
    """Qdrant ë²¡í„°ìŠ¤í† ì–´ ê´€ë¦¬"""

    def __init__(self, config: Config):
        self.config = config
        self._load_env()
        self.embeddings = None
        self.vectorstore = None
        self.client = None

    def _load_env(self):
        """í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ"""
        self.collection_name = os.getenv("QDRANT_COLLECTION_NAME")
        self.qdrant_url = os.getenv("QDRANT_URL")
        self.qdrant_api_key = os.getenv("QDRANT_API_KEY")

        if not self.qdrant_api_key:
            raise ValueError("QDRANT_API_KEYê°€ .envì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")

    def initialize(self) -> QdrantVectorStore:
        """ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™”"""
        logger.info(f"Loading embedding model: {self.config.EMBEDDING_MODEL}")
        self.embeddings = HuggingFaceEmbeddings(
            model_name=self.config.EMBEDDING_MODEL,
            model_kwargs={'trust_remote_code': True},
            encode_kwargs={'normalize_embeddings': True}
        )
        logger.info("Embedding model loaded")

        logger.info("Connecting to Qdrant...")
        warnings.filterwarnings(
            'ignore', message='Api key is used with an insecure connection')

        self.client = QdrantClient(
            url=self.qdrant_url,
            api_key=self.qdrant_api_key,
            timeout=self.config.QDRANT_TIMEOUT,
            prefer_grpc=False
        )
        logger.info("Qdrant connected")

        logger.info("Initializing vectorstore...")
        self.vectorstore = QdrantVectorStore(
            client=self.client,
            collection_name=self.collection_name,
            embedding=self.embeddings,
            content_payload_key="text"
        )
        logger.info("Vectorstore initialized")

        return self.vectorstore

    def get_client(self) -> QdrantClient:
        return self.client

    def get_collection_name(self) -> str:
        return self.collection_name


class BM25Manager:
    """BM25 Retriever ê´€ë¦¬"""

    def __init__(self, config: Config, client: QdrantClient, collection_name: str):
        self.config = config
        self.client = client
        self.collection_name = collection_name
        self.retriever = None

    def initialize(self) -> Optional[BM25Retriever]:
        """BM25 Retriever ì´ˆê¸°í™”"""
        logger.info("Initializing BM25 Retriever...")

        try:
            collection_info = self.client.get_collection(self.collection_name)
            total_points = collection_info.points_count
            logger.info(f"Collection contains {total_points} documents")

            sample_size = min(self.config.BM25_SAMPLE_SIZE, total_points)
            scroll_result = self.client.scroll(
                collection_name=self.collection_name,
                limit=sample_size,
                with_payload=True,
                with_vectors=False
            )

            bm25_docs = []
            for point in scroll_result[0]:
                payload = point.payload
                text = payload.get("text", "")
                if text:
                    doc = Document(
                        page_content=text,
                        metadata={k: v for k, v in payload.items()
                                  if k != "text"}
                    )
                    bm25_docs.append(doc)

            if bm25_docs:
                self.retriever = BM25Retriever.from_documents(
                    bm25_docs, k=self.config.TOP_K_BM25)
                logger.info(
                    f"BM25 Retriever initialized ({len(bm25_docs)} docs)")
                return self.retriever
            else:
                logger.warning("No documents for BM25. Vector Search only.")
                return None

        except Exception as e:
            logger.error(f"BM25 init failed: {e}")
            return None


# ============================================================
# [SECTION 7] Logic Layer - LangGraph ë…¸ë“œ ë° ì›Œí¬í”Œë¡œìš° êµ¬ì„±
# ============================================================
class LegalRAGBuilder:
    """ë²•ë¥  RAG ê·¸ë˜í”„ ë¹Œë”"""

    def __init__(self, config: Config):
        self.config = config
        self.llm = None
        self.vectorstore = None
        self.bm25_retriever = None
        self.query_expander = None
        self.reranker = None

    def _init_infrastructure(self):
        """ì¸í”„ë¼ ì´ˆê¸°í™”"""
        # Vector Store
        vs_manager = VectorStoreManager(self.config)
        self.vectorstore = vs_manager.initialize()

        # BM25
        bm25_manager = BM25Manager(
            self.config,
            vs_manager.get_client(),
            vs_manager.get_collection_name()
        )
        self.bm25_retriever = bm25_manager.initialize()

        # LLM
        logger.info(f"Initializing LLM: {self.config.LLM_MODEL}")
        self.llm = ChatOpenAI(
            model=self.config.LLM_MODEL,
            temperature=self.config.LLM_TEMPERATURE,
            streaming=True
        )

        # Query Expander
        self.query_expander = self._create_query_expander()

        # Reranker
        self.reranker = JinaReranker(
            model_name=self.config.RERANKER_MODEL,
            top_n=self.config.TOP_K_RERANK
        )

    def _create_query_expander(self):
        """Query Expander ìƒì„± [ì‚¬ìš© í”„ë¡¬í”„íŠ¸: PROMPT_QUERY_EXPANSION]"""
        structured_llm = self.llm.with_structured_output(ExpandedQuery)

        expansion_prompt = ChatPromptTemplate.from_messages([
            ("system", self.config.PROMPT_QUERY_EXPANSION),
            ("human", "{query}")
        ])

        def expand_query(query: str) -> ExpandedQuery:
            try:
                chain = expansion_prompt | structured_llm
                return chain.invoke({"query": query})
            except Exception:
                return ExpandedQuery(
                    original_query=query, search_keywords=[],
                    legal_terms=[], synonyms=[], expanded_query=query
                )

        return expand_query

    # --- Nodes ---

    def _create_analyze_node(self):
        """[ë…¸ë“œ: Analyze] ì§ˆë¬¸ ë¶„ì„ ë…¸ë“œ [ì‚¬ìš© í”„ë¡¬í”„íŠ¸: PROMPT_ANALYZE]"""
        structured_llm = self.llm.with_structured_output(QueryAnalysis)

        analyze_prompt = ChatPromptTemplate.from_messages([
            ("system", self.config.PROMPT_ANALYZE),
            ("human", "{query}")
        ])

        def analyze_query(state: AgentState) -> AgentState:
            query = state["user_query"]
            logger.info(f"Analyzing query: {query[:50]}...")

            chain = analyze_prompt | structured_llm
            analysis: QueryAnalysis = chain.invoke({"query": query})

            logger.info(
                f"Analysis: category={analysis.category}, intent={analysis.intent_type}")

            return {"query_analysis": analysis.model_dump()}

        return analyze_query

    def _create_clarify_node(self):
        """[ë…¸ë“œ: Clarify] ëª…í™•í™” ìš”ì²­ ë…¸ë“œ [ì‚¬ìš© í…œí”Œë¦¿: TEMPLATE_CLARIFY]"""
        template = self.config.TEMPLATE_CLARIFY

        def request_clarification(state: AgentState) -> AgentState:
            analysis = state.get("query_analysis", {})
            clarification_q = analysis.get(
                "clarification_question", "ì§ˆë¬¸ì„ ì¢€ ë” êµ¬ì²´ì ìœ¼ë¡œ í•´ì£¼ì‹œê² ì–´ìš”?")

            answer = template.format(clarification_question=clarification_q)
            return {"generated_answer": answer, "next_action": "end"}

        return request_clarification

    def _create_search_node(self):
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë…¸ë“œ"""
        vectorstore = self.vectorstore
        bm25_retriever = self.bm25_retriever
        query_expander = self.query_expander
        reranker = self.reranker
        config = self.config

        def search_documents(state: AgentState) -> AgentState:
            original_query = state["user_query"]
            analysis = state.get("query_analysis", {})
            related_laws = analysis.get("related_laws", [])

            # Query Expansion
            if query_expander:
                try:
                    expanded = query_expander(original_query)
                    search_query = expanded.expanded_query
                    logger.info(f"Expanded query: {search_query[:60]}...")
                except Exception:
                    search_query = original_query
            else:
                search_query = original_query

            all_docs = []

            # 1. Vector Search
            try:
                vector_results = vectorstore.similarity_search_with_score(
                    search_query, k=config.TOP_K_VECTOR)
                vector_docs = [doc for doc, _ in vector_results]
                for doc in vector_docs:
                    doc.metadata["search_source"] = "vector"
                all_docs.extend(vector_docs)
                logger.info(f"Vector search: {len(vector_docs)} docs")
            except Exception as e:
                logger.error(f"Vector search error: {e}")

            # 2. BM25 Search
            if bm25_retriever:
                try:
                    bm25_docs = bm25_retriever.invoke(search_query)
                    for doc in bm25_docs:
                        doc.metadata["search_source"] = "bm25"
                    all_docs.extend(bm25_docs)
                    logger.info(f"BM25 search: {len(bm25_docs)} docs")
                except Exception as e:
                    logger.error(f"BM25 search error: {e}")

            # 3. Deduplicate
            seen = set()
            unique_docs = []
            for doc in all_docs:
                h = hash(doc.page_content[:200])
                if h not in seen:
                    seen.add(h)
                    unique_docs.append(doc)

            logger.info(f"After dedup: {len(unique_docs)} docs")

            if not unique_docs:
                return {"retrieved_docs": []}

            # 4. Rerank
            try:
                reranked_docs = reranker.compress_documents(
                    unique_docs, original_query)

                # 5. Boost related laws
                if related_laws:
                    for doc in reranked_docs:
                        law_name = doc.metadata.get('law_name', '')
                        for rel_law in related_laws:
                            if rel_law in law_name:
                                score = doc.metadata.get('relevance_score', 0)
                                doc.metadata['relevance_score'] = min(
                                    1.0, score + 0.1)
                                doc.metadata['boosted'] = True
                                break

                # 6. Filter by threshold
                filtered_docs = [
                    doc for doc in reranked_docs
                    if doc.metadata.get('relevance_score', 0) >= config.RELEVANCE_THRESHOLD
                ]

                logger.info(f"After rerank/filter: {len(filtered_docs)} docs")

                return {"retrieved_docs": filtered_docs[:config.TOP_K_FINAL]}

            except Exception as e:
                logger.error(f"Rerank error: {e}")
                return {"retrieved_docs": unique_docs[:config.TOP_K_FINAL]}

        return search_documents

    def _create_generate_node(self):
        """[ë…¸ë“œ: Generate] ë‹µë³€ ìƒì„± ë…¸ë“œ [ì‚¬ìš© í”„ë¡¬í”„íŠ¸: PROMPT_GENERATE, TEMPLATE_NO_RESULTS]"""
        llm = self.llm
        system_prompt = self.config.PROMPT_GENERATE
        no_results_template = self.config.TEMPLATE_NO_RESULTS

        answer_prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", """ì‚¬ìš©ì ì§ˆë¬¸: {query}

ğŸ“š ê²€ìƒ‰ëœ ë²•ë ¹/ë¬¸ì„œ:
{context}

{case_law_notice}

ìœ„ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.""")
        ])

        def generate_answer(state: AgentState) -> AgentState:
            query = state["user_query"]
            docs = state.get("retrieved_docs", [])
            analysis = state.get("query_analysis", {})
            needs_case_law = analysis.get("needs_case_law", False)

            logger.info("Generating answer...")

            # Format context
            if docs:
                context_parts = []
                for i, doc in enumerate(docs, 1):
                    meta = doc.metadata
                    law_name = meta.get("law_name", "")
                    article = meta.get("article_no", "")
                    title = meta.get(
                        "article_title", "") or meta.get("title", "")
                    content = doc.page_content[:800]

                    header = f"[ë¬¸ì„œ {i}]"
                    if law_name:
                        header += f" {law_name}"
                        if article:
                            header += f" ì œ{article}ì¡°"
                    if title:
                        header += f" - {title}"

                    context_parts.append(f"{header}\n{content}\n")

                context = "\n".join(context_parts)
            else:
                context = "(ê´€ë ¨ ë²•ë ¹ ë¬¸ì„œê°€ ê²€ìƒ‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤)"

            case_law_notice = ""
            if needs_case_law:
                case_law_notice = "âš ï¸ ì°¸ê³ : íŒë¡€ ê²€ìƒ‰ì´ í•„ìš”í•˜ë‚˜ í˜„ì¬ DBì— í¬í•¨ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤."

            if not docs:
                answer = no_results_template
            else:
                chain = answer_prompt | llm
                response = chain.invoke({
                    "query": query,
                    "context": context,
                    "case_law_notice": case_law_notice
                })
                answer = response.content

            logger.info("Answer generated")
            return {"generated_answer": answer}

        return generate_answer

    def _create_evaluate_node(self):
        """[ë…¸ë“œ: Evaluate] ë‹µë³€ í‰ê°€ ë…¸ë“œ [ì‚¬ìš© í”„ë¡¬í”„íŠ¸: PROMPT_EVALUATE]"""
        structured_llm = self.llm.with_structured_output(AnswerEvaluation)

        evaluate_prompt = ChatPromptTemplate.from_messages([
            ("system", self.config.PROMPT_EVALUATE),
            ("human", """## ì§ˆë¬¸
{query}

## ê²€ìƒ‰ëœ ë¬¸ì„œ ìš”ì•½
{context_summary}

## ìƒì„±ëœ ë‹µë³€
{answer}

í‰ê°€í•´ì£¼ì„¸ìš”.""")
        ])

        def evaluate_answer(state: AgentState) -> AgentState:
            query = state["user_query"]
            answer = state.get("generated_answer", "")
            docs = state.get("retrieved_docs", [])
            retry_count = state.get("retry_count", 0) or 0

            logger.info(f"Evaluating answer (attempt {retry_count + 1})")

            if docs:
                context_summary = "\n".join([
                    f"- {doc.metadata.get('law_name', 'ë¬¸ì„œ')}: {doc.page_content[:100]}..."
                    for doc in docs[:5]
                ])
            else:
                context_summary = "(ê²€ìƒ‰ëœ ë¬¸ì„œ ì—†ìŒ)"

            chain = evaluate_prompt | structured_llm
            evaluation: AnswerEvaluation = chain.invoke({
                "query": query,
                "context_summary": context_summary,
                "answer": answer
            })

            logger.info(
                f"Evaluation: score={evaluation.quality_score}, needs_more={evaluation.needs_more_search}")

            return {
                "evaluation_result": evaluation.model_dump(),
                "retry_count": retry_count + 1
            }

        return evaluate_answer

    # --- Routing ---

    def _route_after_analysis(self, state: AgentState) -> Literal["clarify", "search"]:
        analysis = state.get("query_analysis", {})
        if analysis.get("needs_clarification", False):
            return "clarify"
        return "search"

    def _route_after_evaluation(self, state: AgentState) -> Literal["search", "end"]:
        evaluation = state.get("evaluation_result", {})
        retry_count = state.get("retry_count", 0) or 0

        if retry_count >= self.config.MAX_RETRY:
            logger.warning("Max retry reached")
            return "end"

        if evaluation.get("needs_more_search", False) and evaluation.get("quality_score", 3) <= 2:
            logger.info("Retrying search...")
            return "search"

        return "end"

    # --- Build Graph ---

    def build(self) -> StateGraph:
        """ê·¸ë˜í”„ ë¹Œë“œ"""
        logger.info("Building Legal RAG Graph...")

        # Infrastructure
        self._init_infrastructure()

        # Create nodes
        analyze_node = self._create_analyze_node()
        clarify_node = self._create_clarify_node()
        search_node = self._create_search_node()
        generate_node = self._create_generate_node()
        evaluate_node = self._create_evaluate_node()

        # Build workflow
        workflow = StateGraph(AgentState)

        workflow.add_node("analyze", analyze_node)
        workflow.add_node("clarify", clarify_node)
        workflow.add_node("search", search_node)
        workflow.add_node("generate", generate_node)
        workflow.add_node("evaluate", evaluate_node)

        workflow.set_entry_point("analyze")

        workflow.add_conditional_edges(
            "analyze",
            self._route_after_analysis,
            {"clarify": "clarify", "search": "search"}
        )

        workflow.add_edge("clarify", END)
        workflow.add_edge("search", "generate")
        workflow.add_edge("generate", "evaluate")

        workflow.add_conditional_edges(
            "evaluate",
            self._route_after_evaluation,
            {"search": "search", "end": END}
        )

        graph = workflow.compile()
        logger.info("Graph built successfully")

        return graph


# ============================================================
# [SECTION 8] Execution Layer - ì§„ì…ì  ë° ì‹¤í–‰ ë¡œì§
# ============================================================

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
_DOTENV_PATH = Path(__file__).with_name(".env")
load_dotenv(dotenv_path=_DOTENV_PATH)

# ì „ì—­ Config
config = Config()


def initialize_rag_chatbot():
    """í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ìš© ì´ˆê¸°í™” í•¨ìˆ˜"""
    builder = LegalRAGBuilder(config)
    return builder.build()


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    if not os.getenv("OPENAI_API_KEY"):
        logger.error("OPENAI_API_KEY is not set")
        return

    print("\n" + "=" * 60)
    print("ğŸš€ A-TEAM ë²•ë¥  RAG ì±—ë´‡ (LangGraph V8) ì‹œì‘")
    print("=" * 60 + "\n")

    try:
        graph = initialize_rag_chatbot()

        print("\nâœ… ì±—ë´‡ ì¤€ë¹„ ì™„ë£Œ!")
        print("ğŸ’¡ 'exit' ë˜ëŠ” 'ì¢…ë£Œ'ë¥¼ ì…ë ¥í•˜ë©´ ì¢…ë£Œë©ë‹ˆë‹¤\n")

        while True:
            try:
                user_input = input("ğŸ‘¤ User >> ").strip()

                if user_input.lower() in ["exit", "quit", "ì¢…ë£Œ", "q"]:
                    print("\nğŸ‘‹ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break

                if not user_input:
                    print("âŒ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.\n")
                    continue

                initial_state = {
                    "messages": [HumanMessage(content=user_input)],
                    "user_query": user_input,
                    "query_analysis": None,
                    "retrieved_docs": None,
                    "generated_answer": None,
                    "next_action": None,
                    "evaluation_result": None,
                    "retry_count": 0
                }

                result = graph.invoke(initial_state)

                answer = result.get("generated_answer", "")
                if answer:
                    print("\n" + "=" * 60)
                    print("ğŸ¤– AI ë‹µë³€:")
                    print("=" * 60)
                    print(f"\n{answer}\n")
                    print("=" * 60 + "\n")
                else:
                    print("\nâš ï¸ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n")

            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                logger.error(f"Error: {e}")
                print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}\n")

    except Exception as e:
        logger.error(f"Initialization failed: {e}")
        raise


if __name__ == "__main__":
    main()
