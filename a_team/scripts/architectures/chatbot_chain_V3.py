################################################
# A-TEAM ë²•ë¥  RAG ì±—ë´‡ (LangChain V3)
  # ë²¡í„° ê²€ìƒ‰ + BM25 í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
  # Jina Reranker ê¸°ë°˜ ë¬¸ì„œ ë¦¬ë­í‚¹
  # LangGraph ì œê±° -> ìˆœìˆ˜ LangChain ë° ì ˆì°¨ì  ë¡œì§ìœ¼ë¡œ ë³€ê²½
# ì‘ì„±ì ì •ë³´
  # ì‘ì„±ì: SKN 3-1íŒ€ A-TEAM
  # ì‘ì„±ì¼: 2026-01-08
################################################

import os
import re
import warnings
from pathlib import Path
from typing import Optional, List, Any, Sequence
from dotenv import load_dotenv

from qdrant_client import QdrantClient
from langchain_qdrant import QdrantVectorStore
from a_team.scripts.bm25_search import BM25KeywordRetriever
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.documents import Document, BaseDocumentCompressor
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

_DOTENV_PATH = Path(__file__).with_name(".env")
load_dotenv(dotenv_path=_DOTENV_PATH)


class JinaReranker(BaseDocumentCompressor):
    model_name: str = "jinaai/jina-reranker-v2-base-multilingual"
    top_n: int = 6
    model: Any = None
    tokenizer: Any = None

    class Config:
        arbitrary_types_allowed = True
        extra = "allow"

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)
        self.model = AutoModelForSequenceClassification.from_pretrained(
            self.model_name, trust_remote_code=True, dtype="auto"
        )
        self.model.eval()

    def compress_documents(self, documents: Sequence[Document], query: str, callbacks: Optional[Any] = None) -> Sequence[Document]:
        if not documents:
            return []

        pairs = [[query, doc.page_content] for doc in documents]
        with torch.no_grad():
            inputs = self.tokenizer(pairs, padding=True, truncation=True, return_tensors="pt", max_length=512)
            scores = self.model(**inputs).logits.squeeze(-1).float().cpu()
            scores = torch.sigmoid(scores).tolist()
            if not isinstance(scores, list):
                scores = [scores]

        top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[: self.top_n]
        final_docs = []
        for i in top_indices:
            doc = documents[i]
            doc.metadata["relevance_score"] = scores[i]
            final_docs.append(doc)
        return final_docs


# ----------------------------
# ê²€ìƒ‰ ê´€ë ¨ í—¬í¼
# ----------------------------
def expand_queries(query: str) -> List[str]:
    variants = {query.strip()}
    # ì¡°ì‚¬/ë¶ˆìš©ì–´ ì¼ë¶€ ì œê±° ì‹œë„
    compact = re.sub(r"[\s]+", " ", query).strip()
    variants.add(compact)
    # ê´„í˜¸/ìŠ¬ë˜ì‹œ ì œê±° ë²„ì „
    variants.add(re.sub(r"[()\[\]/]", " ", compact))
    return [v for v in variants if v]


def dedup_documents(docs: List[Document]) -> List[Document]:
    seen = set()
    unique = []
    for doc in docs:
        key = doc.metadata.get("id") or (doc.metadata.get("source"), doc.metadata.get("law_name"), doc.metadata.get("article_no"), doc.page_content[:80])
        if key in seen:
            continue
        seen.add(key)
        unique.append(doc)
    return unique


def format_context_snippets(docs: List[Document], max_docs: int = 5, max_chars: int = 500) -> str:
    parts = []
    for i, doc in enumerate(docs[:max_docs], 1):
        meta = doc.metadata or {}
        law_name = meta.get("law_name", "")
        article = meta.get("article_no", "")
        title = meta.get("article_title") or meta.get("title", "")
        source = meta.get("source", "")
        snippet = doc.page_content[: max_chars].strip()
        header = f"[ê·¼ê±° {i}]"
        if law_name:
            header += f" {law_name}"
            if article:
                header += f" ì œ{article}ì¡°"
        if title:
            header += f" - {title}"
        if source and not law_name:
            header += f" ({source})"
        parts.append(f"{header}\n{snippet}\n")
    return "\n".join(parts) if parts else "(ê´€ë ¨ ë²•ë ¹ ë¬¸ì„œê°€ ê²€ìƒ‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤)"


def retrieve_documents(query: str, vectorstore: QdrantVectorStore, bm25_retriever: Optional[BM25KeywordRetriever]) -> List[Document]:
    print(f"ğŸ” [ë²•ë ¹ ê²€ìƒ‰] ì¿¼ë¦¬: {query[:50]}...")

    variants = expand_queries(query)
    all_docs: List[Document] = []
    vector_scores = []
    
    # 1. ë²¡í„° ê²€ìƒ‰ (cosine similarity)
    for q in variants:
        try:
            res = vectorstore.similarity_search_with_score(q, k=10)
            all_docs.extend([doc for doc, score in res])
            vector_scores.extend([score for doc, score in res])
        except Exception as e:
            print(f"âš ï¸  [ë²¡í„° ê²€ìƒ‰ ì˜¤ë¥˜] {e}")

    # ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆê³ , ëª¨ë“  scoreê°€ 0.5 ì´í•˜ë¼ë©´ ì¿¼ë¦¬ ë³€í˜• í›„ ì¬ê²€ìƒ‰ ì‹œë„
    if vector_scores and all(s <= 0.5 for s in vector_scores):
        print("âš ï¸  [ë²¡í„° ìœ ì‚¬ë„ 0.5 ì´í•˜, ì¿¼ë¦¬ ë³€í˜• í›„ ì¬ê²€ìƒ‰]")
        import re
        keywords = re.findall(r"[\wê°€-í£]+", query)
        simple_query = " ".join(keywords)
        retry_variants = expand_queries(simple_query)
        all_docs = []
        vector_scores = []
        for q in retry_variants:
            try:
                res = vectorstore.similarity_search_with_score(q, k=10)
                all_docs.extend([doc for doc, score in res])
                vector_scores.extend([score for doc, score in res])
            except Exception as e:
                print(f"âš ï¸  [ë²¡í„° ì¬ê²€ìƒ‰ ì˜¤ë¥˜] {e}")
        
        # BM25ë„ ì¬ê²€ìƒ‰
        if bm25_retriever:
            for q in retry_variants:
                try:
                    bm25_docs = bm25_retriever.search(q, k=5)
                    all_docs.extend(bm25_docs)
                except Exception as e:
                    print(f"âš ï¸  [BM25 ì¬ê²€ìƒ‰ ì˜¤ë¥˜] {e}")
    else:
        # 2. BM25/keyword ê²€ìƒ‰ (í•˜ì´ë¸Œë¦¬ë“œ)
        if bm25_retriever:
            for q in variants:
                try:
                    bm25_docs = bm25_retriever.search(q, k=5)
                    all_docs.extend(bm25_docs)
                except Exception as e:
                    print(f"âš ï¸  [BM25 ê²€ìƒ‰ ì˜¤ë¥˜] {e}")

    all_docs = dedup_documents(all_docs)
    if not all_docs:
        print("âš ï¸  [ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ]")
        return []

    try:
        reranker = JinaReranker(top_n=6)
        reranked = reranker.compress_documents(all_docs, query)
        if reranked:
            docs = reranked[:6]
            print(f"âœ… [ë¦¬ë­í‚¹ ì™„ë£Œ] {len(docs)}ê°œ ë¬¸ì„œ ì„ ë³„")
        else:
            docs = all_docs[:6]
            print("âš ï¸  [ë¦¬ë­í‚¹ ê²°ê³¼ ì—†ìŒ] ì›ë³¸ ìƒìœ„ 6ê°œ ì‚¬ìš©")
    except Exception as e:
        print(f"âš ï¸  [ë¦¬ë­í‚¹ ì˜¤ë¥˜] {e}")
        docs = all_docs[:6]

    for i, d in enumerate(docs, 1):
        print(f"   [{i}] score={d.metadata.get('relevance_score', 0):.4f} | {d.page_content[:40]}...")

    return docs


def generate_answer(query: str, docs: List[Document], llm: ChatOpenAI) -> str:
    print("ğŸ’¬ [ë‹µë³€ ìƒì„± ì¤‘...]")

    context = format_context_snippets(docs, max_docs=5, max_chars=500)
    
    answer_prompt = ChatPromptTemplate.from_messages([
        (
            "system",
            """ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë…¸ë™ ë¶„ì•¼ ë²•ë¥ , í˜•ì‚¬ë²• ë²•ë¥ , ë¯¼ì‚¬ë²• ë²•ë¥ ì— ëŒ€í•´ ì „ë¬¸ì ìœ¼ë¡œ í•™ìŠµëœ AI ë„ìš°ë¯¸ì…ë‹ˆë‹¤.
            ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì €ì¥ëœ ë²•ë¥  ì¡°í•­ ë°ì´í„°ì™€ ê´€ë ¨ ì •ë³´(íŒë¡€, í–‰ì •í•´ì„ ë“±)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê³  ì‹ ë¢°ì„± ìˆëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
            1. ë‹µë³€ ì‘ì„± ê¸°ë³¸ ì§€ì¹¨ 
                - ë²•ë¥  ì¡°í•­ì— ê´€í•œ ì§ˆë¬¸ì´ë¼ë©´ ê·¸ ì¡°í•­ì— ê´€í•œ ì „ì²´ ë‚´ìš©ì„ ê°€ì ¸ì˜¨ë‹¤.
                - ì˜ˆë¥¼ë“¤ì–´ 'ê·¼ë¡œê¸°ì¤€ë²• ì œ1ì¡°ì˜ ë‚´ìš©'ì´ë¼ëŠ” ì§ˆë¬¸ì„ ë°›ìœ¼ë©´ ê·¼ë¡œê¸°ì¤€ë²• ì œ1ì¡°ì˜ ì¡°í•­ì„ ì „ë¶€ ë‹¤ ë‹µë³€í•œë‹¤.
                - ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¼ ê´€ë ¨ ì •ë³´ë¥¼ êµ¬ì¡°ì ìœ¼ë¡œ ì‘ì„±í•˜ë©°, ì¤‘ìš” ì„¸ë²• ì¡°ë¬¸ê³¼ ìš”ì•½ëœ ë‚´ìš©ì„ í¬í•¨í•©ë‹ˆë‹¤.
                - ë¹„ì „ë¬¸ê°€ë„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ìš©ì–´ë¥¼ ì¹œì ˆíˆ ì„¤ëª…í•©ë‹ˆë‹¤.
            2. ë‹µë³€ ì‘ì„± ì„¸ë¶€ ì§€ì¹¨:
                - **ê°„ê²°ì„±**: ë‹µë³€ì€ ê°„ë‹¨í•˜ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•˜ë˜, ë²• ì¡°í•­ì— ê´€í•œ ì§ˆë¬¸ì¼ ê²½ìš° ê´€ë ¨ ë²• ì¡°ë¬¸ì˜ ì „ë¬¸ì„ ëª…ì‹œí•©ë‹ˆë‹¤.
                - **êµ¬ì¡°í™”ëœ ì •ë³´ ì œê³µ**:
                    - ì„¸ë²• ì¡°í•­ ë²ˆí˜¸, ì„¸ë²• ì¡°í•­ì˜ ì •ì˜, ì‹œí–‰ë ¹, ê´€ë ¨ ê·œì •ì„ êµ¬ì²´ì ìœ¼ë¡œ ëª…ì‹œí•©ë‹ˆë‹¤.
                    - ë³µì¡í•œ ê°œë…ì€ ì˜ˆì‹œë¥¼ ë“¤ì–´ ì„¤ëª…í•˜ê±°ë‚˜, ë‹¨ê³„ì ìœ¼ë¡œ ì•ˆë‚´í•©ë‹ˆë‹¤.
                - **ì‹ ë¢°ì„± ê°•ì¡°**:
                    - ë‹µë³€ì´ ë²•ì  ì¡°ì–¸ì´ ì•„ë‹ˆë¼ ì •ë³´ ì œê³µ ëª©ì ì„ì„ ëª…í™•íˆ ì•Œë¦½ë‹ˆë‹¤.
                    - "ì´ ë‹µë³€ì€ ì„¸ë²• ê´€ë ¨ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìœ¼ë©°, êµ¬ì²´ì ì¸ ìƒí™©ì— ë”°ë¼ ì „ë¬¸ê°€ì˜ ì¶”ê°€ ì¡°ì–¸ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
                - **ì •í™•ì„±**:
                    - ë²•ë ¹ ë° ë²•ë¥ ì— ê´€í•œì§ˆë¬¸ì€ ì¶”ê°€ì ì¸ ë‚´ìš©ì—†ì´ í•œê°€ì§€ contentì— ì§‘ì¤‘í•˜ì—¬ ë‹µë³€í•œë‹¤.
                    - ì¡°í•­ì— ëŒ€í•œ ì§ˆë¬¸ì€ ì‹œí–‰ë ¹ì´ë‚˜ ì‹œí–‰ê·œì¹™ë³´ë‹¨ í•´ë‹¹ë²•ì—ì„œ ê°€ì ¸ì˜¤ëŠ”ê²ƒì— ì§‘ì¤‘í•œë‹¤.
            3. ì¶”ê°€ì ì¸ ì‚¬ìš©ì ì§€ì›:
                - ë‹µë³€ í›„ ì‚¬ìš©ìì—ê²Œ ì£¼ì œì™€ ê´€ë ¨ëœ í›„ì† ì§ˆë¬¸ ë‘ ê°€ì§€ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.
                - í›„ì† ì§ˆë¬¸ì€ ì‚¬ìš©ìê°€ ë” ê¹Šì´ íƒêµ¬í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„í•˜ë©°, ê° ì§ˆë¬¸ ì•ë’¤ì— í•œ ì¤„ì”© ë„ì–´ì“°ê¸°ë¥¼ í•©ë‹ˆë‹¤.

            4. ì˜ˆì™¸ ìƒí™© ì²˜ë¦¬:
                - ì‚¬ìš©ìê°€ ì§ˆë¬¸ì„ ëª¨í˜¸í•˜ê²Œ ì‘ì„±í•œ ê²½ìš°:
                    - "ì§ˆë¬¸ì´ ëª…í™•í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ ë¶€ë¶„ì„ ì•Œê³  ì‹¶ìœ¼ì‹ ì§€ ë§ì”€í•´ ì£¼ì‹œê² ì–´ìš”?"ì™€ ê°™ì€ ë¬¸êµ¬ë¡œ ì¶”ê°€ ì •ë³´ë¥¼ ìš”ì²­í•©ë‹ˆë‹¤.
                - ì§ˆë¬¸ì´ ì•Œê³  ìˆëŠ” ë²•ë¥ (ë…¸ë™ ë¶„ì•¼ ë²•ë¥ , í˜•ì‚¬ë²•, ë¯¼ì‚¬ë²•)ê³¼ ì§ì ‘ ê´€ë ¨ì´ ì—†ëŠ” ê²½ìš°:
                    - "ì´ ì§ˆë¬¸ì€ ì œê°€ í•™ìŠµí•œ ë²•ë¥  ë²”ìœ„ë¥¼ ë²—ì–´ë‚©ë‹ˆë‹¤."ë¼ê³  ì•Œë¦¬ê³ , ì•Œê³  ìˆëŠ” ë²•ë¥ (ë…¸ë™ ë¶„ì•¼ ë²•ë¥ , í˜•ì‚¬ë²•, ë¯¼ì‚¬ë²•)ê³¼ ê´€ë ¨ëœ ìƒˆë¡œìš´ ì§ˆë¬¸ì„ ìœ ë„í•©ë‹ˆë‹¤.

            5. ì¶”ê°€ ì§€ì¹¨:
                - ê°œí–‰ë¬¸ì ë‘ ê°œ ì´ìƒì€ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
                - ì§ˆë¬¸ ë° ë‹µë³€ì—ì„œ ì‚¬ìš©ëœ ì„¸ë²• ì¡°ë¬¸ì€ ìµœì‹  ë°ì´í„°ì— ê¸°ë°˜í•´ì•¼ í•©ë‹ˆë‹¤.
                - ì§ˆë¬¸ì´ ë³µí•©ì ì¸ ê²½ìš°, ê° í•˜ìœ„ ì§ˆë¬¸ì— ëŒ€í•´ ë³„ë„ë¡œ ë‹µë³€í•˜ê±°ë‚˜, ì‚¬ìš©ìì—ê²Œ ìš°ì„ ìˆœìœ„ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.

            6. ì˜ˆì‹œ ë‹µë³€ í…œí”Œë¦¿:
                - "ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€: ..."
                - "ê´€ë ¨ ì„¸ë²• ì¡°í•­: ..."
                - "ì¶”ê°€ ì„¤ëª…: ..."
                - ìœ„ëŠ” "ì˜ˆì‹œ" í…œí”Œë¦¿ìœ¼ë¡œ, ì˜ˆì • ë‹µë³€ì´ í…œí”Œë¦¿ê³¼ ì¼ì¹˜í•˜ì§€ ì•Šì„ ê²½ìš° ìˆ˜ì • ê°€ëŠ¥í•©ë‹ˆë‹¤."""
        ),
        ("human", "ì§ˆë¬¸: {query}\n\n[ê´€ë ¨ ë²•ë ¹ ë° ê·¼ê±° ìë£Œ]\n{context}")
    ])

    if not docs:
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ê·¼ê±°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•˜ê±°ë‚˜ ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”. ë³µì¡í•œ ì‚¬ì•ˆì´ë©´ ì „ë¬¸ ë²•ë¥  ìƒë‹´ì„ ê¶Œì¥ë“œë¦½ë‹ˆë‹¤."

    chain = answer_prompt | llm
    response = chain.invoke({"query": query, "context": context})
    answer = response.content

    print("âœ… [ë‹µë³€ ìƒì„± ì™„ë£Œ]")
    return answer


def initialize_resources():
    COLLECTION_NAME = os.getenv("QDRANT_COLLECTION_NAME")
    QDRANT_URL = os.getenv("QDRANT_URL")
    QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")
    if not QDRANT_API_KEY:
        raise ValueError("QDRANT_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
    
    print("ğŸ”§ ì„¤ì • ë¡œë“œ ì™„ë£Œ")
    print("\nğŸš€ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘ (Qwen/Qwen3-Embedding-0.6B)...")
    embeddings = HuggingFaceEmbeddings(
        model_name="Qwen/Qwen3-Embedding-0.6B",
        model_kwargs={"trust_remote_code": True},
        encode_kwargs={"normalize_embeddings": True},
    )
    print("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

    print("\nğŸ“¡ Qdrant ì—°ê²° ì¤‘...")
    warnings.filterwarnings("ignore", message="Api key is used with an insecure connection")
    client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY, timeout=30, prefer_grpc=False)
    print("âœ… Qdrant ì—°ê²° ì™„ë£Œ")

    print("\nğŸ—‚ï¸  ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” ì¤‘...")
    vectorstore = QdrantVectorStore(
        client=client,
        collection_name=COLLECTION_NAME,
        embedding=embeddings,
        content_payload_key="text",
    )
    print("âœ… ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” ì™„ë£Œ")
    
    # BM25 ì´ˆê¸°í™”
    BM25_INDEX_DIR = os.getenv("BM25_INDEX_DIR", "whoosh_index")
    try:
        bm25_retriever = BM25KeywordRetriever(index_dir=BM25_INDEX_DIR)
        print(f"âœ… BM25/keyword ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: {BM25_INDEX_DIR}")
    except Exception as e:
        print(f"âš ï¸  BM25 ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
        bm25_retriever = None

    return {"vectorstore": vectorstore, "bm25_retriever": bm25_retriever}


def main():
    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ ì˜¤ë¥˜: OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return

    try:
        print("\n" + "=" * 60)
        print("ğŸš€ A-TEAM ë²•ë¥  RAG ì±—ë´‡ (LangChain V3) ì´ˆê¸°í™”")
        print("=" * 60 + "\n")

        resources = initialize_resources()
        vectorstore = resources["vectorstore"]
        bm25_retriever = resources["bm25_retriever"]
        
        print("\nğŸ¤– LLM ì„¤ì • ì¤‘...")
        llm = ChatOpenAI(model="gpt-4.1", temperature=0, streaming=True)
        print("âœ… LLM ì„¤ì • ì™„ë£Œ")

        print("\n" + "=" * 60)
        print("âœ… ğŸ¤– A-TEAM ë²•ë¥  ì±—ë´‡ ì¤€ë¹„ ì™„ë£Œ (V3)")
        print("=" * 60)
        print("\nì‚¬ìš© ë°©ë²•: ë…¸ë™ë²•/í˜•ì‚¬ë²•/ë¯¼ì‚¬ë²• ì§ˆë¬¸ì— ë‹µë³€, ëª¨í˜¸í•˜ë©´ ëª…í™•í™” ìš”ì²­")
        print("'exit', 'quit', 'ì¢…ë£Œ'ë¡œ ì¢…ë£Œí•©ë‹ˆë‹¤.\n")

        while True:
            try:
                user_input = input("ğŸ‘¤ User >> ").strip()
                if user_input.lower() in ["exit", "quit", "ì¢…ë£Œ", "q"]:
                    print("\nğŸ‘‹ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!")
                    break
                if not user_input:
                    print("âŒ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.\n")
                    continue

                print("\n" + "-" * 60)
                print("ğŸ”„ ë‹µë³€ ìƒì„± ì¤‘...")
                print("-" * 60 + "\n")

                # 1. ê²€ìƒ‰
                docs = retrieve_documents(user_input, vectorstore, bm25_retriever)
                
                # 2. ìƒì„±
                answer = generate_answer(user_input, docs, llm)
                
                if answer:
                    print("\n" + "=" * 60)
                    print("ğŸ¤– AI ë‹µë³€:")
                    print("=" * 60)
                    print(f"\n{answer}\n")
                    print("=" * 60 + "\n")
                else:
                    print("\nâš ï¸ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n")

            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!")
                break
            except Exception as e:
                print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
                print("ğŸ’¡ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.\n")
                import traceback
                traceback.print_exc()

    except Exception as e:
        print(f"\nâŒ ì±—ë´‡ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        raise


if __name__ == "__main__":
    main()
