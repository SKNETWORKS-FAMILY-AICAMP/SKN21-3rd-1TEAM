import os
import warnings
from pathlib import Path
from typing import Annotated, TypedDict, Sequence, Optional, List
from dotenv import load_dotenv

# Qdrant & LangChain ê´€ë ¨ ì„í¬íŠ¸
from qdrant_client import QdrantClient
from langchain_qdrant import QdrantVectorStore
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_openai import ChatOpenAI
from langchain_core.documents import Document

# LangGraph ê´€ë ¨ ì„í¬íŠ¸
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ: ì‹¤í–‰ ìœ„ì¹˜(CWD)ì™€ ë¬´ê´€í•˜ê²Œ ì´ íŒŒì¼ê³¼ ê°™ì€ í´ë”ì˜ .envë¥¼ ì‚¬ìš©
_DOTENV_PATH = Path(__file__).with_name(".env")
load_dotenv(dotenv_path=_DOTENV_PATH)


# ===========================
# State ì •ì˜
# ===========================
class AgentState(TypedDict):
    """LangGraph Agentì˜ ìƒíƒœë¥¼ ì •ì˜í•˜ëŠ” TypedDict"""
    # ëŒ€í™” íˆìŠ¤í† ë¦¬
    messages: Annotated[Sequence[BaseMessage], add_messages]
    # ì‚¬ìš©ì ì§ˆë¬¸
    user_query: str
    # ì§ˆë¬¸ ë¶„ë¥˜ ê²°ê³¼
    query_classification: Optional[str]
    # ê²€ìƒ‰ ê²°ê³¼ (Document ë¦¬ìŠ¤íŠ¸)
    retrieved_docs: Optional[List[Document]]
    # ìƒì„±ëœ ë‹µë³€
    generated_answer: Optional[str]
    # ê²€ì¦ ê²°ê³¼
    validation_result: Optional[bool]
    # ê²€ì¦ í”¼ë“œë°±
    validation_feedback: Optional[str]
    # ì¬ì‹œë„ íšŸìˆ˜
    retry_count: int


# ===========================
# ê²€ìƒ‰ í•¨ìˆ˜ ì •ì˜ (ì‚¬ì „ ì¤€ë¹„ ì˜ì—­)
# ===========================
def create_search_function(vectorstore: QdrantVectorStore):
    """ë²•ë¥  ê²€ìƒ‰ í•¨ìˆ˜ ìƒì„±"""
    
    def search_legal_docs(query: str, k: int = 5) -> List[tuple]:
        """
        ë²•ë¥ /íŒë¡€/í–‰ì •í•´ì„ì„ Qdrantì—ì„œ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜
            
        Returns:
            (Document, score) íŠœí”Œì˜ ë¦¬ìŠ¤íŠ¸
        """
        results = vectorstore.similarity_search_with_score(query, k=k)
        return results
    
    return search_legal_docs


# ===========================
# ë…¸ë“œ í•¨ìˆ˜ ì •ì˜ (LangGraph ì˜ì—­)
# ===========================

def create_classify_node(llm: ChatOpenAI):
    """ë…¸ë“œ 1: ì§ˆë¬¸ ë¶„ë¥˜"""
    
    classify_prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ ë²•ë¥  ì§ˆë¬¸ì„ ë¶„ë¥˜í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”:

1. ë…¸ë™ë²• - ê·¼ë¡œê¸°ì¤€ë²•, ë…¸ë™ì¡°í•©, ì„ê¸ˆ, í‡´ì§ê¸ˆ, í•´ê³  ë“±
2. í˜•ì‚¬ë²• - ë²”ì£„, í˜•ë²Œ, ìˆ˜ì‚¬, ì¬íŒ ë“±
3. ë¯¼ì‚¬ë²• - ê³„ì•½, ì†í•´ë°°ìƒ, ì†Œìœ ê¶Œ, ì±„ê¶Œ ë“±
4. ê¸°íƒ€ - ìœ„ ì¹´í…Œê³ ë¦¬ì— ì†í•˜ì§€ ì•ŠëŠ” ë²•ë¥  ì§ˆë¬¸

ë¶„ë¥˜ ê²°ê³¼ë§Œ ë°˜í™˜í•˜ì„¸ìš”. ì˜ˆ: "ë…¸ë™ë²•", "í˜•ì‚¬ë²•", "ë¯¼ì‚¬ë²•", "ê¸°íƒ€" """),
        ("human", "{query}")
    ])
    
    def classify_query(state: AgentState) -> AgentState:
        """ì§ˆë¬¸ ë¶„ë¥˜ ë…¸ë“œ"""
        query = state["user_query"]
        
        chain = classify_prompt | llm
        response = chain.invoke({"query": query})
        classification = response.content.strip()
        
        print(f"ğŸ“‹ [ì§ˆë¬¸ ë¶„ë¥˜] {classification}")
        
        return {
            "query_classification": classification
        }
    
    return classify_query


def create_search_node(search_function):
    """ë…¸ë“œ 2: ê²€ìƒ‰ ì‹¤í–‰"""
    
    def search_documents(state: AgentState) -> AgentState:
        """ê²€ìƒ‰ ì‹¤í–‰ ë…¸ë“œ"""
        query = state["user_query"]
        classification = state.get("query_classification", "ê¸°íƒ€")
        
        # ê²€ìƒ‰ ìˆ˜í–‰
        print(f"ğŸ” [ê²€ìƒ‰ ì‹¤í–‰] ì¿¼ë¦¬: {query[:50]}...")
        
        # ì¬ì‹œë„ ì‹œ ê²€ìƒ‰ ê°œìˆ˜ ì¦ê°€
        retry_count = state.get("retry_count", 0)
        k = 5 + (retry_count * 3)  # ì¬ì‹œë„ë§ˆë‹¤ 3ê°œì”© ë” ê²€ìƒ‰
        
        results = search_function(query, k=k)
        
        if results:
            docs = [doc for doc, score in results]
            print(f"âœ… [ê²€ìƒ‰ ì™„ë£Œ] {len(docs)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ë¨")
        else:
            docs = []
            print(f"âš ï¸  [ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ]")
        
        return {
            "retrieved_docs": docs
        }
    
    return search_documents


def create_generate_node(llm: ChatOpenAI):
    """ë…¸ë“œ 3: ë‹µë³€ ìƒì„±"""
    
    answer_prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ ë²•ë¥  ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ 'A-TEAM ë´‡'ì…ë‹ˆë‹¤.

ì—­í• :
- ê²€ìƒ‰ëœ ë²•ë¥  ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•©ë‹ˆë‹¤.
- ë²•ë ¹ëª…, ì¡°í•­, íŒë¡€ë²ˆí˜¸ ë“± êµ¬ì²´ì ì¸ ê·¼ê±°ë¥¼ ì œì‹œí•©ë‹ˆë‹¤.
- ë²•ë¥  ìš©ì–´ëŠ” ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª…í•©ë‹ˆë‹¤.

ë‹µë³€ ì‘ì„± ì‹œ:
1. ê²€ìƒ‰ëœ ìë£Œë§Œì„ ê·¼ê±°ë¡œ ë‹µë³€í•˜ì„¸ìš”.
2. ë‹µë³€ì€ êµ¬ì¡°í™”í•˜ì—¬ ì‘ì„±í•˜ì„¸ìš” (ê²°ë¡  â†’ ê·¼ê±° â†’ ì¶”ê°€ ì„¤ëª…).
3. ê´€ë ¨ ë²•ë ¹ê³¼ ì¡°í•­ì„ ëª…ì‹œí•˜ì„¸ìš”.
4. í™•ì‹¤í•˜ì§€ ì•Šì€ ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.
5. í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”."""),
        ("human", """ì§ˆë¬¸ ì¹´í…Œê³ ë¦¬: {classification}

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ê²€ìƒ‰ëœ ê´€ë ¨ ë¬¸ì„œ:
{context}

ìœ„ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.""")
    ])
    
    def generate_answer(state: AgentState) -> AgentState:
        """ë‹µë³€ ìƒì„± ë…¸ë“œ"""
        query = state["user_query"]
        classification = state.get("query_classification", "ê¸°íƒ€")
        docs = state.get("retrieved_docs", [])
        
        print(f"ğŸ’¬ [ë‹µë³€ ìƒì„± ì¤‘...]")
        
        if not docs:
            answer = "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ë²•ë¥  ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì‹œê±°ë‚˜, ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
        else:
            # ë¬¸ì„œë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…
            context_parts = []
            for i, doc in enumerate(docs, 1):
                metadata = doc.metadata
                source = metadata.get("source", "unknown")
                title = metadata.get("title", "")
                content = doc.page_content[:1000]  # ë¬¸ì„œë‹¹ ìµœëŒ€ 1000ì
                
                context_parts.append(f"[ë¬¸ì„œ {i}] {source} - {title}\n{content}\n")
            
            context = "\n".join(context_parts)
            
            # LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±
            chain = answer_prompt | llm
            response = chain.invoke({
                "classification": classification,
                "query": query,
                "context": context
            })
            answer = response.content
        
        print(f"âœ… [ë‹µë³€ ìƒì„± ì™„ë£Œ]")
        
        return {
            "generated_answer": answer
        }
    
    return generate_answer


def create_validation_node(llm: ChatOpenAI):
    """ë…¸ë“œ 4: ê²€ì¦"""
    
    validation_prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ ë²•ë¥  ë‹µë³€ì˜ í’ˆì§ˆì„ ê²€ì¦í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ë‹µë³€ì„ í‰ê°€í•˜ì—¬ ë‹¤ìŒ ê¸°ì¤€ì„ í™•ì¸í•˜ì„¸ìš”:
1. ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ê·¼ê±°ë¡œ ë‹µë³€í–ˆëŠ”ê°€?
2. ë²•ë ¹ëª…ì´ë‚˜ ì¡°í•­ ë“± êµ¬ì²´ì ì¸ ê·¼ê±°ê°€ ìˆëŠ”ê°€?
3. ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ë‹µë³€í–ˆëŠ”ê°€?
4. ë‹µë³€ì´ ì¶©ë¶„íˆ ìƒì„¸í•œê°€?

ê²€ì¦ ê²°ê³¼ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”:
{
  "valid": true/false,
  "feedback": "ê²€ì¦ ê²°ê³¼ì— ëŒ€í•œ í”¼ë“œë°±"
}"""),
        ("human", """ì‚¬ìš©ì ì§ˆë¬¸: {query}

ê²€ìƒ‰ëœ ë¬¸ì„œ ê°œìˆ˜: {doc_count}

ìƒì„±ëœ ë‹µë³€:
{answer}

ìœ„ ë‹µë³€ì„ ê²€ì¦í•´ì£¼ì„¸ìš”.""")
    ])
    
    def validate_answer(state: AgentState) -> AgentState:
        """ë‹µë³€ ê²€ì¦ ë…¸ë“œ"""
        query = state["user_query"]
        answer = state.get("generated_answer", "")
        docs = state.get("retrieved_docs", [])
        
        print(f"ğŸ” [ë‹µë³€ ê²€ì¦ ì¤‘...]")
        
        # ê¸°ë³¸ ê²€ì¦: ë‹µë³€ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì‹¤íŒ¨
        if len(answer) < 50 or not docs:
            print(f"âŒ [ê²€ì¦ ì‹¤íŒ¨] ë‹µë³€ì´ ë¶ˆì¶©ë¶„í•©ë‹ˆë‹¤.")
            return {
                "validation_result": False,
                "validation_feedback": "ë‹µë³€ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
            }
        
        # LLMìœ¼ë¡œ ê²€ì¦
        chain = validation_prompt | llm
        response = chain.invoke({
            "query": query,
            "doc_count": len(docs),
            "answer": answer
        })
        
        # ì‘ë‹µ íŒŒì‹± (ê°„ë‹¨í•˜ê²Œ "valid": true/false ì°¾ê¸°)
        content = response.content.lower()
        is_valid = "true" in content or "í†µê³¼" in content or "ì ì ˆ" in content
        
        if is_valid:
            print(f"âœ… [ê²€ì¦ í†µê³¼]")
        else:
            print(f"âš ï¸  [ê²€ì¦ ì‹¤íŒ¨] ì¬ì‹œë„ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        return {
            "validation_result": is_valid,
            "validation_feedback": response.content
        }
    
    return validate_answer


def create_retry_decision_node():
    """ë…¸ë“œ 5: ì¬ì‹œë„ íŒë‹¨"""
    
    def decide_retry(state: AgentState) -> AgentState:
        """ì¬ì‹œë„ íŒë‹¨ ë…¸ë“œ"""
        retry_count = state.get("retry_count", 0)
        validation_result = state.get("validation_result", False)
        
        if not validation_result and retry_count < 2:  # ìµœëŒ€ 2ë²ˆ ì¬ì‹œë„
            new_retry_count = retry_count + 1
            print(f"ğŸ”„ [ì¬ì‹œë„ {new_retry_count}/2] ê²€ìƒ‰ì„ ë‹¤ì‹œ ì‹œë„í•©ë‹ˆë‹¤.")
            return {"retry_count": new_retry_count}
        elif not validation_result:
            print(f"âš ï¸  [ì¬ì‹œë„ ì œí•œ ë„ë‹¬] í˜„ì¬ ë‹µë³€ì„ ë°˜í™˜í•©ë‹ˆë‹¤.")
        
        return {"retry_count": retry_count}
    
    return decide_retry


# ===========================
# ì¡°ê±´ë¶€ ì—£ì§€ í•¨ìˆ˜
# ===========================
def should_retry(state: AgentState) -> str:
    """ê²€ì¦ í›„ ì¬ì‹œë„ ì—¬ë¶€ ê²°ì •"""
    validation_result = state.get("validation_result", False)
    retry_count = state.get("retry_count", 0)
    
    if not validation_result and retry_count < 2:
        return "retry"  # ì¬ì‹œë„ ë…¸ë“œë¡œ
    else:
        return "end"  # ì¢…ë£Œ


# ===========================
# ì‚¬ì „ ì¤€ë¹„ ì˜ì—­: ë¦¬ì†ŒìŠ¤ ì´ˆê¸°í™”
# ===========================
def initialize_resources():
    """ì„ë² ë”© ëª¨ë¸, ë²¡í„°ìŠ¤í† ì–´, Retriever, Tools ì´ˆê¸°í™”"""
    
    # 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
    COLLECTION_NAME = os.getenv("QDRANT_COLLECTION_NAME")
    QDRANT_URL = os.getenv("QDRANT_URL")
    QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")
    
    if not QDRANT_API_KEY:
        raise ValueError("QDRANT_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
    
    print(f"ğŸ”§ ì„¤ì • ë¡œë“œ ì™„ë£Œ")
    
    # 2. ì„ë² ë”© ëª¨ë¸ ì„¤ì •
    print(f"\nğŸš€ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘ (Qwen/Qwen3-Embedding-0.6B)...")
    embeddings = HuggingFaceEmbeddings(
        model_name="Qwen/Qwen3-Embedding-0.6B",
        model_kwargs={'trust_remote_code': True},
        encode_kwargs={'normalize_embeddings': True}
    )
    print("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    # 3. Qdrant í´ë¼ì´ì–¸íŠ¸ ì—°ê²°
    print(f"\nğŸ“¡ Qdrant ì—°ê²° ì¤‘...")
    warnings.filterwarnings(
        'ignore', message='Api key is used with an insecure connection')
    
    client = QdrantClient(
        url=QDRANT_URL,
        api_key=QDRANT_API_KEY,
        timeout=30,
        prefer_grpc=False)
    print("âœ… Qdrant ì—°ê²° ì™„ë£Œ")
    
    # 4. ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
    print(f"\nğŸ—‚ï¸  ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” ì¤‘...")
    print("   (ì»¬ë ‰ì…˜ ê²€ì¦ ì¤‘... ë„¤íŠ¸ì›Œí¬ ìƒíƒœì— ë”°ë¼ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    vectorstore = QdrantVectorStore(
        client=client,
        collection_name=COLLECTION_NAME,
        embedding=embeddings,
        content_payload_key="text"
    )
    print("âœ… ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” ì™„ë£Œ")
    
    # 5. Retriever ìƒì„± (ì‚¬ìš© ì•ˆ í•  ìˆ˜ë„ ìˆì§€ë§Œ ì¤€ë¹„)
    print(f"\nğŸ” Retriever ìƒì„± ì¤‘...")
    retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 5}
    )
    print("âœ… Retriever ìƒì„± ì™„ë£Œ")
    
    # 6. ê²€ìƒ‰ í•¨ìˆ˜ ìƒì„±
    print(f"\nğŸ› ï¸  ê²€ìƒ‰ í•¨ìˆ˜ ìƒì„± ì¤‘...")
    search_function = create_search_function(vectorstore)
    print("âœ… ê²€ìƒ‰ í•¨ìˆ˜ ìƒì„± ì™„ë£Œ")
    
    return {
        "embeddings": embeddings,
        "vectorstore": vectorstore,
        "retriever": retriever,
        "search_function": search_function
    }


# ===========================
# LangGraph ì´ˆê¸°í™”
# ===========================
def initialize_langgraph_chatbot():
    """LangGraph ê¸°ë°˜ RAG ì±—ë´‡ ì´ˆê¸°í™”"""
    
    # ì‚¬ì „ ì¤€ë¹„: ë¦¬ì†ŒìŠ¤ ì´ˆê¸°í™”
    resources = initialize_resources()
    search_function = resources["search_function"]
    
    # LLM ì„¤ì •
    print(f"\nğŸ¤– LLM ì„¤ì • ì¤‘...")
    llm = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0,
        streaming=True
    )
    print("âœ… LLM ì„¤ì • ì™„ë£Œ")
    
    # ë…¸ë“œ ìƒì„±
    print(f"\nâš™ï¸  LangGraph ë…¸ë“œ ìƒì„± ì¤‘...")
    classify_node = create_classify_node(llm)
    search_node = create_search_node(search_function)
    generate_node = create_generate_node(llm)
    validation_node = create_validation_node(llm)
    retry_node = create_retry_decision_node()
    print("âœ… ë…¸ë“œ ìƒì„± ì™„ë£Œ")
    
    # StateGraph êµ¬ì„±
    print(f"\nğŸ”— LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„± ì¤‘...")
    workflow = StateGraph(AgentState)
    
    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("classify", classify_node)
    workflow.add_node("search", search_node)
    workflow.add_node("generate", generate_node)
    workflow.add_node("validate", validation_node)
    workflow.add_node("retry_decision", retry_node)
    
    # ì—£ì§€ ì¶”ê°€
    workflow.set_entry_point("classify")
    workflow.add_edge("classify", "search")
    workflow.add_edge("search", "generate")
    workflow.add_edge("generate", "validate")
    
    # ì¡°ê±´ë¶€ ì—£ì§€: ê²€ì¦ í›„ ì¬ì‹œë„ ë˜ëŠ” ì¢…ë£Œ
    workflow.add_conditional_edges(
        "validate",
        should_retry,
        {
            "retry": "retry_decision",
            "end": END
        }
    )
    workflow.add_edge("retry_decision", "search")
    
    # ê·¸ë˜í”„ ì»´íŒŒì¼
    graph = workflow.compile()
    print("âœ… LangGraph êµ¬ì„± ì™„ë£Œ")
    
    return graph


# ===========================
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ===========================
def main():
    """LangGraph RAG ì±—ë´‡ ì‹¤í–‰ ë©”ì¸ í•¨ìˆ˜"""
    
    # API Key í™•ì¸
    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ ì˜¤ë¥˜: OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ .env íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
        return
    
    try:
        # ì±—ë´‡ ì´ˆê¸°í™”
        print("\n" + "="*60)
        print("ğŸš€ A-TEAM ë²•ë¥  RAG ì±—ë´‡ (LangGraph V1) ì´ˆê¸°í™” ì‹œì‘")
        print("="*60 + "\n")
        
        graph = initialize_langgraph_chatbot()
        
        print("\n" + "="*60)
        print("âœ… ğŸ¤– A-TEAM ë²•ë¥  ì±—ë´‡ ì¤€ë¹„ ì™„ë£Œ!")
        print("="*60)
        print("\nğŸ’¡ ì‚¬ìš© ë°©ë²•:")
        print("  - ë…¸ë™ë¶„ì•¼ ë²•ë¥ , í˜•ì‚¬ë²•, ë¯¼ì‚¬ë²• ê´€ë ¨ ì§ˆë¬¸ì— ì‘ë‹µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print("  - 'exit', 'quit', 'ì¢…ë£Œ'ë¥¼ ì…ë ¥í•˜ë©´ ì¢…ë£Œë©ë‹ˆë‹¤")
        print("\nğŸ“Š ì›Œí¬í”Œë¡œìš°:")
        print("  1. ì§ˆë¬¸ ë¶„ë¥˜ â†’ 2. ê²€ìƒ‰ ì‹¤í–‰ â†’ 3. ë‹µë³€ ìƒì„±")
        print("  â†’ 4. ê²€ì¦ â†’ 5. ì¬ì‹œë„ íŒë‹¨ (í•„ìš”ì‹œ)")
        print("="*60 + "\n")
        
        # ëŒ€í™” ë£¨í”„
        while True:
            try:
                # ì‚¬ìš©ì ì…ë ¥
                user_input = input("ğŸ‘¤ User >> ").strip()
                
                # ì¢…ë£Œ ëª…ë ¹ í™•ì¸
                if user_input.lower() in ["exit", "quit", "ì¢…ë£Œ", "q"]:
                    print("\nğŸ‘‹ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!")
                    break
                
                # ë¹ˆ ì…ë ¥ ì²´í¬
                if not user_input:
                    print("âŒ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.\n")
                    continue
                
                # ì´ˆê¸° ìƒíƒœ ì„¤ì •
                initial_state = {
                    "messages": [HumanMessage(content=user_input)],
                    "user_query": user_input,
                    "query_classification": None,
                    "retrieved_docs": None,
                    "generated_answer": None,
                    "validation_result": None,
                    "validation_feedback": None,
                    "retry_count": 0
                }
                
                # ê·¸ë˜í”„ ì‹¤í–‰
                print("\n" + "="*60)
                print("ğŸ”„ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘...")
                print("="*60 + "\n")
                
                result = graph.invoke(initial_state)
                
                # ìµœì¢… ë‹µë³€ ì¶œë ¥
                answer = result.get("generated_answer", "")
                if answer:
                    print("\n" + "="*60)
                    print("ğŸ¤– AI ë‹µë³€:")
                    print("="*60)
                    print(f"\n{answer}\n")
                    print("="*60 + "\n")
                else:
                    print("\nâš ï¸ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n")
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!")
                break
            except Exception as e:
                print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
                print("ğŸ’¡ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.\n")
                import traceback
                traceback.print_exc()
    
    except Exception as e:
        print(f"\nâŒ ì±—ë´‡ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        print("ğŸ’¡ ì„¤ì •ì„ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        import traceback
        traceback.print_exc()
        raise


if __name__ == "__main__":
    main()
