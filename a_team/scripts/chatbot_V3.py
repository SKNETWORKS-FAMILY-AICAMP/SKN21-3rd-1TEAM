import os
import warnings
from pathlib import Path
from typing import Annotated, TypedDict, Sequence, Optional, List, Literal
from dotenv import load_dotenv

# Qdrant & LangChain ê´€ë ¨ ì„í¬íŠ¸
from qdrant_client import QdrantClient
from langchain_qdrant import QdrantVectorStore
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_openai import ChatOpenAI
from langchain_core.documents import Document, BaseDocumentCompressor
from langchain_community.tools import TavilySearchResults
from pydantic import BaseModel, Field
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from typing import Any

# LangGraph ê´€ë ¨ ì„í¬íŠ¸
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ: ì‹¤í–‰ ìœ„ì¹˜(CWD)ì™€ ë¬´ê´€í•˜ê²Œ ì´ íŒŒì¼ê³¼ ê°™ì€ í´ë”ì˜ .envë¥¼ ì‚¬ìš©
_DOTENV_PATH = Path(__file__).with_name(".env")
load_dotenv(dotenv_path=_DOTENV_PATH)


# ===========================
# State ì •ì˜
# ===========================
class AgentState(TypedDict):
    """LangGraph Agentì˜ ìƒíƒœë¥¼ ì •ì˜í•˜ëŠ” TypedDict"""
    # ëŒ€í™” íˆìŠ¤í† ë¦¬
    messages: Annotated[Sequence[BaseMessage], add_messages]
    # ì‚¬ìš©ì ì§ˆë¬¸
    user_query: str
    # ì§ˆë¬¸ ë¶„ì„ ê²°ê³¼
    # {category, needs_clarification, needs_case_law, clarification_question}
    query_analysis: Optional[dict]
    # ê²€ìƒ‰ ê²°ê³¼ (Document ë¦¬ìŠ¤íŠ¸)
    retrieved_docs: Optional[List[Document]]
    # ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ì°¾ì€ íŒë¡€ ì •ë³´
    case_law_results: Optional[List[dict]]
    # ìƒì„±ëœ ë‹µë³€
    generated_answer: Optional[str]
    # í˜„ì¬ ë¼ìš°íŒ… ê²°ì •
    next_action: Optional[str]


# ===========================
# Reranker ì •ì˜
# ===========================
class JinaReranker(BaseDocumentCompressor):
    model_name: str = "jinaai/jina-reranker-v2-base-multilingual"
    top_n: int = 5
    model: Any = None
    tokenizer: Any = None

    class Config:
        arbitrary_types_allowed = True
        extra = "allow"

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name, trust_remote_code=True)
        self.model = AutoModelForSequenceClassification.from_pretrained(
            self.model_name, trust_remote_code=True, dtype="auto"
        )
        self.model.eval()

    def compress_documents(
        self, documents: Sequence[Document], query: str, callbacks: Optional[Any] = None
    ) -> Sequence[Document]:
        if not documents:
            return []

        pairs = [[query, doc.page_content] for doc in documents]

        with torch.no_grad():
            inputs = self.tokenizer(
                pairs,
                padding=True,
                truncation=True,
                return_tensors="pt",
                max_length=512
            )
            # Sigmoid ì ìš©í•˜ì—¬ 0~1 ì‚¬ì´ í™•ë¥ ë¡œ ë³€í™˜
            scores = self.model(**inputs).logits.squeeze(-1).float().cpu()
            scores = torch.sigmoid(scores).tolist()
            if not isinstance(scores, list):
                scores = [scores]

        # Sort and select top_n
        top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[
            :self.top_n]

        final_docs = []
        for i in top_indices:
            doc = documents[i]
            doc.metadata["relevance_score"] = scores[i]
            final_docs.append(doc)

        return final_docs


# ===========================
# ë…¸ë“œ í•¨ìˆ˜ ì •ì˜ (LangGraph ì˜ì—­)
# ===========================

# Pydantic ëª¨ë¸: ì§ˆë¬¸ ë¶„ì„ ê²°ê³¼
class QueryAnalysis(BaseModel):
    """LLMì´ ë°˜í™˜í•  ì§ˆë¬¸ ë¶„ì„ ê²°ê³¼"""
    category: str = Field(description="ë²•ë¥  ë¶„ì•¼: ë…¸ë™ë²•, í˜•ì‚¬ë²•, ë¯¼ì‚¬ë²•, ê¸°íƒ€ ì¤‘ í•˜ë‚˜")
    needs_clarification: bool = Field(
        default=False, description="ì§ˆë¬¸ì´ ê·¹ë„ë¡œ ëª¨í˜¸í•˜ì—¬ ë‹µë³€ ë¶ˆê°€ëŠ¥í•œì§€")
    needs_case_law: bool = Field(default=False, description="ëŒ€ë²•ì› íŒë¡€ ê²€ìƒ‰ì´ í•„ìš”í•œì§€")
    clarification_question: str = Field(
        default="", description="ëª…í™•í™” í•„ìš” ì‹œ ì‚¬ìš©ìì—ê²Œ ë¬¼ì–´ë³¼ ì§ˆë¬¸")


def create_analyze_query_node(llm: ChatOpenAI):
    """ë…¸ë“œ 1: ì§ˆë¬¸ ë¶„ì„ (Structured Output ì‚¬ìš©)"""

    # Structured Outputì„ ìœ„í•œ LLM
    structured_llm = llm.with_structured_output(QueryAnalysis)

    analyze_prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ ë²•ë¥  ì§ˆë¬¸ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

1. category: ì§ˆë¬¸ì˜ ë²•ë¥  ë¶„ì•¼
   - "ë…¸ë™ë²•": ê·¼ë¡œê¸°ì¤€ë²•, ì„ê¸ˆ, í‡´ì§ê¸ˆ, í•´ê³ , ì‚°ì¬, ì£¼íœ´ìˆ˜ë‹¹ ë“±
   - "í˜•ì‚¬ë²•": ë²”ì£„, í˜•ë²Œ, ìˆ˜ì‚¬, ì¬íŒ, ê³ ì†Œ/ê³ ë°œ ë“±
   - "ë¯¼ì‚¬ë²•": ê³„ì•½, ì†í•´ë°°ìƒ, ì†Œìœ ê¶Œ, ì±„ê¶Œ ë“±
   - "ê¸°íƒ€": ìœ„ ì¹´í…Œê³ ë¦¬ì— ì†í•˜ì§€ ì•ŠëŠ” ë²•ë¥  ì§ˆë¬¸

2. needs_clarification: ì§ˆë¬¸ì´ ê·¹ë„ë¡œ ëª¨í˜¸í•˜ì—¬ ì–´ë–¤ ë‹µë³€ë„ ë¶ˆê°€ëŠ¥í•œì§€ (true/false)
   - true: "ë²•ë¥  ì§ˆë¬¸ì´ìš”", "ë„ì™€ì£¼ì„¸ìš”", "ê³„ì•½" ì²˜ëŸ¼ 1~2ë‹¨ì–´ë§Œ ìˆëŠ” ê²½ìš°
   - false (ëŒ€ë¶€ë¶„): ìƒí™©ì´ ì¡°ê¸ˆì´ë¼ë„ ì„¤ëª…ë˜ì–´ ìˆìœ¼ë©´ ë‹µë³€ ê°€ëŠ¥
   - ì˜ˆ: "ì£¼15ì‹œê°„ ì´ìƒ ê·¼ë¬´í–ˆëŠ”ë° ì£¼íœ´ìˆ˜ë‹¹ì„ ì•ˆ ì¤˜" â†’ false (ë‹µë³€ ê°€ëŠ¥)
   - ì˜ˆ: "í•´ê³ ë‹¹í–ˆì–´ìš”" â†’ false (ë¶€ë‹¹í•´ê³  ì¼ë°˜ë¡  ì„¤ëª… ê°€ëŠ¥)

3. needs_case_law: ëŒ€ë²•ì› íŒë¡€ê°€ í•„ìš”í•œì§€ (true/false)
   - true: "íŒë¡€", "íŒê²°", "ëŒ€ë²•ì›" ë“±ì„ ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰í•˜ê±°ë‚˜, ë²•ì  í•´ì„ì´ í•„ìš”í•œ ìŸì  ì‚¬ì•ˆ
   - false: ë‹¨ìˆœ ë²•ë ¹ ì¡°íšŒ, ì ˆì°¨/ì„œì‹ ë¬¸ì˜

4. clarification_question: needs_clarificationì´ trueì¼ ë•Œë§Œ ì‘ì„±"""),
        ("human", "{query}")
    ])

    def analyze_query(state: AgentState) -> AgentState:
        """ì§ˆë¬¸ ë¶„ì„ ë…¸ë“œ: Structured Outputìœ¼ë¡œ ë¶„ë¥˜/ëª…í™•í™”/íŒë¡€ í•„ìš” ì—¬ë¶€ íŒë‹¨"""
        query = state["user_query"]

        print(f"ğŸ” [ì§ˆë¬¸ ë¶„ì„ ì¤‘...]")

        chain = analyze_prompt | structured_llm
        analysis: QueryAnalysis = chain.invoke({"query": query})

        print(f"ğŸ“‹ [ë¶„ì„ ê²°ê³¼] ë¶„ì•¼: {analysis.category}")
        print(f"   ëª…í™•í™” í•„ìš”: {'ì˜ˆ' if analysis.needs_clarification else 'ì•„ë‹ˆì˜¤'}")
        print(f"   íŒë¡€ í•„ìš”: {'ì˜ˆ' if analysis.needs_case_law else 'ì•„ë‹ˆì˜¤'}")

        return {
            "query_analysis": analysis.model_dump()
        }

    return analyze_query


def create_clarify_node(llm: ChatOpenAI):
    """ë…¸ë“œ 2: ì‚¬ìš©ìì—ê²Œ ëª…í™•í™” ìš”ì²­"""

    def request_clarification(state: AgentState) -> AgentState:
        """ëª…í™•í™” ìš”ì²­ ë…¸ë“œ: ëª¨í˜¸í•œ ì§ˆë¬¸ì— ëŒ€í•´ êµ¬ì²´ì ì¸ ì •ë³´ ìš”ì²­"""
        analysis = state.get("query_analysis", {})
        clarification_q = analysis.get("clarification_question", "")

        if not clarification_q:
            # ê¸°ë³¸ ëª…í™•í™” ì§ˆë¬¸
            clarification_q = "ì§ˆë¬¸ì„ ì¢€ ë” êµ¬ì²´ì ìœ¼ë¡œ í•´ì£¼ì‹œê² ì–´ìš”? ì–´ë–¤ ìƒí™©ì¸ì§€, ë¬´ì—‡ì´ ê¶ê¸ˆí•˜ì‹ ì§€ ìì„¸íˆ ì•Œë ¤ì£¼ì‹œë©´ ë” ì •í™•í•œ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤."

        print(f"â“ [ëª…í™•í™” ìš”ì²­]")

        # ì¹œì ˆí•œ í˜•ì‹ìœ¼ë¡œ ë‹µë³€ êµ¬ì„±
        answer = f"""ì•ˆë…•í•˜ì„¸ìš”! ì§ˆë¬¸ì„ ì˜ ì´í•´í•˜ê¸° ìœ„í•´ ëª‡ ê°€ì§€ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.

{clarification_q}

ìœ„ ë‚´ìš©ì„ í¬í•¨í•´ì„œ ë‹¤ì‹œ ì§ˆë¬¸í•´ ì£¼ì‹œë©´, ë” ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ğŸ˜Š"""

        return {
            "generated_answer": answer,
            "next_action": "end"
        }

    return request_clarification


def create_search_node(vectorstore: QdrantVectorStore):
    """ë…¸ë“œ 3: Qdrant ë²¡í„°DB ê²€ìƒ‰"""

    def search_documents(state: AgentState) -> AgentState:
        """ê²€ìƒ‰ ì‹¤í–‰ ë…¸ë“œ: Qdrantì—ì„œ ê´€ë ¨ ë²•ë ¹/ë¬¸ì„œ ê²€ìƒ‰"""
        query = state["user_query"]
        analysis = state.get("query_analysis", {})
        category = analysis.get("category", "ê¸°íƒ€")

        print(f"ğŸ” [ë²•ë ¹ ê²€ìƒ‰] ì¿¼ë¦¬: {query[:50]}...")

        # ì¹´í…Œê³ ë¦¬ì— ë”°ë¥¸ ê²€ìƒ‰ ìµœì í™” (í–¥í›„ í•„í„° ì¶”ê°€ ê°€ëŠ¥)
        # 1. 1ì°¨ ê²€ìƒ‰ (ìœ ì‚¬ë„ ê¸°ë°˜, ë” ë„“ê²Œ ê²€ìƒ‰)
        results = vectorstore.similarity_search_with_score(query, k=20)

        if results:
            docs = [doc for doc, score in results]

            # 2. ë¦¬ë­í‚¹ (Jina Reranker)
            print(f"ğŸ”„ [ë¦¬ë­í‚¹] Jina Rerankerë¡œ ìƒìœ„ 5ê°œ ë¬¸ì„œ ì„ ë³„ ì¤‘...")
            try:
                reranker = JinaReranker(top_n=5)
                reranked_docs = reranker.compress_documents(docs, query)

                if reranked_docs:
                    print(f"âœ… [ë¦¬ë­í‚¹ ì™„ë£Œ] {len(reranked_docs)}ê°œ ë¬¸ì„œ ì„ ë³„")
                    # ë¦¬ë­í‚¹ ì ìˆ˜ ì¶œë ¥
                    for i, doc in enumerate(reranked_docs, 1):
                        print(
                            f"   [{i}] ì ìˆ˜: {doc.metadata.get('relevance_score', 0):.4f} | {doc.page_content[:30]}...")
                    docs = reranked_docs
                else:
                    print(f"âš ï¸  [ë¦¬ë­í‚¹ ê²°ê³¼ ì—†ìŒ] ì›ë³¸ ê²€ìƒ‰ ê²°ê³¼ ì‚¬ìš© (ìƒìœ„ 5ê°œ)")
                    docs = docs[:5]
            except Exception as e:
                print(f"âš ï¸  [ë¦¬ë­í‚¹ ì˜¤ë¥˜] {e}")
                print(f"   ì›ë³¸ ê²€ìƒ‰ ê²°ê³¼ ì‚¬ìš© (ìƒìœ„ 5ê°œ)")
                docs = docs[:5]

            # avg_score logic updated for re-ranking scores
            if docs:
                scores = [doc.metadata.get("relevance_score", 0)
                          for doc in docs]
                avg_score = sum(scores) / len(scores) if scores else 0.0
            else:
                avg_score = 0.0

            print(f"âœ… [ê²€ìƒ‰ ìµœì¢… ì™„ë£Œ] {len(docs)}ê°œ ë¬¸ì„œ")
        else:
            docs = []
            print(f"âš ï¸  [ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ]")

        return {
            "retrieved_docs": docs
        }

    return search_documents


def create_case_law_search_node(llm: ChatOpenAI):
    """ë…¸ë“œ 4: ì›¹ ê²€ìƒ‰ì„ í†µí•œ ëŒ€ë²•ì› íŒë¡€ ê²€ìƒ‰"""

    def search_case_law(state: AgentState) -> AgentState:
        """ëŒ€ë²•ì› íŒë¡€ ê²€ìƒ‰ ë…¸ë“œ: Tavilyë¥¼ í†µí•´ ê´€ë ¨ íŒë¡€ ì›¹ ê²€ìƒ‰"""
        query = state["user_query"]
        analysis = state.get("query_analysis", {})
        category = analysis.get("category", "ê¸°íƒ€")

        print(f"âš–ï¸  [íŒë¡€ ê²€ìƒ‰] ëŒ€ë²•ì› íŒë¡€ ì›¹ ê²€ìƒ‰ ì¤‘...")

        # Tavily API í‚¤ í™•ì¸
        tavily_api_key = os.getenv("TAVILY_API_KEY")
        if not tavily_api_key:
            print(f"âš ï¸  [íŒë¡€ ê²€ìƒ‰ ìŠ¤í‚µ] TAVILY_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return {"case_law_results": []}

        try:
            # Tavily ê²€ìƒ‰ ë„êµ¬ ì„¤ì •
            search_tool = TavilySearchResults(
                max_results=3,
                search_depth="advanced",
                include_answer=True,
                include_raw_content=False
            )

            # íŒë¡€ ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™”
            search_query = f"ëŒ€ë²•ì› íŒë¡€ {category} {query}"

            # ê²€ìƒ‰ ì‹¤í–‰
            results = search_tool.invoke({"query": search_query})

            if results:
                case_laws = []
                for r in results:
                    case_laws.append({
                        "title": r.get("title", ""),
                        "url": r.get("url", ""),
                        "content": r.get("content", "")[:500]  # ë‚´ìš© ì œí•œ
                    })
                print(f"âœ… [íŒë¡€ ê²€ìƒ‰ ì™„ë£Œ] {len(case_laws)}ê±´ ë°œê²¬")
                return {"case_law_results": case_laws}
            else:
                print(f"âš ï¸  [íŒë¡€ ê²€ìƒ‰] ê´€ë ¨ íŒë¡€ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                return {"case_law_results": []}

        except Exception as e:
            print(f"âš ï¸  [íŒë¡€ ê²€ìƒ‰ ì˜¤ë¥˜] {e}")
            return {"case_law_results": []}

    return search_case_law


def create_generate_node(llm: ChatOpenAI):
    """ë…¸ë“œ 5: ìµœì¢… ë‹µë³€ ìƒì„±"""

    answer_prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ ë²•ë¥  ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ 'A-TEAM ë´‡'ì…ë‹ˆë‹¤.

ì—­í• :
- ê²€ìƒ‰ëœ ë²•ë¥  ë¬¸ì„œì™€ íŒë¡€ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•©ë‹ˆë‹¤.
- ë²•ë ¹ëª…, ì¡°í•­, íŒë¡€ë²ˆí˜¸ ë“± êµ¬ì²´ì ì¸ ê·¼ê±°ë¥¼ ì œì‹œí•©ë‹ˆë‹¤.
- ë²•ë¥  ìš©ì–´ëŠ” ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª…í•©ë‹ˆë‹¤.

ë‹µë³€ ì‘ì„± ê·œì¹™:
1. ê²€ìƒ‰ëœ ìë£Œë¥¼ ê·¼ê±°ë¡œ ë‹µë³€í•˜ì„¸ìš”.
2. ë‹µë³€ êµ¬ì¡°: ğŸ“Œ ê²°ë¡  â†’ ğŸ“– ë²•ì  ê·¼ê±° â†’ ğŸ’¡ ì¶”ê°€ ì„¤ëª…
3. ê´€ë ¨ ë²•ë ¹ê³¼ ì¡°í•­ì„ [ë²•ë ¹ëª… ì œXì¡°]ì²˜ëŸ¼ ëª…ì‹œí•˜ì„¸ìš”.
4. íŒë¡€ê°€ ìˆìœ¼ë©´ [ëŒ€ë²•ì› XXXX. X. X. ì„ ê³  XXXë‹¤XXXX íŒê²°] í˜•ì‹ìœ¼ë¡œ ì¸ìš©í•˜ì„¸ìš”.
5. í™•ì‹¤í•˜ì§€ ì•Šì€ ë‚´ìš©ì€ "~ë¡œ í•´ì„ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤" ë“±ìœ¼ë¡œ ì‹ ì¤‘í•˜ê²Œ í‘œí˜„í•˜ì„¸ìš”.
6. ì „ë¬¸ ë²•ë¥  ìƒë‹´ì´ í•„ìš”í•œ ê²½ìš° ì•ˆë‚´í•˜ì„¸ìš”.
7. í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”."""),
        ("human", """ì§ˆë¬¸ ë¶„ì•¼: {category}

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ğŸ“š ê²€ìƒ‰ëœ ë²•ë ¹/ë¬¸ì„œ:
{context}

âš–ï¸ ê´€ë ¨ íŒë¡€ (ì›¹ ê²€ìƒ‰):
{case_law}

ìœ„ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.""")
    ])

    def generate_answer(state: AgentState) -> AgentState:
        """ë‹µë³€ ìƒì„± ë…¸ë“œ: ê²€ìƒ‰ ê²°ê³¼ì™€ íŒë¡€ë¥¼ ì¢…í•©í•˜ì—¬ ë‹µë³€ ìƒì„±"""
        query = state["user_query"]
        analysis = state.get("query_analysis", {})
        category = analysis.get("category", "ê¸°íƒ€")
        docs = state.get("retrieved_docs", [])
        case_laws = state.get("case_law_results", [])

        print(f"ğŸ’¬ [ë‹µë³€ ìƒì„± ì¤‘...]")

        # ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…
        if docs:
            context_parts = []
            for i, doc in enumerate(docs, 1):
                metadata = doc.metadata
                source = metadata.get("source", "")
                law_name = metadata.get("law_name", "")
                article = metadata.get("article_no", "")
                title = metadata.get(
                    "article_title", "") or metadata.get("title", "")
                content = doc.page_content[:800]

                header = f"[ë¬¸ì„œ {i}]"
                if law_name:
                    header += f" {law_name}"
                    if article:
                        header += f" ì œ{article}ì¡°"
                if title:
                    header += f" - {title}"

                context_parts.append(f"{header}\n{content}\n")

            context = "\n".join(context_parts)
        else:
            context = "(ê´€ë ¨ ë²•ë ¹ ë¬¸ì„œê°€ ê²€ìƒ‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤)"

        # íŒë¡€ ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…
        if case_laws:
            case_parts = []
            for i, case in enumerate(case_laws, 1):
                case_parts.append(
                    f"[íŒë¡€ {i}] {case.get('title', '')}\n{case.get('content', '')}\nì¶œì²˜: {case.get('url', '')}\n")
            case_law_context = "\n".join(case_parts)
        else:
            case_law_context = "(ê´€ë ¨ íŒë¡€ ì •ë³´ ì—†ìŒ)"

        # ê²€ìƒ‰ ê²°ê³¼ê°€ ì „í˜€ ì—†ëŠ” ê²½ìš°
        if not docs and not case_laws:
            answer = """ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë²•ë¥  ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.

ë‹¤ìŒê³¼ ê°™ì´ ì‹œë„í•´ ë³´ì‹œê² ì–´ìš”?
1. ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš” (ì˜ˆ: ìƒí™©, ê´€ë ¨ ë²•ë ¹ ë“±)
2. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ì§ˆë¬¸í•´ ë³´ì„¸ìš”
3. ë³µì¡í•œ ì‚¬ì•ˆì˜ ê²½ìš° ì „ë¬¸ ë²•ë¥  ìƒë‹´ì„ ê¶Œì¥ë“œë¦½ë‹ˆë‹¤."""
        else:
            # LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±
            chain = answer_prompt | llm
            response = chain.invoke({
                "category": category,
                "query": query,
                "context": context,
                "case_law": case_law_context
            })
            answer = response.content

        print(f"âœ… [ë‹µë³€ ìƒì„± ì™„ë£Œ]")

        return {
            "generated_answer": answer
        }

    return generate_answer


# ===========================
# ë¼ìš°íŒ… í•¨ìˆ˜ (ì¡°ê±´ë¶€ ë¶„ê¸°)
# ===========================

def route_after_analysis(state: AgentState) -> Literal["clarify", "search"]:
    """ë¶„ì„ í›„ ë¼ìš°íŒ…: ëª…í™•í™” í•„ìš” ì—¬ë¶€ì— ë”°ë¼ ë¶„ê¸°"""
    analysis = state.get("query_analysis", {})
    needs_clarification = analysis.get("needs_clarification", False)

    if needs_clarification:
        return "clarify"
    else:
        return "search"


def route_after_search(state: AgentState) -> Literal["case_law_search", "generate"]:
    """ê²€ìƒ‰ í›„ ë¼ìš°íŒ…: íŒë¡€ í•„ìš” ì—¬ë¶€ì— ë”°ë¼ ë¶„ê¸°"""
    analysis = state.get("query_analysis", {})
    needs_case_law = analysis.get("needs_case_law", False)

    if needs_case_law:
        return "case_law_search"
    else:
        return "generate"


# ===========================
# ì‚¬ì „ ì¤€ë¹„ ì˜ì—­: ë¦¬ì†ŒìŠ¤ ì´ˆê¸°í™”
# ===========================
def initialize_resources():
    """ì„ë² ë”© ëª¨ë¸, ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™”"""

    # 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
    COLLECTION_NAME = os.getenv("QDRANT_COLLECTION_NAME")
    QDRANT_URL = os.getenv("QDRANT_URL")
    QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")

    if not QDRANT_API_KEY:
        raise ValueError("QDRANT_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")

    print(f"ğŸ”§ ì„¤ì • ë¡œë“œ ì™„ë£Œ")

    # 2. ì„ë² ë”© ëª¨ë¸ ì„¤ì •
    print(f"\nğŸš€ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘ (Qwen/Qwen3-Embedding-0.6B)...")
    embeddings = HuggingFaceEmbeddings(
        model_name="Qwen/Qwen3-Embedding-0.6B",
        model_kwargs={'trust_remote_code': True},
        encode_kwargs={'normalize_embeddings': True}
    )
    print("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

    # 3. Qdrant í´ë¼ì´ì–¸íŠ¸ ì—°ê²°
    print(f"\nğŸ“¡ Qdrant ì—°ê²° ì¤‘...")
    warnings.filterwarnings(
        'ignore', message='Api key is used with an insecure connection')

    client = QdrantClient(
        url=QDRANT_URL,
        api_key=QDRANT_API_KEY,
        timeout=30,
        prefer_grpc=False)
    print("âœ… Qdrant ì—°ê²° ì™„ë£Œ")

    # 4. ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
    print(f"\nğŸ—‚ï¸  ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” ì¤‘...")
    print("   (ì»¬ë ‰ì…˜ ê²€ì¦ ì¤‘... ë„¤íŠ¸ì›Œí¬ ìƒíƒœì— ë”°ë¼ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    vectorstore = QdrantVectorStore(
        client=client,
        collection_name=COLLECTION_NAME,
        embedding=embeddings,
        content_payload_key="text"
    )
    print("âœ… ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” ì™„ë£Œ")

    return {
        "embeddings": embeddings,
        "vectorstore": vectorstore
    }


# ===========================
# LangGraph ì´ˆê¸°í™”
# ===========================
def initialize_langgraph_chatbot():
    """LangGraph ê¸°ë°˜ RAG ì±—ë´‡ ì´ˆê¸°í™” (ì¡°ê±´ë¶€ ë¶„ê¸° í¬í•¨)"""

    # ì‚¬ì „ ì¤€ë¹„: ë¦¬ì†ŒìŠ¤ ì´ˆê¸°í™”
    resources = initialize_resources()
    vectorstore = resources["vectorstore"]

    # LLM ì„¤ì •
    print(f"\nğŸ¤– LLM ì„¤ì • ì¤‘...")
    llm = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0,
        streaming=True
    )
    print("âœ… LLM ì„¤ì • ì™„ë£Œ")

    # ë…¸ë“œ ìƒì„±
    print(f"\nâš™ï¸  LangGraph ë…¸ë“œ ìƒì„± ì¤‘...")
    analyze_node = create_analyze_query_node(llm)
    clarify_node = create_clarify_node(llm)
    search_node = create_search_node(vectorstore)
    case_law_node = create_case_law_search_node(llm)
    generate_node = create_generate_node(llm)
    print("âœ… ë…¸ë“œ ìƒì„± ì™„ë£Œ (5ê°œ)")

    # StateGraph êµ¬ì„±
    print(f"\nğŸ”— LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„± ì¤‘...")
    workflow = StateGraph(AgentState)

    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("analyze", analyze_node)
    workflow.add_node("clarify", clarify_node)
    workflow.add_node("search", search_node)
    workflow.add_node("case_law_search", case_law_node)
    workflow.add_node("generate", generate_node)

    # ì—£ì§€ ì¶”ê°€
    workflow.set_entry_point("analyze")

    # ì¡°ê±´ë¶€ ë¶„ê¸° 1: ë¶„ì„ í›„ â†’ ëª…í™•í™” í•„ìš”? â†’ clarify / search
    workflow.add_conditional_edges(
        "analyze",
        route_after_analysis,
        {
            "clarify": "clarify",
            "search": "search"
        }
    )

    # clarifyëŠ” ë°”ë¡œ ì¢…ë£Œ
    workflow.add_edge("clarify", END)

    # ì¡°ê±´ë¶€ ë¶„ê¸° 2: ê²€ìƒ‰ í›„ â†’ íŒë¡€ í•„ìš”? â†’ case_law_search / generate
    workflow.add_conditional_edges(
        "search",
        route_after_search,
        {
            "case_law_search": "case_law_search",
            "generate": "generate"
        }
    )

    # íŒë¡€ ê²€ìƒ‰ í›„ â†’ ë‹µë³€ ìƒì„±
    workflow.add_edge("case_law_search", "generate")

    # ë‹µë³€ ìƒì„± í›„ â†’ ì¢…ë£Œ
    workflow.add_edge("generate", END)

    # ê·¸ë˜í”„ ì»´íŒŒì¼
    graph = workflow.compile()
    print("âœ… LangGraph êµ¬ì„± ì™„ë£Œ")

    return graph


# ===========================
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ===========================
def main():
    """LangGraph RAG ì±—ë´‡ ì‹¤í–‰ ë©”ì¸ í•¨ìˆ˜"""

    # API Key í™•ì¸
    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ ì˜¤ë¥˜: OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ .env íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
        return

    # Tavily API Key í™•ì¸ (ê²½ê³ ë§Œ)
    if not os.getenv("TAVILY_API_KEY"):
        print("âš ï¸  ê²½ê³ : TAVILY_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   íŒë¡€ ì›¹ ê²€ìƒ‰ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.\n")

    try:
        # ì±—ë´‡ ì´ˆê¸°í™”
        print("\n" + "="*60)
        print("ğŸš€ A-TEAM ë²•ë¥  RAG ì±—ë´‡ (LangGraph V1) ì´ˆê¸°í™” ì‹œì‘")
        print("="*60 + "\n")

        graph = initialize_langgraph_chatbot()

        print("\n" + "="*60)
        print("âœ… ğŸ¤– A-TEAM ë²•ë¥  ì±—ë´‡ ì¤€ë¹„ ì™„ë£Œ!")
        print("="*60)
        print("\nğŸ’¡ ì‚¬ìš© ë°©ë²•:")
        print("  - ë…¸ë™ë²•, í˜•ì‚¬ë²•, ë¯¼ì‚¬ë²• ê´€ë ¨ ì§ˆë¬¸ì— ì‘ë‹µí•©ë‹ˆë‹¤.")
        print("  - íŒë¡€ê°€ í•„ìš”í•˜ë©´ ìë™ìœ¼ë¡œ ì›¹ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
        print("  - ì§ˆë¬¸ì´ ëª¨í˜¸í•˜ë©´ êµ¬ì²´í™”ë¥¼ ìš”ì²­í•©ë‹ˆë‹¤.")
        print("  - 'exit', 'quit', 'ì¢…ë£Œ'ë¥¼ ì…ë ¥í•˜ë©´ ì¢…ë£Œë©ë‹ˆë‹¤")
        print("\nğŸ“Š ì›Œí¬í”Œë¡œìš°:")
        print("  â”Œâ”€ ì§ˆë¬¸ ë¶„ì„ â”€â”¬â”€ [ëª¨í˜¸í•¨] â†’ ëª…í™•í™” ìš”ì²­ â†’ ì¢…ë£Œ")
        print("  â”‚            â””â”€ [ëª…í™•í•¨] â†’ ë²•ë ¹ ê²€ìƒ‰ â”€â”¬â”€ [íŒë¡€ í•„ìš”] â†’ íŒë¡€ ê²€ìƒ‰ â†’ ë‹µë³€ ìƒì„±")
        print("  â”‚                                     â””â”€ [ë¶ˆí•„ìš”] â†’ ë‹µë³€ ìƒì„±")
        print("="*60 + "\n")

        # ëŒ€í™” ë£¨í”„
        while True:
            try:
                # ì‚¬ìš©ì ì…ë ¥
                user_input = input("ğŸ‘¤ User >> ").strip()

                # ì¢…ë£Œ ëª…ë ¹ í™•ì¸
                if user_input.lower() in ["exit", "quit", "ì¢…ë£Œ", "q"]:
                    print("\nğŸ‘‹ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!")
                    break

                # ë¹ˆ ì…ë ¥ ì²´í¬
                if not user_input:
                    print("âŒ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.\n")
                    continue

                # ì´ˆê¸° ìƒíƒœ ì„¤ì •
                initial_state = {
                    "messages": [HumanMessage(content=user_input)],
                    "user_query": user_input,
                    "query_analysis": None,
                    "retrieved_docs": None,
                    "case_law_results": None,
                    "generated_answer": None,
                    "next_action": None
                }

                # ê·¸ë˜í”„ ì‹¤í–‰
                print("\n" + "-"*60)
                print("ğŸ”„ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘...")
                print("-"*60 + "\n")

                result = graph.invoke(initial_state)

                # ìµœì¢… ë‹µë³€ ì¶œë ¥
                answer = result.get("generated_answer", "")
                if answer:
                    print("\n" + "="*60)
                    print("ğŸ¤– AI ë‹µë³€:")
                    print("="*60)
                    print(f"\n{answer}\n")
                    print("="*60 + "\n")
                else:
                    print("\nâš ï¸ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n")

            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!")
                break
            except Exception as e:
                print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
                print("ğŸ’¡ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.\n")
                import traceback
                traceback.print_exc()

    except Exception as e:
        print(f"\nâŒ ì±—ë´‡ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        print("ğŸ’¡ ì„¤ì •ì„ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        import traceback
        traceback.print_exc()
        raise


if __name__ == "__main__":
    main()
