"""
Qdrant Cloudì˜ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ RAG ì±—ë´‡ í‰ê°€ë¥¼ ìœ„í•œ Golden Set ìƒì„± ìŠ¤í¬ë¦½íŠ¸ (Ragas 0.4.x)

ìš”êµ¬ì‚¬í•­(ì»¤ìŠ¤í…€):
1) ì´ 20ê°œ
2) ë…¸ë™ë²• 10, ë¯¼ì‚¬ë²• 5, í˜•ì‚¬ë²• 5
3) ê° ë¶„ì•¼ë³„ ë‚œì´ë„ ë¹„ìœ¨ ê³ ê¸‰:ì¤‘ê¸‰:ì´ˆê¸‰ = 2:1:1 (ì •ìˆ˜í™”ëŠ” ë°˜ì˜¬ë¦¼ í›„ ë³´ì •)
4) ë…¸ë™ë²• ì§ˆë¬¸ì€ ë²•ë ¹ ì™¸ ë¬¸ì„œë„ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•  ìˆ˜ ìˆë„ë¡ ì§ˆë¬¸ ìƒì„±(ê°€ëŠ¥í•˜ë©´ í•´ë‹¹ í”Œë˜ê·¸ trueì¸ ì§ˆë¬¸ì„ ìš°ì„  ì„ íƒ)

êµ¬í˜„ ë°©ì‹:
- RAGASë¡œ ë¶„ì•¼ë³„ë¡œ ì¶©ë¶„íˆ í° í’€ì„ ìƒì„±
- LLMìœ¼ë¡œ (ë¶„ì•¼/ë‚œì´ë„/ë…¸ë™-ë¹„ë²•ë ¹ì°¸ê³ ê°€ëŠ¥) ë¼ë²¨ë§
- ì¿¼í„°ì— ë§ì¶° ìƒ˜í”Œë§
"""

import os
import re
import warnings
from pathlib import Path
from typing import Dict, List, Tuple

from ragas.testset import TestsetGenerator
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader, TextLoader

import pandas as pd
from dotenv import load_dotenv

# .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
warnings.filterwarnings("ignore", category=DeprecationWarning)


# -----------------------------
# ìœ í‹¸: ì¿¼í„° ê³„ì‚° (2:1:1 ë¹„ìœ¨)
# -----------------------------
def compute_difficulty_quota(n: int) -> Dict[str, int]:
    """
    ê³ ê¸‰:ì¤‘ê¸‰:ì´ˆê¸‰ = 2:1:1 ë¹„ìœ¨ì„ nê°œì— ë§ê²Œ ì •ìˆ˜ë¡œ í• ë‹¹.
    - round í›„ ì´í•© ë³´ì • ë°©ì‹.
    ë°˜í™˜ í‚¤: {"ê³ ê¸‰": x, "ì¤‘ê¸‰": y, "ì´ˆê¸‰": z}
    """
    ratio = {"ê³ ê¸‰": 2, "ì¤‘ê¸‰": 1, "ì´ˆê¸‰": 1}
    total = sum(ratio.values())
    raw = {k: n * v / total for k, v in ratio.items()}
    q = {k: int(round(val)) for k, val in raw.items()}

    # ì´í•© ë³´ì •
    diff = n - sum(q.values())
    # diff>0ì´ë©´ ê°€ì¥ í° ë¹„ìœ¨(ê³ ê¸‰)ë¶€í„° +, diff<0ì´ë©´ ê°€ì¥ í° ê²ƒë¶€í„° -
    order = ["ê³ ê¸‰", "ì¤‘ê¸‰", "ì´ˆê¸‰"]
    i = 0
    while diff != 0:
        k = order[i % len(order)]
        if diff > 0:
            q[k] += 1
            diff -= 1
        else:
            if q[k] > 0:
                q[k] -= 1
                diff += 1
        i += 1
    return q


def normalize_domain_label(s: str) -> str:
    s = (s or "").strip()
    if "ë…¸ë™" in s:
        return "ë…¸ë™ë²•"
    if "ë¯¼ì‚¬" in s:
        return "ë¯¼ì‚¬ë²•"
    if "í˜•ì‚¬" in s:
        return "í˜•ì‚¬ë²•"
    return "ê¸°íƒ€"


def normalize_level_label(s: str) -> str:
    s = (s or "").strip()
    if "ê³ ê¸‰" in s:
        return "ê³ ê¸‰"
    if "ì¤‘ê¸‰" in s:
        return "ì¤‘ê¸‰"
    if "ì´ˆê¸‰" in s:
        return "ì´ˆê¸‰"
    return "ì¤‘ê¸‰"


def parse_label_line(line: str) -> Tuple[str, str, bool]:
    """
    ê¸°ëŒ€ í˜•ì‹: "ë¶„ì•¼|ë‚œì´ë„|ë…¸ë™-ë¹„ë²•ë ¹ì°¸ê³ ê°€ëŠ¥(yes/no)"
    ì˜ˆ: "ë…¸ë™ë²•|ê³ ê¸‰|yes"
    """
    parts = [p.strip() for p in (line or "").split("|")]
    if len(parts) < 3:
        return ("ê¸°íƒ€", "ì¤‘ê¸‰", False)
    domain = normalize_domain_label(parts[0])
    level = normalize_level_label(parts[1])
    ns = parts[2].lower()
    non_statute_ok = ns in ("yes", "y", "true", "1", "ê°€ëŠ¥")
    return (domain, level, non_statute_ok)


def build_labeler_llm(model_name: str) -> ChatOpenAI:
    return ChatOpenAI(model=model_name, temperature=0)


def label_rows(df: pd.DataFrame, llm: ChatOpenAI) -> pd.DataFrame:
    """
    ê° rowì— ëŒ€í•´ (domain, difficulty, labor_non_statute_ok) ë¼ë²¨ì„ ë¶€ì—¬.
    RAGAS DF ì»¬ëŸ¼ì´ ë²„ì „ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆì–´ ë„“ê²Œ ëŒ€ì‘.
    """
    from langchain_core.prompts import ChatPromptTemplate

    prompt = ChatPromptTemplate.from_messages([
        ("system",
         "ë„ˆëŠ” ë²•ë¥  QA í‰ê°€ ë°ì´í„° ë¼ë²¨ëŸ¬ë‹¤.\n"
         "ì…ë ¥(ì§ˆë¬¸/ì •ë‹µ/ì»¨í…ìŠ¤íŠ¸ ì¼ë¶€)ì„ ë³´ê³  ë‹¤ìŒì„ íŒì •í•œë‹¤:\n"
         "1) ë¶„ì•¼: ë…¸ë™ë²•/ë¯¼ì‚¬ë²•/í˜•ì‚¬ë²•/ê¸°íƒ€\n"
         "2) ë‚œì´ë„: ì´ˆê¸‰/ì¤‘ê¸‰/ê³ ê¸‰\n"
         "3) (ë…¸ë™ë²•ì¸ ê²½ìš°) ë²•ë ¹ ì¡°ë¬¸ë§Œìœ¼ë¡œ ë‹µí•˜ê¸°ë³´ë‹¤, ì§€ì¹¨/ì‹¤ë¬´ìë£Œ/ì„œì‹/í–‰ì •í•´ì„/ê°€ì´ë“œ/ì‚¬ë‚´ê·œì • ë“±\n"
         "   'ë²•ë ¹ ì™¸ ë¬¸ì„œ' ì°¸ê³ ê°€ ìœ ë¦¬í•œ ì§ˆë¬¸ì´ë©´ yes, ì•„ë‹ˆë©´ no\n\n"
         "ì¶œë ¥ì€ ë°˜ë“œì‹œ í•œ ì¤„ë¡œë§Œ, ë‹¤ìŒ í˜•ì‹:\n"
         "ë¶„ì•¼|ë‚œì´ë„|yes/no\n"
         "ì˜ˆ: ë…¸ë™ë²•|ê³ ê¸‰|yes"),
        ("human",
         "ì§ˆë¬¸:\n{q}\n\nì •ë‹µ(ì°¸ê³ ):\n{a}\n\nì»¨í…ìŠ¤íŠ¸(ë°œì·Œ):\n{ctx}")
    ])

    def get_col(row, *names):
        for n in names:
            if n in row and pd.notna(row[n]):
                return row[n]
        return ""

    domains = []
    levels = []
    non_statutes = []

    for _, row in df.iterrows():
        q = get_col(row, "user_input", "question")
        a = get_col(row, "reference", "ground_truth", "answer")
        ctx_val = get_col(row, "contexts", "context")
        # contextsê°€ listì¼ ìˆ˜ë„ ìˆì–´ì„œ í…ìŠ¤íŠ¸ë¡œ ì¶•ì•½
        if isinstance(ctx_val, list):
            ctx = "\n---\n".join([str(x) for x in ctx_val[:3]])
        else:
            ctx = str(ctx_val)[:1500]

        chain = prompt | llm
        out = chain.invoke({"q": str(q)[:800], "a": str(a)[:1200], "ctx": ctx}).content.strip()
        domain, level, non_statute_ok = parse_label_line(out)

        domains.append(domain)
        levels.append(level)
        non_statutes.append(bool(non_statute_ok))

    df = df.copy()
    df["domain"] = domains
    df["difficulty"] = levels
    df["labor_non_statute_ok"] = non_statutes
    return df


def sample_with_quota(df: pd.DataFrame, domain_targets: Dict[str, int]) -> pd.DataFrame:
    """
    domain_targets ì˜ˆ: {"ë…¸ë™ë²•":10, "ë¯¼ì‚¬ë²•":5, "í˜•ì‚¬ë²•":5}
    ê° ë„ë©”ì¸ ë‚´ë¶€ì—ì„œ ë‚œì´ë„ ì¿¼í„°(2:1:1)ë¥¼ ê³„ì‚°í•´ì„œ ì¶©ì¡±í•˜ë„ë¡ ìƒ˜í”Œë§.
    ë…¸ë™ë²•ì€ labor_non_statute_ok == True ë¥¼ ìš°ì„  ì±„íƒ(ê°€ëŠ¥í•˜ë©´).
    """
    picked_frames = []

    for domain, n in domain_targets.items():
        dq = compute_difficulty_quota(n)
        domain_df = df[df["domain"] == domain].copy()

        # ë…¸ë™ë²•ì´ë©´ non_statute_ok ìš°ì„ ìˆœìœ„ ë¶€ì—¬
        if domain == "ë…¸ë™ë²•":
            domain_df["__priority"] = domain_df["labor_non_statute_ok"].apply(lambda x: 0 if x else 1)
            domain_df = domain_df.sort_values(["__priority"])

        for level, k in dq.items():
            sub = domain_df[domain_df["difficulty"] == level]
            if len(sub) < k:
                raise ValueError(f"ì¿¼í„° ì¶©ì¡± ì‹¤íŒ¨: {domain}/{level} í•„ìš” {k}ê°œ, ë³´ìœ  {len(sub)}ê°œ")
            picked_frames.append(sub.sample(n=k, random_state=42))

    out = pd.concat(picked_frames, ignore_index=True)
    if len(out) != sum(domain_targets.values()):
        raise ValueError("ìƒ˜í”Œë§ ê²°ê³¼ ê°œìˆ˜ê°€ ëª©í‘œì™€ ë‹¤ë¦…ë‹ˆë‹¤.")
    return out


# --------------------------------
# Qdrantì—ì„œ ë¬¸ì„œ ë¡œë“œ(ê¸°ì¡´ ìœ ì§€)
# --------------------------------
def load_documents_from_qdrant(
    collection_name: str = None,
    limit: int = 0
) -> list:
    from langchain_core.documents import Document
    from qdrant_client import QdrantClient

    qdrant_url = os.getenv("QDRANT_URL")
    qdrant_api_key = os.getenv("QDRANT_API_KEY")
    collection = collection_name or os.getenv("QDRANT_COLLECTION_NAME", "A-TEAM")

    print(f"ğŸ“‚ Qdrant DBì—ì„œ ë¬¸ì„œ ë¡œë“œ ì¤‘...")
    print(f"   Collection: {collection}")

    if qdrant_url and qdrant_api_key:
        print(f"   URL: {qdrant_url[:30]}...")
        client = QdrantClient(
            url=qdrant_url,
            api_key=qdrant_api_key,
            timeout=60
        )
    else:
        print("   Local Docker: localhost:6333")
        client = QdrantClient(host="localhost", port=6333)

    try:
        collection_info = client.get_collection(collection_name=collection)
        total_points = collection_info.points_count
        print(f"   ì´ í¬ì¸íŠ¸ ìˆ˜: {total_points}")
    except Exception as e:
        raise ConnectionError(f"Qdrant ì»¬ë ‰ì…˜ '{collection}'ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")

    documents = []
    offset = None
    batch_size = 100

    while True:
        results = client.scroll(
            collection_name=collection,
            limit=batch_size,
            offset=offset,
            with_payload=True,
            with_vectors=False
        )

        points, next_offset = results

        if not points:
            break

        for point in points:
            payload = point.payload or {}
            text = payload.get("text", "")

            if text and len(text) > 30:
                doc = Document(
                    page_content=text,
                    metadata={
                        "id": str(point.id),
                        "source": payload.get("source", ""),
                        "law_name": payload.get("law_name", ""),
                        "law_id": payload.get("law_id", ""),
                        "article_no": payload.get("article_no", ""),
                        "article_title": payload.get("article_title", ""),
                        "paragraph_no": payload.get("paragraph_no", ""),
                        "chunk_type": payload.get("chunk_type", ""),
                        "category": payload.get("category", ""),
                        "chunk_index": payload.get("chunk_index", 0),
                    }
                )
                documents.append(doc)

        offset = next_offset
        if limit > 0 and len(documents) >= limit:
            documents = documents[:limit]
            break
        if next_offset is None:
            break

    print(f"\nğŸ“„ ì´ {len(documents)}ê°œ ì²­í‚¹ëœ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ\n")
    return documents


# ---------------------------------------------------------
# [ìˆ˜ì •] ì˜¨ë„ë¥¼ ê°•ì œë¡œ 1ë¡œ ê³ ì •í•˜ëŠ” ì»¤ìŠ¤í…€ LLM í´ë˜ìŠ¤
# ---------------------------------------------------------
class ForceTemperature1ChatOpenAI(ChatOpenAI):
    def _generate(self, messages, stop=None, run_manager=None, **kwargs):
        if 'temperature' in kwargs:
            kwargs['temperature'] = 1
        return super()._generate(messages, stop=stop, run_manager=run_manager, **kwargs)

    async def _agenerate(self, messages, stop=None, run_manager=None, **kwargs):
        if 'temperature' in kwargs:
            kwargs['temperature'] = 1
        return await super()._agenerate(messages, stop=stop, run_manager=run_manager, **kwargs)


def setup_generator(model_name: str = "gpt-4o-mini") -> TestsetGenerator:
    print(f"ğŸ¤– LLM ì„¤ì • ì¤‘(ìƒì„±): {model_name}")

    generator_llm = ForceTemperature1ChatOpenAI(
        model=model_name,
        temperature=1,
    )

    embeddings = OpenAIEmbeddings(model="text-embedding-3-large")

    generator = TestsetGenerator.from_langchain(
        llm=generator_llm,
        embedding_model=embeddings
    )

    print("âœ… TestsetGenerator ì„¤ì • ì™„ë£Œ\n")
    return generator


def generate_testset(generator: TestsetGenerator, documents: list, test_size: int) -> pd.DataFrame:
    from ragas.run_config import RunConfig

    run_config = RunConfig(
        max_retries=3,
        max_wait=60,
        max_workers=4,
        timeout=120,
        exception_types=(Exception,),
    )

    testset = generator.generate_with_langchain_docs(
        documents=documents,
        testset_size=test_size,
        raise_exceptions=False,
        run_config=run_config,
    )
    return testset.to_pandas()


def split_docs_by_domain(docs: list) -> Dict[str, list]:
    """
    Qdrant payload metadata ê¸°ë°˜ìœ¼ë¡œ ëŒ€ëµì ì¸ ë¶„ì•¼ë³„ ë¬¸ì„œ ë¶„ë¦¬.
    - metadata.category ë˜ëŠ” law_name/source ë“±ì— 'ë…¸ë™/ë¯¼ì‚¬/í˜•ì‚¬' í¬í•¨ ì—¬ë¶€ë¡œ ë¶„ë¥˜
    """
    buckets = {"ë…¸ë™ë²•": [], "ë¯¼ì‚¬ë²•": [], "í˜•ì‚¬ë²•": [], "ê¸°íƒ€": []}

    def guess_domain(doc) -> str:
        meta = doc.metadata or {}
        cat = str(meta.get("category", ""))
        law = str(meta.get("law_name", ""))
        src = str(meta.get("source", ""))
        hay = f"{cat} {law} {src}"

        if re.search(r"ë…¸ë™|ê·¼ë¡œ|ì„ê¸ˆ|í•´ê³ |í‡´ì§", hay):
            return "ë…¸ë™ë²•"
        if re.search(r"ë¯¼ì‚¬|ê³„ì•½|ì†í•´ë°°ìƒ|ì±„ê¶Œ|ì†Œìœ ", hay):
            return "ë¯¼ì‚¬ë²•"
        if re.search(r"í˜•ì‚¬|ë²”ì£„|ìˆ˜ì‚¬|í˜•ë²Œ|ê³µì†Œ", hay):
            return "í˜•ì‚¬ë²•"
        return "ê¸°íƒ€"

    for d in docs:
        buckets[guess_domain(d)].append(d)
    return buckets


def make_labor_mixed_docs(labor_docs: list, max_docs: int) -> list:
    """
    ë…¸ë™ë²• ë¬¸ì„œ ì¤‘ 'ë²•ë ¹ ì™¸ ë¬¸ì„œ'ê°€ ì„ì´ë„ë¡ ê°„ë‹¨íˆ ë¯¹ìŠ¤.
    - law_nameì´ ë¹„ì–´ìˆê±°ë‚˜ chunk_typeì´ ë²•ë ¹ ì²­í¬ê°€ ì•„ë‹Œ ê²ƒë“¤ì„ non-statuteë¡œ ê°„ì£¼
    """
    statutes = []
    non_statutes = []
    for d in labor_docs:
        meta = d.metadata or {}
        law_name = str(meta.get("law_name", "")).strip()
        chunk_type = str(meta.get("chunk_type", "")).strip().lower()

        if law_name and ("law" in chunk_type or chunk_type in ("law", "statute", "ì¡°ë¬¸", "ë²•ë ¹")):
            statutes.append(d)
        elif law_name:
            # law_nameì€ ìˆëŠ”ë° chunk_typeì´ ì• ë§¤í•˜ë©´ ì¼ë‹¨ statuteë¡œ ë¶„ë¥˜
            statutes.append(d)
        else:
            non_statutes.append(d)

    # 70% statute + 30% non-statute ëª©í‘œ(ê°€ëŠ¥í•œ ë§Œí¼)
    target_non = int(max_docs * 0.3)
    target_stat = max_docs - target_non

    picked = []
    if statutes:
        picked += statutes[:min(len(statutes), target_stat)]
    if non_statutes:
        picked += non_statutes[:min(len(non_statutes), target_non)]

    # ë¶€ì¡±í•˜ë©´ ë‚˜ë¨¸ì§€ë¡œ ì±„ì›€
    if len(picked) < max_docs:
        rest = [d for d in labor_docs if d not in picked]
        picked += rest[: (max_docs - len(picked))]

    return picked[:max_docs]


def main():
    import argparse

    parser = argparse.ArgumentParser(description="RAGAS ê¸°ë°˜ Golden Set ìƒì„±(ì»¤ìŠ¤í…€ ì¿¼í„°/ë‚œì´ë„)")
    parser.add_argument('--collection', type=str, default=None)
    parser.add_argument('--sample-size', type=int, default=0, help="Qdrantì—ì„œ ê°€ì ¸ì˜¬ ë¬¸ì„œ ìˆ˜(0=ì „ì²´)")
    parser.add_argument('--model', type=str, default='gpt-4o-mini', help='ìƒì„±/ë¼ë²¨ë§ì— ì‚¬ìš©í•  LLM ëª¨ë¸')
    parser.add_argument('--output', type=str, default='golden_set_quota_20.json')
    parser.add_argument('--pool-mult', type=int, default=6, help="ë¶„ì•¼ë³„ ìƒì„± í’€ í¬ê¸° ë°°ìˆ˜(ê¸°ë³¸ 6ë°°)")
    args = parser.parse_args()

    if not os.environ.get("OPENAI_API_KEY"):
        print("âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        return

    print("=" * 60)
    print("ğŸ›ï¸  RAG í‰ê°€ìš© Golden Set ìƒì„± (RAGAS + ì¿¼í„° ìƒ˜í”Œë§)")
    print("=" * 60)

    # 1) ë¬¸ì„œ ë¡œë“œ
    all_docs = load_documents_from_qdrant(collection_name=args.collection, limit=args.sample_size)
    if not all_docs:
        print("âŒ ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. Qdrant ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return

    # 2) ë¶„ì•¼ë³„ ë¬¸ì„œ ë²„í‚·
    buckets = split_docs_by_domain(all_docs)
    for k, v in buckets.items():
        print(f"   ğŸ“Œ {k}: {len(v)} docs")

    # ëª©í‘œ ì¿¼í„°
    domain_targets = {"ë…¸ë™ë²•": 10, "ë¯¼ì‚¬ë²•": 5, "í˜•ì‚¬ë²•": 5}

    # 3) RAGAS generator ì„¤ì •
    generator = setup_generator(args.model)

    # 4) ë¶„ì•¼ë³„ë¡œ í’€ ìƒì„±(ì¶©ë¶„íˆ í¬ê²Œ)
    frames = []
    for domain, target_n in domain_targets.items():
        docs = buckets.get(domain, [])
        if not docs:
            raise ValueError(f"'{domain}' ë¬¸ì„œê°€ Qdrantì—ì„œ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. category/metadataë¥¼ í™•ì¸í•˜ì„¸ìš”.")

        # ë…¸ë™ë²•ì€ ë¹„ë²•ë ¹ ë¬¸ì„œ ì„ì´ë„ë¡ ë¯¹ìŠ¤
        if domain == "ë…¸ë™ë²•":
            docs_for_gen = make_labor_mixed_docs(docs, max_docs=min(len(docs), 300))
        else:
            docs_for_gen = docs[:min(len(docs), 300)]

        pool_n = target_n * max(2, args.pool_mult)
        print(f"\nğŸ§ª [{domain}] í’€ ìƒì„±: {pool_n}ê°œ (ëª©í‘œ {target_n})")
        df_pool = generate_testset(generator, docs_for_gen, test_size=pool_n)
        df_pool["__generated_domain_hint"] = domain
        frames.append(df_pool)

    df_all = pd.concat(frames, ignore_index=True)
    print(f"\nâœ… ì „ì²´ í’€ ìƒì„± ì™„ë£Œ: {len(df_all)} rows")

    # 5) ë¼ë²¨ë§
    labeler = build_labeler_llm(args.model)
    print("\nğŸ·ï¸  ë¼ë²¨ë§ ì¤‘(ë¶„ì•¼/ë‚œì´ë„/ë…¸ë™-ë¹„ë²•ë ¹ì°¸ê³ )...")
    df_labeled = label_rows(df_all, labeler)

    # 6) ì¿¼í„° ìƒ˜í”Œë§(ë¶€ì¡±í•˜ë©´ ì—ëŸ¬ë¡œ ì•Œë¦¼)
    print("\nğŸ¯ ì¿¼í„° ìƒ˜í”Œë§ ì¤‘...")
    df_selected = sample_with_quota(df_labeled, domain_targets)

    # 7) ì €ì¥
    script_dir = Path(__file__).parent
    output_dir = script_dir.parent / "data" / "evaluation"
    output_dir.mkdir(parents=True, exist_ok=True)

    output_path = output_dir / args.output
    df_selected.to_json(output_path, orient='records', force_ascii=False, indent=2)

    print(f"\nğŸ’¾ ì €ì¥ ì™„ë£Œ: {output_path}")
    print("\nğŸ“Š ìµœì¢… ë¶„í¬:")
    print(df_selected.groupby(["domain", "difficulty"]).size().to_string())
    print("\n(ë…¸ë™ë²•) ë¹„ë²•ë ¹ì°¸ê³ ê°€ëŠ¥ ê°œìˆ˜:", int(df_selected[df_selected["domain"] == "ë…¸ë™ë²•"]["labor_non_statute_ok"].sum()))


if __name__ == '__main__':
    main()