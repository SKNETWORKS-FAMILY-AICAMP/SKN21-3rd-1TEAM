"""
Qdrant Cloudì˜ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ RAG ì±—ë´‡ í‰ê°€ë¥¼ ìœ„í•œ Golden Set ìƒì„± ìŠ¤í¬ë¦½íŠ¸ (Ragas 0.4.x)

ìš”êµ¬ì‚¬í•­(ì»¤ìŠ¤í…€):
1) ì´ 20ê°œ
2) ë…¸ë™ë²• 10, ë¯¼ì‚¬ë²• 5, í˜•ì‚¬ë²• 5
3) ê° ë¶„ì•¼ë³„ ë‚œì´ë„ ë¹„ìœ¨ ê³ ê¸‰:ì¤‘ê¸‰:ì´ˆê¸‰ = 2:1:1 (ì •ìˆ˜í™”ëŠ” ë°˜ì˜¬ë¦¼ í›„ ë³´ì •)
4) ë…¸ë™ë²• ì§ˆë¬¸ì€ ë²•ë ¹ ì™¸ ë¬¸ì„œë„ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•  ìˆ˜ ìˆë„ë¡ ì§ˆë¬¸ ìƒì„±(ê°€ëŠ¥í•˜ë©´ í•´ë‹¹ í”Œë˜ê·¸ trueì¸ ì§ˆë¬¸ì„ ìš°ì„  ì„ íƒ)

êµ¬í˜„ ë°©ì‹:
- RAGASë¡œ ë¶„ì•¼ë³„ë¡œ ì¶©ë¶„íˆ í° í’€ì„ ìƒì„±
- LLMìœ¼ë¡œ (ë¶„ì•¼/ë‚œì´ë„/ë…¸ë™-ë¹„ë²•ë ¹ì°¸ê³ ê°€ëŠ¥) ë¼ë²¨ë§
- ì¿¼í„°ì— ë§ì¶° ìƒ˜í”Œë§
"""

import os
import re
import warnings
from pathlib import Path
from typing import Dict, List, Tuple

from ragas.testset import TestsetGenerator
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader, TextLoader

import pandas as pd
from dotenv import load_dotenv

# .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
warnings.filterwarnings("ignore", category=DeprecationWarning)


# -----------------------------
# ìœ í‹¸: ì¿¼í„° ê³„ì‚° (2:1:1 ë¹„ìœ¨)
# -----------------------------
def compute_difficulty_quota(n: int) -> Dict[str, int]:
    """
    ê³ ê¸‰:ì¤‘ê¸‰:ì´ˆê¸‰ = 2:1:1 ë¹„ìœ¨ì„ nê°œì— ë§ê²Œ ì •ìˆ˜ë¡œ í• ë‹¹.
    - round í›„ ì´í•© ë³´ì • ë°©ì‹.
    ë°˜í™˜ í‚¤: {"ê³ ê¸‰": x, "ì¤‘ê¸‰": y, "ì´ˆê¸‰": z}
    """
    ratio = {"ê³ ê¸‰": 2, "ì¤‘ê¸‰": 1, "ì´ˆê¸‰": 1}
    total = sum(ratio.values())
    raw = {k: n * v / total for k, v in ratio.items()}
    q = {k: int(round(val)) for k, val in raw.items()}

    # ì´í•© ë³´ì •
    diff = n - sum(q.values())
    # diff>0ì´ë©´ ê°€ì¥ í° ë¹„ìœ¨(ê³ ê¸‰)ë¶€í„° +, diff<0ì´ë©´ ê°€ì¥ í° ê²ƒë¶€í„° -
    order = ["ê³ ê¸‰", "ì¤‘ê¸‰", "ì´ˆê¸‰"]
    i = 0
    while diff != 0:
        k = order[i % len(order)]
        if diff > 0:
            q[k] += 1
            diff -= 1
        else:
            if q[k] > 0:
                q[k] -= 1
                diff += 1
        i += 1
    return q


def normalize_domain_label(s: str) -> str:
    s = (s or "").strip()
    if "ë…¸ë™" in s:
        return "ë…¸ë™ë²•"
    if "ë¯¼ì‚¬" in s:
        return "ë¯¼ì‚¬ë²•"
    if "í˜•ì‚¬" in s:
        return "í˜•ì‚¬ë²•"
    return "ê¸°íƒ€"


def normalize_level_label(s: str) -> str:
    s = (s or "").strip()
    if "ê³ ê¸‰" in s:
        return "ê³ ê¸‰"
    if "ì¤‘ê¸‰" in s:
        return "ì¤‘ê¸‰"
    if "ì´ˆê¸‰" in s:
        return "ì´ˆê¸‰"
    return "ì¤‘ê¸‰"


def parse_label_line(line: str) -> Tuple[str, str, bool]:
    """
    ê¸°ëŒ€ í˜•ì‹: "ë¶„ì•¼|ë‚œì´ë„|ë…¸ë™-ë¹„ë²•ë ¹ì°¸ê³ ê°€ëŠ¥(yes/no)"
    ì˜ˆ: "ë…¸ë™ë²•|ê³ ê¸‰|yes"
    """
    parts = [p.strip() for p in (line or "").split("|")]
    if len(parts) < 3:
        return ("ê¸°íƒ€", "ì¤‘ê¸‰", False)
    domain = normalize_domain_label(parts[0])
    level = normalize_level_label(parts[1])
    ns = parts[2].lower()
    non_statute_ok = ns in ("yes", "y", "true", "1", "ê°€ëŠ¥")
    return (domain, level, non_statute_ok)


def build_labeler_llm(model_name: str) -> ChatOpenAI:
    return ChatOpenAI(model=model_name, temperature=0)


def label_rows(df: pd.DataFrame, llm: ChatOpenAI) -> pd.DataFrame:
    """
    ê° rowì— ëŒ€í•´ (domain, difficulty, labor_non_statute_ok) ë¼ë²¨ì„ ë¶€ì—¬.
    RAGAS DF ì»¬ëŸ¼ì´ ë²„ì „ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆì–´ ë„“ê²Œ ëŒ€ì‘.
    """
    from langchain_core.prompts import ChatPromptTemplate

    prompt = ChatPromptTemplate.from_messages([
        ("system",
         "ë„ˆëŠ” ë²•ë¥  QA í‰ê°€ ë°ì´í„° ë¼ë²¨ëŸ¬ë‹¤.\n"
         "ì…ë ¥(ì§ˆë¬¸/ì •ë‹µ/ì»¨í…ìŠ¤íŠ¸ ì¼ë¶€)ì„ ë³´ê³  ë‹¤ìŒì„ íŒì •í•œë‹¤:\n"
         "1) ë¶„ì•¼: ë…¸ë™ë²•/ë¯¼ì‚¬ë²•/í˜•ì‚¬ë²•/ê¸°íƒ€\n"
         "2) ë‚œì´ë„: ì´ˆê¸‰/ì¤‘ê¸‰/ê³ ê¸‰\n"
         "3) (ë…¸ë™ë²•ì¸ ê²½ìš°) ë²•ë ¹ ì¡°ë¬¸ë§Œìœ¼ë¡œ ë‹µí•˜ê¸°ë³´ë‹¤, ì§€ì¹¨/ì‹¤ë¬´ìë£Œ/ì„œì‹/í–‰ì •í•´ì„/ê°€ì´ë“œ/ì‚¬ë‚´ê·œì • ë“±\n"
         "   'ë²•ë ¹ ì™¸ ë¬¸ì„œ' ì°¸ê³ ê°€ ìœ ë¦¬í•œ ì§ˆë¬¸ì´ë©´ yes, ì•„ë‹ˆë©´ no\n\n"
         "ì¶œë ¥ì€ ë°˜ë“œì‹œ í•œ ì¤„ë¡œë§Œ, ë‹¤ìŒ í˜•ì‹:\n"
         "ë¶„ì•¼|ë‚œì´ë„|yes/no\n"
         "ì˜ˆ: ë…¸ë™ë²•|ê³ ê¸‰|yes"),
        ("human",
         "ì§ˆë¬¸:\n{q}\n\nì •ë‹µ(ì°¸ê³ ):\n{a}\n\nì»¨í…ìŠ¤íŠ¸(ë°œì·Œ):\n{ctx}")
    ])

    def get_col(row, *names):
        for n in names:
            if n in row and pd.notna(row[n]):
                return row[n]
        return ""

    domains = []
    levels = []
    non_statutes = []

    for _, row in df.iterrows():
        q = get_col(row, "user_input", "question")
        a = get_col(row, "reference", "ground_truth", "answer")
        ctx_val = get_col(row, "contexts", "context")
        # contextsê°€ listì¼ ìˆ˜ë„ ìˆì–´ì„œ í…ìŠ¤íŠ¸ë¡œ ì¶•ì•½
        if isinstance(ctx_val, list):
            ctx = "\n---\n".join([str(x) for x in ctx_val[:3]])
        else:
            ctx = str(ctx_val)[:1500]

        chain = prompt | llm
        out = chain.invoke({"q": str(q)[:800], "a": str(a)[:1200], "ctx": ctx}).content.strip()
        domain, level, non_statute_ok = parse_label_line(out)

        domains.append(domain)
        levels.append(level)
        non_statutes.append(bool(non_statute_ok))

    df = df.copy()
    df["domain"] = domains
    df["difficulty"] = levels
    df["labor_non_statute_ok"] = non_statutes
    return df


def sample_with_quota(df: pd.DataFrame, domain_targets: Dict[str, int]) -> pd.DataFrame:
    """
    domain_targets ì˜ˆ: {"ë…¸ë™ë²•":10, "ë¯¼ì‚¬ë²•":5, "í˜•ì‚¬ë²•":5}
    ê° ë„ë©”ì¸ ë‚´ë¶€ì—ì„œ ë‚œì´ë„ ì¿¼í„°(2:1:1)ë¥¼ ê³„ì‚°í•´ì„œ ì¶©ì¡±í•˜ë„ë¡ ìƒ˜í”Œë§.
    ë…¸ë™ë²•ì€ labor_non_statute_ok == True ë¥¼ ìš°ì„  ì±„íƒ(ê°€ëŠ¥í•˜ë©´).
    
    ì¿¼í„° ë¶€ì¡± ì‹œ: ë‹¤ë¥¸ ë‚œì´ë„ì—ì„œ ëŒ€ì²´ ìƒ˜í”Œë§ (ìœ ì—° ëª¨ë“œ)
    """
    picked_frames = []

    for domain, n in domain_targets.items():
        dq = compute_difficulty_quota(n)
        domain_df = df[df["domain"] == domain].copy()

        if len(domain_df) == 0:
            print(f"âš ï¸  [{domain}] í•´ë‹¹ ë¶„ì•¼ ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤. ìŠ¤í‚µí•©ë‹ˆë‹¤.")
            continue

        # ë…¸ë™ë²•ì´ë©´ non_statute_ok ìš°ì„ ìˆœìœ„ ë¶€ì—¬
        if domain == "ë…¸ë™ë²•":
            domain_df["__priority"] = domain_df["labor_non_statute_ok"].apply(lambda x: 0 if x else 1)
            domain_df = domain_df.sort_values(["__priority"])

        picked_ids = set()
        domain_picked = []
        shortage = 0  # ë¶€ì¡±ë¶„ ëˆ„ì 

        for level, k in dq.items():
            sub = domain_df[(domain_df["difficulty"] == level) & (~domain_df.index.isin(picked_ids))]
            available = len(sub)
            take = min(available, k)
            
            if take < k:
                shortage += (k - take)
                print(f"âš ï¸  [{domain}/{level}] í•„ìš” {k}ê°œ, ë³´ìœ  {available}ê°œ â†’ {take}ê°œ ìƒ˜í”Œë§ (ë¶€ì¡± {k - take}ê°œ)")
            
            if take > 0:
                sampled = sub.sample(n=take, random_state=42)
                domain_picked.append(sampled)
                picked_ids.update(sampled.index)

        # ë¶€ì¡±ë¶„ì„ ë‹¤ë¥¸ ë‚œì´ë„ì—ì„œ ë³´ì¶©
        if shortage > 0:
            remaining = domain_df[~domain_df.index.isin(picked_ids)]
            è£œå…… = min(len(remaining), shortage)
            if è£œå…… > 0:
                print(f"   â†³ [{domain}] ë¶€ì¡±ë¶„ {shortage}ê°œ ì¤‘ {è£œå……}ê°œë¥¼ ë‹¤ë¥¸ ë‚œì´ë„ì—ì„œ ë³´ì¶©")
                è£œå……_sampled = remaining.sample(n=è£œå……, random_state=42)
                domain_picked.append(è£œå……_sampled)
                picked_ids.update(è£œå……_sampled.index)

        if domain_picked:
            picked_frames.extend(domain_picked)

    if not picked_frames:
        raise ValueError("ìƒ˜í”Œë§ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ìƒì„±ëœ í’€ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    out = pd.concat(picked_frames, ignore_index=True)
    
    expected = sum(domain_targets.values())
    if len(out) != expected:
        print(f"âš ï¸  ìµœì¢… ìƒ˜í”Œ ìˆ˜: {len(out)}ê°œ (ëª©í‘œ {expected}ê°œ)")
    
    return out


# --------------------------------
# Qdrantì—ì„œ ë¬¸ì„œ ë¡œë“œ (ìµœì í™” ë²„ì „)
# --------------------------------
def load_documents_from_qdrant_by_domain(
    collection_name: str = None,
    docs_per_domain: int = 500
) -> Dict[str, list]:
    """
    Qdrantì—ì„œ ë¶„ì•¼ë³„ë¡œ í•„ìš”í•œ ë¬¸ì„œë§Œ ìƒ˜í”Œë§í•´ì„œ ë¡œë“œ.
    ì „ì²´ ìŠ¤ìº” ëŒ€ì‹  ëœë¤ ìƒ˜í”Œë§ìœ¼ë¡œ ë¹ ë¥´ê²Œ ê°€ì ¸ì˜´.
    """
    from langchain_core.documents import Document
    from qdrant_client import QdrantClient
    from qdrant_client.models import Filter, FieldCondition, MatchAny
    import random

    qdrant_url = os.getenv("QDRANT_URL")
    qdrant_api_key = os.getenv("QDRANT_API_KEY")
    collection = collection_name or os.getenv("QDRANT_COLLECTION_NAME", "A-TEAM")

    print(f"ğŸ“‚ Qdrant DBì—ì„œ ë¶„ì•¼ë³„ ë¬¸ì„œ ìƒ˜í”Œë§ ì¤‘...")
    print(f"   Collection: {collection}")
    print(f"   ë¶„ì•¼ë³„ ìµœëŒ€: {docs_per_domain}ê°œ")

    if qdrant_url and qdrant_api_key:
        print(f"   URL: {qdrant_url[:30]}...")
        client = QdrantClient(
            url=qdrant_url,
            api_key=qdrant_api_key,
            timeout=60
        )
    else:
        print("   Local Docker: localhost:6333")
        client = QdrantClient(host="localhost", port=6333)

    try:
        collection_info = client.get_collection(collection_name=collection)
        total_points = collection_info.points_count
        print(f"   ì´ í¬ì¸íŠ¸ ìˆ˜: {total_points}")
    except Exception as e:
        raise ConnectionError(f"Qdrant ì»¬ë ‰ì…˜ '{collection}'ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")

    # ë¶„ì•¼ë³„ í‚¤ì›Œë“œ íŒ¨í„´
    domain_patterns = {
        "ë…¸ë™ë²•": ["ë…¸ë™", "ê·¼ë¡œ", "ì„ê¸ˆ", "í•´ê³ ", "í‡´ì§", "ê³ ìš©", "ì‚°ì¬", "ì‚°ì—…ì¬í•´", "ê·¼ë¡œê¸°ì¤€"],
        "ë¯¼ì‚¬ë²•": ["ë¯¼ì‚¬", "ê³„ì•½", "ì†í•´ë°°ìƒ", "ì±„ê¶Œ", "ì†Œìœ ", "ë¯¼ë²•", "ë¶€ë™ì‚°", "ì„ëŒ€ì°¨"],
        "í˜•ì‚¬ë²•": ["í˜•ì‚¬", "ë²”ì£„", "ìˆ˜ì‚¬", "í˜•ë²Œ", "ê³µì†Œ", "í˜•ë²•", "ì²˜ë²Œ", "í”¼ì˜ì"],
    }

    buckets = {"ë…¸ë™ë²•": [], "ë¯¼ì‚¬ë²•": [], "í˜•ì‚¬ë²•": [], "ê¸°íƒ€": []}
    
    # ë‹¤ì–‘ì„± í™•ë³´ë¥¼ ìœ„í•œ ë¶„ì‚° ìƒ˜í”Œë§
    sample_size = min(docs_per_domain * 6, 5000)
    batch_size = 20  # ì‘ì€ ë°°ì¹˜ë¡œ ë‹¤ì–‘í•œ ìœ„ì¹˜ì—ì„œ ìƒ˜í”Œë§
    num_sampling_points = max(100, sample_size // batch_size)  # ìµœì†Œ 100ê°œ ìœ„ì¹˜
    
    print(f"\n   ğŸ“¥ ë¶„ì‚° ëœë¤ ìƒ˜í”Œë§ ì¤‘ ({sample_size}ê°œ ëª©í‘œ, {num_sampling_points}ê°œ ìœ„ì¹˜)")
    
    # ì „ì²´ ë²”ìœ„ë¥¼ ê· ë“± ë¶„í•  í›„ ê° êµ¬ê°„ì—ì„œ ëœë¤ ìƒ˜í”Œë§
    all_sampled = []
    
    if total_points > sample_size:
        # ì „ì²´ ë²”ìœ„ë¥¼ num_sampling_pointsê°œ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ„ê¸°
        segment_size = total_points // num_sampling_points
        
        for i in range(num_sampling_points):
            if len(all_sampled) >= sample_size:
                break
            
            # ê° êµ¬ê°„ ë‚´ì—ì„œ ëœë¤ ì˜¤í”„ì…‹ ì„ íƒ
            segment_start = i * segment_size
            segment_end = min((i + 1) * segment_size, total_points)
            
            if segment_end - segment_start < batch_size:
                random_offset = segment_start
            else:
                random_offset = random.randint(segment_start, max(segment_start, segment_end - batch_size))
            
            try:
                results = client.scroll(
                    collection_name=collection,
                    limit=batch_size,
                    offset=random_offset,
                    with_payload=True,
                    with_vectors=False
                )
                
                points, _ = results
                
                for point in points:
                    if len(all_sampled) >= sample_size:
                        break
                    
                    payload = point.payload or {}
                    text = payload.get("text", "")
                    
                    if text and len(text) > 30:
                        doc = Document(
                            page_content=text,
                            metadata={
                                "id": str(point.id),
                                "source": payload.get("source", ""),
                                "law_name": payload.get("law_name", ""),
                                "law_id": payload.get("law_id", ""),
                                "article_no": payload.get("article_no", ""),
                                "article_title": payload.get("article_title", ""),
                                "paragraph_no": payload.get("paragraph_no", ""),
                                "chunk_type": payload.get("chunk_type", ""),
                                "category": payload.get("category", ""),
                                "chunk_index": payload.get("chunk_index", 0),
                            }
                        )
                        all_sampled.append(doc)
            except Exception as e:
                # ì¼ë¶€ ì˜¤í”„ì…‹ì—ì„œ ì˜¤ë¥˜ ë°œìƒ ì‹œ ìŠ¤í‚µ
                continue
    else:
        # ì „ì²´ ë°ì´í„°ê°€ ìƒ˜í”Œ ì‚¬ì´ì¦ˆë³´ë‹¤ ì‘ìœ¼ë©´ ì „ì²´ ë¡œë“œ
        offset = None
        while True:
            results = client.scroll(
                collection_name=collection,
                limit=100,
                offset=offset,
                with_payload=True,
                with_vectors=False
            )
            
            points, next_offset = results
            if not points:
                break
                
            for point in points:
                payload = point.payload or {}
                text = payload.get("text", "")
                
                if text and len(text) > 30:
                    doc = Document(
                        page_content=text,
                        metadata={
                            "id": str(point.id),
                            "source": payload.get("source", ""),
                            "law_name": payload.get("law_name", ""),
                            "law_id": payload.get("law_id", ""),
                            "article_no": payload.get("article_no", ""),
                            "article_title": payload.get("article_title", ""),
                            "paragraph_no": payload.get("paragraph_no", ""),
                            "chunk_type": payload.get("chunk_type", ""),
                            "category": payload.get("category", ""),
                            "chunk_index": payload.get("chunk_index", 0),
                        }
                    )
                    all_sampled.append(doc)
            
            offset = next_offset
            if offset is None:
                break

    print(f"   âœ… {len(all_sampled)}ê°œ ë¬¸ì„œ ìƒ˜í”Œë§ ì™„ë£Œ")

    # ë¶„ì•¼ë³„ ë¶„ë¥˜ (ë‹¤ì–‘ì„± í™•ë³´ë¥¼ ìœ„í•´ ë¶„ì•¼ë³„ë¡œ ê· ë“±í•˜ê²Œ ë¶„ì‚°)
    def classify_domain(doc) -> str:
        meta = doc.metadata or {}
        cat = str(meta.get("category", ""))
        law = str(meta.get("law_name", ""))
        src = str(meta.get("source", ""))
        text_preview = doc.page_content[:200] if doc.page_content else ""
        hay = f"{cat} {law} {src} {text_preview}"

        for domain, keywords in domain_patterns.items():
            if any(kw in hay for kw in keywords):
                return domain
        return "ê¸°íƒ€"

    # ì…”í”Œí•´ì„œ ìˆœì„œ ëœë¤í™” (ê°™ì€ ë²•ë¥ ì´ ì—°ì†ìœ¼ë¡œ ì˜¤ëŠ” ê²ƒ ë°©ì§€)
    random.shuffle(all_sampled)
    
    for doc in all_sampled:
        domain = classify_domain(doc)
        if len(buckets[domain]) < docs_per_domain:
            buckets[domain].append(doc)
    
    # ê° ë¶„ì•¼ ë‚´ì—ì„œë„ ë‹¤ì‹œ ì…”í”Œ (ë²•ë¥ ëª… ê¸°ì¤€ ë‹¤ì–‘ì„± í™•ë³´)
    for domain in buckets:
        random.shuffle(buckets[domain])

    print(f"\nğŸ“„ ë¶„ì•¼ë³„ ë¡œë“œ ì™„ë£Œ:")
    for k, v in buckets.items():
        if v:
            # í•´ë‹¹ ë¶„ì•¼ì˜ ë²•ë¥ ëª… ë‹¤ì–‘ì„± ì²´í¬
            law_names = set(doc.metadata.get("law_name", "ì•Œ ìˆ˜ ì—†ìŒ") for doc in v if doc.metadata.get("law_name"))
            print(f"   ğŸ“Œ {k}: {len(v)} docs (ë²•ë¥  {len(law_names)}ì¢…ë¥˜)")
        else:
            print(f"   ğŸ“Œ {k}: {len(v)} docs")

    return buckets


# ---------------------------------------------------------
# [ìˆ˜ì •] ì˜¨ë„ë¥¼ ê°•ì œë¡œ 1ë¡œ ê³ ì •í•˜ëŠ” ì»¤ìŠ¤í…€ LLM í´ë˜ìŠ¤
# ---------------------------------------------------------
class ForceTemperature1ChatOpenAI(ChatOpenAI):
    def _generate(self, messages, stop=None, run_manager=None, **kwargs):
        if 'temperature' in kwargs:
            kwargs['temperature'] = 1
        return super()._generate(messages, stop=stop, run_manager=run_manager, **kwargs)

    async def _agenerate(self, messages, stop=None, run_manager=None, **kwargs):
        if 'temperature' in kwargs:
            kwargs['temperature'] = 1
        return await super()._agenerate(messages, stop=stop, run_manager=run_manager, **kwargs)


def setup_generator(model_name: str = "gpt-4o-mini") -> TestsetGenerator:
    print(f"ğŸ¤– LLM ì„¤ì • ì¤‘(ìƒì„±): {model_name}")

    generator_llm = ForceTemperature1ChatOpenAI(
        model=model_name,
        temperature=1,
    )

    embeddings = OpenAIEmbeddings(model="text-embedding-3-large")

    generator = TestsetGenerator.from_langchain(
        llm=generator_llm,
        embedding_model=embeddings
    )

    print("âœ… TestsetGenerator ì„¤ì • ì™„ë£Œ\n")
    return generator


def generate_testset(generator: TestsetGenerator, documents: list, test_size: int) -> pd.DataFrame:
    from ragas.run_config import RunConfig

    run_config = RunConfig(
        max_retries=3,
        max_wait=60,
        max_workers=4,
        timeout=120,
        exception_types=(Exception,),
    )

    testset = generator.generate_with_langchain_docs(
        documents=documents,
        testset_size=test_size,
        raise_exceptions=False,
        run_config=run_config,
    )
    return testset.to_pandas()


def split_docs_by_domain(docs: list) -> Dict[str, list]:
    """
    Qdrant payload metadata ê¸°ë°˜ìœ¼ë¡œ ëŒ€ëµì ì¸ ë¶„ì•¼ë³„ ë¬¸ì„œ ë¶„ë¦¬.
    - metadata.category ë˜ëŠ” law_name/source ë“±ì— 'ë…¸ë™/ë¯¼ì‚¬/í˜•ì‚¬' í¬í•¨ ì—¬ë¶€ë¡œ ë¶„ë¥˜
    """
    buckets = {"ë…¸ë™ë²•": [], "ë¯¼ì‚¬ë²•": [], "í˜•ì‚¬ë²•": [], "ê¸°íƒ€": []}

    def guess_domain(doc) -> str:
        meta = doc.metadata or {}
        cat = str(meta.get("category", ""))
        law = str(meta.get("law_name", ""))
        src = str(meta.get("source", ""))
        hay = f"{cat} {law} {src}"

        if re.search(r"ë…¸ë™|ê·¼ë¡œ|ì„ê¸ˆ|í•´ê³ |í‡´ì§", hay):
            return "ë…¸ë™ë²•"
        if re.search(r"ë¯¼ì‚¬|ê³„ì•½|ì†í•´ë°°ìƒ|ì±„ê¶Œ|ì†Œìœ ", hay):
            return "ë¯¼ì‚¬ë²•"
        if re.search(r"í˜•ì‚¬|ë²”ì£„|ìˆ˜ì‚¬|í˜•ë²Œ|ê³µì†Œ", hay):
            return "í˜•ì‚¬ë²•"
        return "ê¸°íƒ€"

    for d in docs:
        buckets[guess_domain(d)].append(d)
    return buckets


def make_labor_mixed_docs(labor_docs: list, max_docs: int) -> list:
    """
    ë…¸ë™ë²• ë¬¸ì„œ ì¤‘ 'ë²•ë ¹ ì™¸ ë¬¸ì„œ'ê°€ ì„ì´ë„ë¡ ê°„ë‹¨íˆ ë¯¹ìŠ¤.
    - law_nameì´ ë¹„ì–´ìˆê±°ë‚˜ chunk_typeì´ ë²•ë ¹ ì²­í¬ê°€ ì•„ë‹Œ ê²ƒë“¤ì„ non-statuteë¡œ ê°„ì£¼
    """
    statutes = []
    non_statutes = []
    for d in labor_docs:
        meta = d.metadata or {}
        law_name = str(meta.get("law_name", "")).strip()
        chunk_type = str(meta.get("chunk_type", "")).strip().lower()

        if law_name and ("law" in chunk_type or chunk_type in ("law", "statute", "ì¡°ë¬¸", "ë²•ë ¹")):
            statutes.append(d)
        elif law_name:
            # law_nameì€ ìˆëŠ”ë° chunk_typeì´ ì• ë§¤í•˜ë©´ ì¼ë‹¨ statuteë¡œ ë¶„ë¥˜
            statutes.append(d)
        else:
            non_statutes.append(d)

    # 70% statute + 30% non-statute ëª©í‘œ(ê°€ëŠ¥í•œ ë§Œí¼)
    target_non = int(max_docs * 0.3)
    target_stat = max_docs - target_non

    picked = []
    if statutes:
        picked += statutes[:min(len(statutes), target_stat)]
    if non_statutes:
        picked += non_statutes[:min(len(non_statutes), target_non)]

    # ë¶€ì¡±í•˜ë©´ ë‚˜ë¨¸ì§€ë¡œ ì±„ì›€
    if len(picked) < max_docs:
        rest = [d for d in labor_docs if d not in picked]
        picked += rest[: (max_docs - len(picked))]

    return picked[:max_docs]


def main():
    import argparse

    parser = argparse.ArgumentParser(description="RAGAS ê¸°ë°˜ Golden Set ìƒì„±(ì»¤ìŠ¤í…€ ì¿¼í„°/ë‚œì´ë„)")
    parser.add_argument('--collection', type=str, default=None)
    parser.add_argument('--docs-per-domain', type=int, default=500, help="ë¶„ì•¼ë³„ ìƒ˜í”Œë§í•  ë¬¸ì„œ ìˆ˜(ê¸°ë³¸ 500)")
    parser.add_argument('--model', type=str, default='gpt-4o-mini', help='ìƒì„±/ë¼ë²¨ë§ì— ì‚¬ìš©í•  LLM ëª¨ë¸')
    parser.add_argument('--output', type=str, default='golden_set_quota_20.json')
    parser.add_argument('--pool-mult', type=int, default=6, help="ë¶„ì•¼ë³„ ìƒì„± í’€ í¬ê¸° ë°°ìˆ˜(ê¸°ë³¸ 6ë°°)")
    args = parser.parse_args()

    if not os.environ.get("OPENAI_API_KEY"):
        print("âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        return

    print("=" * 60)
    print("ğŸ›ï¸  RAG í‰ê°€ìš© Golden Set ìƒì„± (RAGAS + ì¿¼í„° ìƒ˜í”Œë§)")
    print("=" * 60)

    # 1) ë¶„ì•¼ë³„ ë¬¸ì„œ ì§ì ‘ ìƒ˜í”Œë§ (ìµœì í™”)
    buckets = load_documents_from_qdrant_by_domain(
        collection_name=args.collection, 
        docs_per_domain=args.docs_per_domain
    )
    
    total_docs = sum(len(v) for v in buckets.values())
    if total_docs == 0:
        print("âŒ ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. Qdrant ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return

    # ëª©í‘œ ì¿¼í„°
    domain_targets = {"ë…¸ë™ë²•": 10, "ë¯¼ì‚¬ë²•": 5, "í˜•ì‚¬ë²•": 5}

    # 2) RAGAS generator ì„¤ì •
    generator = setup_generator(args.model)

    # 3) ë¶„ì•¼ë³„ë¡œ í’€ ìƒì„±(ì¶©ë¶„íˆ í¬ê²Œ)
    frames = []
    for domain, target_n in domain_targets.items():
        docs = buckets.get(domain, [])
        if not docs:
            print(f"âš ï¸  '{domain}' ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ìŠ¤í‚µí•©ë‹ˆë‹¤.")
            continue

        # ë…¸ë™ë²•ì€ ë¹„ë²•ë ¹ ë¬¸ì„œ ì„ì´ë„ë¡ ë¯¹ìŠ¤
        if domain == "ë…¸ë™ë²•":
            docs_for_gen = make_labor_mixed_docs(docs, max_docs=min(len(docs), 300))
        else:
            docs_for_gen = docs[:min(len(docs), 300)]

        pool_n = target_n * max(2, args.pool_mult)
        print(f"\nğŸ§ª [{domain}] í’€ ìƒì„±: {pool_n}ê°œ (ëª©í‘œ {target_n})")
        df_pool = generate_testset(generator, docs_for_gen, test_size=pool_n)
        df_pool["__generated_domain_hint"] = domain
        frames.append(df_pool)

    df_all = pd.concat(frames, ignore_index=True)
    print(f"\nâœ… ì „ì²´ í’€ ìƒì„± ì™„ë£Œ: {len(df_all)} rows")

    # 5) ë¼ë²¨ë§
    labeler = build_labeler_llm(args.model)
    print("\nğŸ·ï¸  ë¼ë²¨ë§ ì¤‘(ë¶„ì•¼/ë‚œì´ë„/ë…¸ë™-ë¹„ë²•ë ¹ì°¸ê³ )...")
    df_labeled = label_rows(df_all, labeler)

    # 6) ì¿¼í„° ìƒ˜í”Œë§(ë¶€ì¡±í•˜ë©´ ì—ëŸ¬ë¡œ ì•Œë¦¼)
    print("\nğŸ¯ ì¿¼í„° ìƒ˜í”Œë§ ì¤‘...")
    df_selected = sample_with_quota(df_labeled, domain_targets)

    # 7) ì €ì¥
    script_dir = Path(__file__).parent
    output_dir = script_dir.parent / "data" / "evaluation"
    output_dir.mkdir(parents=True, exist_ok=True)

    output_path = output_dir / args.output
    df_selected.to_json(output_path, orient='records', force_ascii=False, indent=2)

    print(f"\nğŸ’¾ ì €ì¥ ì™„ë£Œ: {output_path}")
    print("\nğŸ“Š ìµœì¢… ë¶„í¬:")
    print(df_selected.groupby(["domain", "difficulty"]).size().to_string())
    print("\n(ë…¸ë™ë²•) ë¹„ë²•ë ¹ì°¸ê³ ê°€ëŠ¥ ê°œìˆ˜:", int(df_selected[df_selected["domain"] == "ë…¸ë™ë²•"]["labor_non_statute_ok"].sum()))


if __name__ == '__main__':
    main()