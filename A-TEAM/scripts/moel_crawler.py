"""
ë…¸ë™ê³ ìš©ë¶€ í–‰ì •í•´ì„(ì§ˆì˜íšŒì‹œ) í¬ë¡¤ëŸ¬
URL: https://www.moel.go.kr/info/publicdata/qnrinfo/qnrInfoList.do
"""

import requests
from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright
import json
import time
import re
from datetime import datetime

# ì„¸ì…˜
session = requests.Session()

headers = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
}

BASE_URL = "https://www.moel.go.kr"
LIST_URL = "https://www.moel.go.kr/info/publicdata/qnrinfo/qnrInfoList.do"


def clean_text(text):
    """
    ì§ˆë¬¸/ë‹µë³€ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
    - HTML íƒœê·¸ ì œê±°
    - íŠ¹ìˆ˜ ê¸°í˜¸ ì •ë¦¬ (ã€‡, â—‹, ã…‡, >, - ë“±)
    - ì—°ì† ê³µë°±/ì¤„ë°”ê¿ˆ ì •ë¦¬
    """
    if not text:
        return ""

    # HTML íƒœê·¸ ì œê±°
    text = re.sub(r'<BR\s*/?>', '\n', text, flags=re.IGNORECASE)
    text = re.sub(r'<[^>]+>', '', text)

    # HTML ì—”í‹°í‹° ë³€í™˜
    text = text.replace('&lt;', '<').replace('&gt;', '>')
    text = text.replace('&amp;', '&').replace('&nbsp;', ' ')
    text = text.replace('&quot;', '"').replace('&#39;', "'")

    # íŠ¹ìˆ˜ ê¸°í˜¸ ì •ë¦¬ (ì¤„ ì‹œì‘ì˜ ë¶ˆë¦¿ í¬ì¸íŠ¸ë“¤)
    # ã€‡, â—‹, ã…‡, â—¦, â—, Â· ë“±ì„ ì¼ê´€ë˜ê²Œ ì²˜ë¦¬
    text = re.sub(r'^[\s]*[ã€‡â—‹ã…‡â—¦â—Â·â–¶â–ºâ—‡â—†â– â–¡â–ªâ–«]\s*', 'â€¢ ', text, flags=re.MULTILINE)

    # > ê¸°í˜¸ (ì¸ìš© í‘œì‹œ) ì •ë¦¬
    text = re.sub(r'^[\s]*>\s*', '', text, flags=re.MULTILINE)

    # - ë¡œ ì‹œì‘í•˜ëŠ” ì¤„ ì •ë¦¬
    text = re.sub(r'^[\s]*-\s+', '- ', text, flags=re.MULTILINE)

    # ì—°ì† ì¤„ë°”ê¿ˆ ì •ë¦¬ (3ê°œ ì´ìƒ â†’ 2ê°œ)
    text = re.sub(r'\n{3,}', '\n\n', text)

    # ì—°ì† ê³µë°± ì •ë¦¬
    text = re.sub(r'[ \t]+', ' ', text)

    # ì¤„ ì•ë’¤ ê³µë°± ì •ë¦¬
    lines = [line.strip() for line in text.split('\n')]
    text = '\n'.join(lines)

    return text.strip()


def get_post_list_page(page=1):
    """ê²Œì‹œê¸€ ëª©ë¡ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° (GET with pageIndex)"""
    params = {"pageIndex": page}

    try:
        response = session.get(LIST_URL, headers=headers, params=params)
        response.encoding = 'utf-8'

        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')

            # í…Œì´ë¸” tbody ë‚´ì˜ trë“¤
            rows = soup.select('.tstyle_list tbody tr')

            posts = []
            for row in rows:
                cols = row.select('td')
                if len(cols) >= 5:
                    # ë§í¬ ì¶”ì¶œ
                    link = row.select_one('a.ellipsis')
                    if link:
                        href = link.get('href', '')
                        title = link.get(
                            'title', '') or link.get_text(strip=True)

                        posts.append({
                            'number': cols[0].get('title', '') or cols[0].get_text(strip=True),
                            'title': title,
                            'url': BASE_URL + href if href.startswith('/') else href,
                            # ë‹´ë‹¹ë¶€ì„œ
                            'department': cols[2].get('title', '') or cols[2].get_text(strip=True),
                            # ë‹´ë‹¹ì
                            'person': cols[3].get('title', '') or cols[3].get_text(strip=True),
                            # ê²Œì‹œì¼
                            'reg_date': cols[4].get('title', '') or cols[4].get_text(strip=True),
                            # ì¡°íšŒìˆ˜
                            'views': cols[5].get('title', '') if len(cols) > 5 else '',
                        })

            return posts
    except Exception as e:
        print(f"ëª©ë¡ ì¡°íšŒ ì—ëŸ¬ (í˜ì´ì§€ {page}): {e}")

    return []


def get_post_detail(url):
    """ê²Œì‹œê¸€ ìƒì„¸ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°"""
    try:
        response = session.get(url, headers=headers)
        response.encoding = 'utf-8'

        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')

            # ë³¸ë¬¸ ì˜ì—­ ì°¾ê¸° (ì—¬ëŸ¬ ì…€ë ‰í„° ì‹œë„)
            content_area = (
                soup.select_one('.view_cont') or
                soup.select_one('.board_view') or
                soup.select_one('#contents') or
                soup.select_one('.content')
            )

            raw_content = content_area.get_text(
                strip=False) if content_area else ''
            raw_html = str(content_area) if content_area else ''

            return {
                'raw_content': raw_content.strip(),
                'raw_html': raw_html,
            }
    except Exception as e:
        print(f"ìƒì„¸ ì¡°íšŒ ì—ëŸ¬ ({url}): {e}")

    return None


def parse_qa_content(raw_content):
    """ë‹¤ì–‘í•œ í˜•ì‹ì˜ ì§ˆì˜/ë‹µë³€ íŒŒì‹±"""

    result = {
        'questions': [],
        'answers': [],
        'parse_success': False,
        'format_detected': None,
    }

    if not raw_content:
        return result

    # íŒ¨í„´ 1: <ì§ˆ ì˜>, <ë‹µ ë³€>, <íšŒ ì‹œ> í˜•ì‹
    pattern1_q = re.search(
        r'<\s*ì§ˆ\s*ì˜?\s*>(.+?)(?=<\s*(?:ë‹µ|íšŒ)\s*)', raw_content, re.DOTALL)
    pattern1_a = re.search(
        r'<\s*(?:ë‹µ\s*ë³€|íšŒ\s*ì‹œ)\s*>(.+?)(?=<|$)', raw_content, re.DOTALL)

    if pattern1_q and pattern1_a:
        result['questions'].append(pattern1_q.group(1).strip())
        result['answers'].append(pattern1_a.group(1).strip())
        result['parse_success'] = True
        result['format_detected'] = 'angle_bracket'
        return result

    # íŒ¨í„´ 2: [ì§ˆì˜], [ë‹µë³€], [íšŒì‹œ] í˜•ì‹
    pattern2_q = re.search(
        r'\[\s*ì§ˆì˜\s*\](.+?)(?=\[\s*(?:ë‹µë³€|íšŒì‹œ)\s*\])', raw_content, re.DOTALL)
    pattern2_a = re.search(
        r'\[\s*(?:ë‹µë³€|íšŒì‹œ)\s*\](.+?)(?=\[|$)', raw_content, re.DOTALL)

    if pattern2_q and pattern2_a:
        result['questions'].append(pattern2_q.group(1).strip())
        result['answers'].append(pattern2_a.group(1).strip())
        result['parse_success'] = True
        result['format_detected'] = 'square_bracket'
        return result

    # íŒ¨í„´ 3: â–¡ ì§ˆì˜1 / â–¡ ë‹µë³€1, â–¡ì§ˆì˜.01 / â–¡ë‹µë³€.01 í˜•ì‹ (ìƒˆë¡œ ì¶”ê°€)
    # â–¡ ì§ˆì˜1, â–¡ ì§ˆì˜.01, â–¡ì§ˆì˜1, â–¡ì§ˆì˜.01 ë“± ë‹¤ì–‘í•œ ë³€í˜•
    box_qs = re.findall(
        r'â–¡\s*ì§ˆì˜[\s.]*\d*\s*\n?(.*?)(?=â–¡\s*(?:ë‹µë³€|ì§ˆì˜)[\s.]*\d*|$)', raw_content, re.DOTALL)
    box_as = re.findall(
        r'â–¡\s*ë‹µë³€[\s.]*\d*\s*\n?(.*?)(?=â–¡\s*(?:ë‹µë³€|ì§ˆì˜)[\s.]*\d*|$)', raw_content, re.DOTALL)

    if box_qs and box_as:
        result['questions'] = [q.strip() for q in box_qs if q.strip()]
        result['answers'] = [a.strip() for a in box_as if a.strip()]
        result['parse_success'] = True
        result['format_detected'] = 'box_numbered'
        return result

    # íŒ¨í„´ 4: â–¡ ì§ˆì˜ë‚´ìš© / â–¡ ë‹µë³€ë‚´ìš© í˜•ì‹
    box_q_content = re.search(
        r'â–¡\s*ì§ˆì˜\s*ë‚´ìš©\s*\n?(.*?)(?=â–¡\s*ë‹µë³€|$)', raw_content, re.DOTALL)
    box_a_content = re.search(
        r'â–¡\s*ë‹µë³€\s*ë‚´ìš©\s*\n?(.*?)(?=â–¡|ëª©ë¡|$)', raw_content, re.DOTALL)

    if box_q_content and box_a_content:
        result['questions'].append(box_q_content.group(1).strip())
        result['answers'].append(box_a_content.group(1).strip())
        result['parse_success'] = True
        result['format_detected'] = 'box_content'
        return result

    # íŒ¨í„´ 5: ì§ˆì˜ìš”ì§€ + (ì§ˆì˜1)(ì§ˆì˜2) / ë‹µë³€ë‚´ìš© í˜•ì‹
    if 'ì§ˆì˜ìš”ì§€' in raw_content or 'ì§ˆì˜ ìš”ì§€' in raw_content:
        q_section = re.search(r'(?:ì§ˆì˜\s*ìš”ì§€)(.+?)(?=ë‹µë³€|íšŒì‹œ|$)',
                              raw_content, re.DOTALL | re.IGNORECASE)
        a_section = re.search(r'(?:ë‹µë³€\s*ë‚´ìš©|íšŒì‹œ\s*ë‚´ìš©|ë‹µ\s*ë³€)(.+?)$',
                              raw_content, re.DOTALL | re.IGNORECASE)

        if q_section:
            q_text = q_section.group(1)
            individual_qs = re.findall(
                r'\(ì§ˆì˜\s*\d*\)(.+?)(?=\(ì§ˆì˜\s*\d*\)|$)', q_text, re.DOTALL)
            result['questions'] = [q.strip() for q in individual_qs] if individual_qs else [
                q_text.strip()]

        if a_section:
            a_text = a_section.group(1)
            individual_as = re.findall(
                r'\(ë‹µë³€\s*\d*\)(.+?)(?=\(ë‹µë³€\s*\d*\)|$)', a_text, re.DOTALL)
            result['answers'] = [a.strip() for a in individual_as] if individual_as else [
                a_text.strip()]

        if result['questions'] or result['answers']:
            result['parse_success'] = True
            result['format_detected'] = 'numbered'
            return result

    # íŒ¨í„´ 6: â—‹ (ì§ˆì˜N) ... <ë‹µë³€ë‚´ìš©> (ë‹µë³€N) í˜•ì‹
    circle_qs = re.findall(
        r'â—‹\s*\(ì§ˆì˜\d*\)\s*(.*?)(?=â—‹\s*\(ì§ˆì˜\d*\)|<ë‹µë³€|$)', raw_content, re.DOTALL)
    if '<ë‹µë³€ë‚´ìš©>' in raw_content or '<ë‹µë³€ ë‚´ìš©>' in raw_content:
        circle_as = re.findall(
            r'\(ë‹µë³€\d*\)\s*(.*?)(?=\(ë‹µë³€\d*\)|$)', raw_content, re.DOTALL)
        if circle_qs and circle_as:
            result['questions'] = [q.strip() for q in circle_qs if q.strip()]
            result['answers'] = [a.strip() for a in circle_as if a.strip()]
            result['parse_success'] = True
            result['format_detected'] = 'circle_numbered'
            return result

    # íŒ¨í„´ 7: <íšŒì‹  ë‚´ìš©> í˜•ì‹ (ì§ˆì˜ëŠ” ë³¸ë¬¸ì—ì„œ ì¶”ì¶œ)
    if '<íšŒì‹  ë‚´ìš©>' in raw_content or '<íšŒì‹ ë‚´ìš©>' in raw_content:
        # íšŒì‹  ë‚´ìš© ì•ë¶€ë¶„ì„ ì§ˆì˜ë¡œ ì¶”ì •
        reply_match = re.search(
            r'<íšŒì‹ \s*ë‚´ìš©>(.*?)(?=ëª©ë¡|$)', raw_content, re.DOTALL)
        if reply_match:
            result['answers'].append(reply_match.group(1).strip())
            # ì§ˆì˜ëŠ” íšŒì‹  ì „ ë‚´ìš©ì—ì„œ ì¶”ì¶œ
            before_reply = raw_content.split('<íšŒì‹ ')[0]
            # ë§ˆì§€ë§‰ ë¬¸ë‹¨ì„ ì§ˆì˜ë¡œ ì‚¬ìš©
            last_paragraph = before_reply.strip().split(
                '\n\n')[-1] if before_reply else ''
            if last_paragraph:
                result['questions'].append(last_paragraph.strip())
            result['parse_success'] = True
            result['format_detected'] = 'reply_format'
            return result

    result['format_detected'] = 'unknown'
    return result


def apply_clean_text_to_result(result):
    """íŒŒì‹± ê²°ê³¼ì— clean_text ì ìš©"""
    if result['parse_success']:
        result['questions'] = [clean_text(q) for q in result['questions']]
        result['answers'] = [clean_text(a) for a in result['answers']]
    return result


def crawl_all_posts():
    """ëª¨ë“  í˜ì´ì§€ ê²Œì‹œê¸€ ëª©ë¡ ìˆ˜ì§‘"""
    all_posts = []
    page = 1

    while True:
        print(f"  í˜ì´ì§€ {page} ì¡°íšŒ ì¤‘...")
        posts = get_post_list_page(page)

        if not posts:
            print(f"  â†’ í˜ì´ì§€ {page}ì—ì„œ ê²°ê³¼ ì—†ìŒ. ìˆ˜ì§‘ ì¢…ë£Œ.")
            break

        all_posts.extend(posts)
        print(f"  â†’ {len(posts)}ê°œ ê²Œì‹œê¸€ ìˆ˜ì§‘ (ëˆ„ì : {len(all_posts)}ê°œ)")

        page += 1
        time.sleep(0.3)

    return all_posts


def crawl_with_details(posts, save_interval=100):
    """ìƒì„¸ ë‚´ìš©ê¹Œì§€ í¬í•¨í•˜ì—¬ í¬ë¡¤ë§"""
    results = []

    for i, post in enumerate(posts):
        print(f"[{i+1}/{len(posts)}] {post['title'][:40]}...")

        detail = get_post_detail(post['url'])

        if detail:
            parsed = parse_qa_content(detail['raw_content'])
            # ì§ˆë¬¸/ë‹µë³€ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì ìš©
            parsed = apply_clean_text_to_result(parsed)

            results.append({
                'number': post['number'],
                'title': post['title'],
                'department': post['department'],
                'person': post.get('person', ''),
                'reg_date': post['reg_date'],
                'views': post.get('views', ''),
                'url': post['url'],
                'raw_content': detail['raw_content'],
                'parsed': parsed,
            })

            status = "âœ“" if parsed['parse_success'] else "âœ—"
            print(f"  â†’ {status} {parsed['format_detected']}")
        else:
            print(f"  â†’ âœ— ìƒì„¸ ë‚´ìš© ì—†ìŒ")

        # ì¤‘ê°„ ì €ì¥
        if (i + 1) % save_interval == 0:
            save_results(results, f'í–‰ì •í•´ì„_temp_{i+1}.json')
            print(f"  ğŸ’¾ ì¤‘ê°„ ì €ì¥ ì™„ë£Œ ({i+1}ê°œ)")

        time.sleep(0.3)

    return results


def save_results(data, filename):
    """ê²°ê³¼ ì €ì¥"""
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"ì €ì¥: {filename}")


def main():
    print("=== ë…¸ë™ê³ ìš©ë¶€ í–‰ì •í•´ì„(ì§ˆì˜íšŒì‹œ) í¬ë¡¤ë§ ì‹œì‘ ===\n")

    # Step 1: ê²Œì‹œê¸€ ëª©ë¡ ì „ì²´ ìˆ˜ì§‘
    print("[Step 1] ê²Œì‹œê¸€ ëª©ë¡ ìˆ˜ì§‘ ì¤‘...")
    posts = crawl_all_posts()
    print(f"\nì´ {len(posts)}ê°œ ê²Œì‹œê¸€ ë°œê²¬\n")

    if not posts:
        print("ê²Œì‹œê¸€ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return

    # ëª©ë¡ë§Œ ë¨¼ì € ì €ì¥ (ì•ˆì „ë§)
    save_results(posts, 'í–‰ì •í•´ì„_ëª©ë¡.json')

    # Step 2: ìƒì„¸ ë‚´ìš© í¬ë¡¤ë§ + íŒŒì‹±
    print("\n[Step 2] ìƒì„¸ ë‚´ìš© í¬ë¡¤ë§ ë° íŒŒì‹± ì¤‘...")
    results = crawl_with_details(posts)

    # Step 3: ìµœì¢… ê²°ê³¼ ì €ì¥
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    save_results(results, f'í–‰ì •í•´ì„_{timestamp}.json')

    # í†µê³„
    success_count = sum(1 for r in results if r['parsed']['parse_success'])
    print(f"\n=== í¬ë¡¤ë§ ì™„ë£Œ ===")
    print(f"ì´ {len(results)}ê°œ ê²Œì‹œê¸€")
    print(
        f"íŒŒì‹± ì„±ê³µ: {success_count}ê°œ ({100*success_count/len(results):.1f}%)" if results else "")


if __name__ == "__main__":
    main()
